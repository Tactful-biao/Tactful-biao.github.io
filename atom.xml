<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>BiaoBiao&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.bbiao.me/"/>
  <updated>2018-06-05T14:20:53.206Z</updated>
  <id>https://www.bbiao.me/</id>
  
  <author>
    <name>标标</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据之美</title>
    <link href="https://www.bbiao.me/2018/06/05/%E6%95%B0%E6%8D%AE%E4%B9%8B%E7%BE%8E/"/>
    <id>https://www.bbiao.me/2018/06/05/数据之美/</id>
    <published>2018-06-05T08:21:40.000Z</published>
    <updated>2018-06-05T14:20:53.206Z</updated>
    
    <content type="html"><![CDATA[<p>现如今是互联网时代，是大数据时代！ 数据变得越来越重要。随之而来的数据分析也就变得举足轻重了，数据终究是死的东西，而把数据变成有意义的东西就是数据分析。现在各个行业都需要数据分析，因为只有分析之后才能够准确的定位，才能够准确的把握市场。</p><h2 id="数据获取"><a href="#数据获取" class="headerlink" title="数据获取"></a>数据获取</h2><p>  谈到数据就会想到数据的获取，针对数据获取，不同的人可能会有不同的想法。做安全的可能会想到通过渗透去”脱裤”，不过大部分人可能会想到通过爬虫去获取。</p><p>  前几天在逛知乎的时候无意间看到一篇文章<a href="https://zhuanlan.zhihu.com/p/37395436" target="_blank" rel="external">Python爬虫，FineBI画图，让数据报告更专业一点！</a>。当时就激起了我的兴趣，原来数据展示也可以这么的美，写了这么长时间的爬虫，除了上次对QQ空间的分析之外，还没怎么做过数据可视化。就打算自己动手尝试一下。经过两天的努力，复现了上面文章的内容。下面简单总结一下。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p>   数据虽然很多，但是我们获取之前一定要准确筛选，在做一件事情之前，一定要清楚的知道该怎样去做，至少大致的方向要清楚。我们一定要想好获取什么数据，没有意义的数据获取下来并没有什么实际的意义。就跟做饭一样，选材很重要，我选择的是房源。房价一直是大家关注的热点，可是关于房价大家并没有什么太直观的感觉。虽然，我还买不起房，但是我可以先分析一波啊。</p><h3 id="获取赶集网房源信息"><a href="#获取赶集网房源信息" class="headerlink" title="获取赶集网房源信息"></a>获取赶集网房源信息</h3><p>  从赶集网上可以看到很多的房源信息，我相信信息的准确性还是很高的。我选择的是乌鲁木齐房价信息，有很多城市可以选取，大家根据自己的兴趣来，如下图:<br>  <img src="https://moetu.fastmirror.org/images/2018/06/05/_001e2079a95738335cd.png" alt="1"></p><p>  这里我获取了如下内容:</p><ul><li>名称</li><li>地址</li><li>户型</li><li>面积</li><li>单价</li><li>总价</li><li>地区</li><li>区域</li></ul><p>然后把数据保存到mysql数据库里。</p><p>赶集网的发爬机制还是十分的强的。如果你访问频率稍微快那么一点点就会跳出验证码，就算正常访问，你翻两页就会弹出验证码，后面每一页都会出现验证码。这种用户体验好吗？反正我是不能接受！</p><p>验证码如下:<br><img src="https://moetu.fastmirror.org/images/2018/06/05/_002aa3a048bb0beba77.png" alt="2"></p><p>如果仅仅只是滑动验证码的话，倒也可以通过写代码搞定，但是我滑了几次之后，验证码再次升级如下:</p><p><img src="https://moetu.fastmirror.org/images/2018/06/05/_003780db346e844f3bd.png" alt="3"></p><font color="red">这…</font><p>然后我第一次为了获取数据而花钱，还花了两元    (／‵Д′)／~ ╧╧， 因为我之前看到了一篇关于付费代理性能测试的文章，然后我就想到了它，本来一块钱可以搞定的事情，由于我的大意多花了一块。</p><p>我在阿布云买了一个小时的动态代理，关于阿布云的内容感兴趣的自己去查，通过代理的方式可以绕过验证码，思路是通过代理然后把每一页都保存成html文件到本地，只要数据到本地了。剩下的还不是任由自己想怎么玩就怎么玩。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> <span class="keyword">self</span>.<span class="symbol">pages:</span></div><div class="line">    url = <span class="string">'http://xj.ganji.com/fang5/o'</span>+str(<span class="keyword">self</span>.pages[<span class="number">0</span>])+<span class="string">'/'</span></div><div class="line">    headers = &#123;</div><div class="line">        <span class="string">'Host'</span>: <span class="string">'xj.ganji.com'</span>,</div><div class="line">        <span class="string">'Referer'</span>: <span class="string">'http://xj.ganji.com'</span>,</div><div class="line">        <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span>,</div><div class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span></div><div class="line">    &#125;</div><div class="line">    proxy = <span class="string">"http://%(user)s:%(pass)s@%(host)s:%(port)s"</span> % &#123;</div><div class="line">              <span class="string">"host"</span>: <span class="keyword">self</span>.proxyHost,</div><div class="line">              <span class="string">"port"</span>: <span class="keyword">self</span>.proxyPort,</div><div class="line">              <span class="string">"user"</span>: <span class="keyword">self</span>.proxyUser,</div><div class="line">              <span class="string">"pass"</span>: <span class="keyword">self</span>.proxyPass,</div><div class="line">            &#125;</div><div class="line">    proxies = &#123;</div><div class="line">        <span class="string">'http'</span>:  proxy,</div><div class="line">        <span class="string">'https'</span>: proxy</div><div class="line">    &#125;</div><div class="line">    html = requests.get(url, headers=headers, proxies=proxies).text</div><div class="line">    <span class="keyword">if</span> <span class="string">'Cache Access Denied.'</span> <span class="keyword">in</span> html <span class="keyword">or</span> <span class="string">'访问过于频繁，本次访问做以下验证码校验'</span> <span class="keyword">in</span> <span class="symbol">html:</span></div><div class="line">        pass</div><div class="line">    <span class="symbol">else:</span></div><div class="line">        with open(str(<span class="keyword">self</span>.pages[<span class="number">0</span>]) + <span class="string">'.html'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) as <span class="symbol">ht:</span></div><div class="line">            ht.write(html)</div><div class="line">            <span class="keyword">self</span>.pages.remove(<span class="keyword">self</span>.pages[<span class="number">0</span>])</div></pre></td></tr></table></figure><p>通过上面的代码可以把所有的网页保存到本地，通过手工测试可以发现总共有109页(源码里没发现)。</p><p>这是保存到到本地的HTML文件：<br><img src="https://moetu.fastmirror.org/images/2018/06/05/_004dd5f80a8a75b5437.png" alt="4"></p><p>然后通过BeautifulSoup读取html文件，然后解析出我们想要的内容。</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> page in range(<span class="number">1</span>, <span class="number">110</span>):</div><div class="line">    soup = BeautifulSoup(open(<span class="string">'html/'</span>+str(page)+<span class="string">'.html'</span>), <span class="string">'lxml'</span>)</div><div class="line">    name = soup.<span class="keyword">findAll</span>(class_=<span class="string">'dd-item title'</span>)</div><div class="line">    url = soup.<span class="keyword">findAll</span>(class_=<span class="string">'f-list-item ershoufang-list'</span>)</div><div class="line">    community = soup.<span class="keyword">findAll</span>(class_=<span class="string">'area'</span>)</div><div class="line">    DoorModel = soup.<span class="keyword">findAll</span>(class_=<span class="string">'dd-item size'</span>)</div><div class="line">    AreaSize = soup.<span class="keyword">findAll</span>(class_=<span class="string">'dd-item size'</span>)</div><div class="line">    price = soup.<span class="keyword">findAll</span>(class_=<span class="string">'num js-price'</span>)</div><div class="line">    UnintPrice = soup.<span class="keyword">findAll</span>(class_=<span class="string">'time'</span>)</div><div class="line">    area = soup.<span class="keyword">findAll</span>(class_=<span class="string">'address-eara'</span>)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i, j, k, l, m, n, o, p in zip(name, url, community, DoorModel, AreaSize, price, UnintPrice, area):</div><div class="line">        title = i.<span class="keyword">find</span>(<span class="string">'a'</span>).<span class="keyword">getText</span>()</div><div class="line">        address = j[<span class="string">'href'</span>] <span class="keyword">if</span> <span class="string">'http'</span> in j[<span class="string">'href'</span>] <span class="keyword">else</span> <span class="string">'http://xj.ganji.com'</span> + j[<span class="string">'href'</span>]</div><div class="line">        xiaoqu = re.sub(<span class="string">'(\(.*|\.\.\.)'</span>, <span class="string">''</span>, k.<span class="keyword">getText</span>().split(<span class="string">'-'</span>)[<span class="number">0</span>].strip()).replace(<span class="string">'小区'</span>, <span class="string">''</span>)</div><div class="line">        huxing = l[<span class="string">'data-huxing'</span>]</div><div class="line">        mianji = m[<span class="string">'data-area'</span>].replace(<span class="string">'㎡'</span>, <span class="string">'平'</span>)</div><div class="line">        zongjia = n.<span class="keyword">getText</span>()</div><div class="line">        danjia = o.<span class="keyword">getText</span>().replace(<span class="string">'元/㎡'</span>, <span class="string">''</span>)</div><div class="line">        quyu = re.sub(<span class="string">'\(.*'</span>, <span class="string">''</span>, p.<span class="keyword">getText</span>().split(<span class="string">'-'</span>)[-<span class="number">1</span>].strip().replace(<span class="string">'二手房出售'</span>, <span class="string">''</span>))</div><div class="line">        <span class="keyword">print</span>(<span class="string">'名称:'</span>, title, <span class="string">'地址:'</span>, address, <span class="string">'小区:'</span>, xiaoqu, <span class="string">'户型:'</span>, huxing, <span class="string">'面积:'</span>, mianji,</div><div class="line">              <span class="string">'总价:'</span>, zongjia, <span class="string">'单价:'</span>, danjia, <span class="string">'区域:'</span>, quyu)</div></pre></td></tr></table></figure><p>效果如下：</p><p><img src="https://moetu.fastmirror.org/images/2018/06/05/_0059a93d532ba699362.png" alt="5"></p><p>接着把数据保存到mysql数据库，数据库的表设计如下:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">DROP</span> <span class="keyword">DATABASE</span> house;</div><div class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> house;</div><div class="line"><span class="keyword">use</span> house;</div><div class="line"></div><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> info(</div><div class="line">  <span class="string">`名称`</span> <span class="built_in">CHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">  <span class="string">`地址`</span> <span class="built_in">VARCHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">  <span class="string">`小区`</span> <span class="built_in">CHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">  <span class="string">`户型`</span> <span class="built_in">CHAR</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">  <span class="string">`面积`</span> <span class="built_in">CHAR</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">  <span class="string">`总价`</span> <span class="built_in">FLOAT</span>(<span class="number">6</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">  <span class="string">`单价`</span> <span class="built_in">FLOAT</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">  <span class="string">`区域`</span> <span class="built_in">CHAR</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</div><div class="line">  <span class="string">`id`</span> <span class="built_in">INT</span> auto_increment PRIMARY <span class="keyword">KEY</span></div><div class="line">);</div></pre></td></tr></table></figure><p>写入数据库的代码如下:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">sql = <span class="string">'insert into info(名称, 地址, 小区, 户型, 面积, 总价, 单价, 区域) values (%s, %s, %s, %s, %s, %s, %s, %s)'</span></div><div class="line">try:</div><div class="line">    self<span class="selector-class">.cursor</span><span class="selector-class">.execute</span>(sql, (title, <span class="selector-tag">address</span>, xiaoqu, huxing, mianji, <span class="attribute">float</span>(zongjia), float(danjia), quyu))</div><div class="line">    self<span class="selector-class">.db</span><span class="selector-class">.commit</span>()</div><div class="line">except pymysql<span class="selector-class">.MySQLError</span> as e:</div><div class="line">    print(e.args)</div><div class="line">    self<span class="selector-class">.db</span><span class="selector-class">.rollback</span>()</div></pre></td></tr></table></figure><p>查看数据如下:</p><p><img src="https://moetu.fastmirror.org/images/2018/06/05/_006c26811afd61ee689.png" alt="6"></p><p>下面就是把数据展示出来，数据展示有常用的方法是使用matplotlib，不过对于没有系统学过的用户来说想要用它画出优美的图形，还是有点困难的。</p><p>现在已经有的数据可视化工具做的还不错的，已经有好多了，我们不妨选一个来尝试一下。这里我选了一个FineBI， 不知道具体怎么样，不过对于个人开发者免费使用，这种免费的东西，哪怕它复杂一点，我都是能够接受的。 感兴趣的朋友可以下载下来尝试一下，Windows、MAC、Linux都支持，还是很不错的，官网给出了安装文档等一些基本信息。</p><p>不过，我一般是懒得看文档的，除非实在是找不到解决办法的时候，我使用它做了一些基本的图形如下：</p><p><img src="https://moetu.fastmirror.org/images/2018/06/05/_0074d41837197a5e3b6.png" alt="7"></p><p>操作还是十分简单的，只需要把响应的数据字段拖到响应的位置就可以自动生成图表了。其中需要注意的有，数据字段的设计要注意，因为CHAR类型和INT类型在操作上是不同的，比如在数据的拖到上，CHAR类型的数据是不能拖到数值类型的框里面去的。</p><p>其中要连接数据库:</p><p><img src="https://moetu.fastmirror.org/images/2018/06/05/_0084cc9280973edf31d.png" alt="8"></p><p>数据库连接之后就可以通过数据库查询语句进行数据查询了：</p><p><img src="https://moetu.fastmirror.org/images/2018/06/05/_009b0c58f74c70fc60d.png" alt="9"></p><p>最终只需要拖动响应的字段就可以实现数据可视化了：</p><p><img src="https://moetu.fastmirror.org/images/2018/06/05/dbcafdbb0483def7a7b9b59c575dcd02e50da4d871397424.gif" alt="10"></p><p>其他的也是图表也是类似，就看自己的发挥。</p><h3 id="Data-V的使用"><a href="#Data-V的使用" class="headerlink" title="Data V的使用"></a>Data V的使用</h3><p>阿里云的数加平台也是一个数据可视化平台。双十一的交易大屏我相信很多人都有了解，阿里云同样给用户提供了这样的工具。当初我买的个人版(有些模板不能用)，9.9元一年，当初感觉也不贵，就买来玩玩。</p><p>能用的模板还是有一部分的：</p><p><img src="https://moetu.fastmirror.org/images/2018/06/05/_010ae067bcdd1fffaec.png" alt="11"></p><p>通过连接数据库导入数据，当然还有其他的连接方式(csv,静态数据,API等), 可以根据实际情况来选择:</p><p><img src="https://moetu.fastmirror.org/images/2018/06/05/_012e0b3216a008fd2d2.png" alt="12"></p><p>查询响应结果可以看到:</p><p><img src="https://moetu.fastmirror.org/images/2018/06/05/_011f4e2a2fb44ac84b6.png" alt="13"></p><p>然后在设置对应的字段就可以了:</p><p><img src="https://moetu.fastmirror.org/images/2018/06/05/_013fbb1d2879ee0c38f.png" alt="14"></p><p>然后对应的数据就以图形化的方式展示出来了，这两种方法都是比较简单的，大家可以根据自己的感觉去选择。</p><p><img src="https://moetu.fastmirror.org/images/2018/06/05/_0140301dfba95ff8667.png" alt="15"></p><blockquote><p>明年就要面临毕业了，打算在这方面做个毕业设计，这里先拿来练练手。感觉还行，看起来效果要好一点，这种展示起来要好很多，但是这一个显然不可以，因为它涉及到的分析内容很少，主要是可视化展示这一块比较多。</p></blockquote><p>如果，你感觉文章对你有用，文章下方有打赏按钮，你的打赏就是我最大的动力。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;现如今是互联网时代，是大数据时代！ 数据变得越来越重要。随之而来的数据分析也就变得举足轻重了，数据终究是死的东西，而把数据变成有意义的东西就是数据分析。现在各个行业都需要数据分析，因为只有分析之后才能够准确的定位，才能够准确的把握市场。&lt;/p&gt;
&lt;h2 id=&quot;数据获取&quot;&gt;
      
    
    </summary>
    
      <category term="python" scheme="https://www.bbiao.me/categories/python/"/>
    
    
      <category term="数据分析" scheme="https://www.bbiao.me/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
      <category term="FineBI" scheme="https://www.bbiao.me/tags/FineBI/"/>
    
  </entry>
  
  <entry>
    <title>分享两个有趣的思路</title>
    <link href="https://www.bbiao.me/2018/05/23/%E5%88%86%E4%BA%AB%E4%B8%A4%E4%B8%AA%E6%9C%89%E8%B6%A3%E7%9A%84%E6%80%9D%E8%B7%AF/"/>
    <id>https://www.bbiao.me/2018/05/23/分享两个有趣的思路/</id>
    <published>2018-05-23T03:51:20.000Z</published>
    <updated>2018-05-23T04:40:29.926Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>思路在编程过程中十分的重要，一个人能否写出高质量的代码，关键就在于是否有一个清晰的思路。这里分享两个我最近一段时间遇到的两个比较有意思的解决问题的思路。</p></blockquote><h2 id="两个思路"><a href="#两个思路" class="headerlink" title="两个思路"></a>两个思路</h2><h3 id="思路一-方法一"><a href="#思路一-方法一" class="headerlink" title="思路一 (方法一)"></a>思路一 (方法一)</h3><p>这是我做的《基于lucene的全文搜索引擎》课程作业，其中添加了电影搜索功能，前端接受一个关键字，后端通过python脚本去获取数据，然后再返回给前端。效果如下：</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_005ae5e9adc377cd7e6.png" alt="1"></p><p>数据是通过种子帝(www.btdtdy.org)获取的。一般获取数据的方法是输入关键词，通过构造请求链接，然后获取数据，解析数据。但是该网站的请求链接比较特殊，它把关键字加密了，如下图我们搜索的 钢铁侠 请求链接里的search后面的关键词显然被加密了。想要请求它的内容就需要知道它的加密方式。</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_00654e5ecde6c684a8d.png" alt="2"></p><p>一般加密都是通过前端加密。我们通过分析请求发现s.php?id=243这个链接里面定义了加密方式是Base64</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_007880adaace23e0e2f.png" alt="3"></p><p>我们使用python通过测试如下：</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_0085f897adb4f68460e.png" alt="4"></p><ul><li>先导入base64模块</li><li>定义关键字</li><li>把关键字编码为utf-8</li><li>使用b64encode编码后的关键词</li><li>解码关键词</li></ul><p>通过上面的操作，我们就得到了加密后的关键词。</p><p>通过对比我们发现url中的关键词是：<font color="red">b-6ZKi6Z0B5L6g</font></p><p>我们加密得到的是            <font color="red">6ZKi6Z0B5L6g</font></p><p>经过比较我们发现前面的b-是固定不变的。这样我们就相当于破解了它的加密方式。接下的请求和解析就很简单了，这里就不在介绍了，我们接下来介绍第二种方法。</p><h3 id="思路一-方法二"><a href="#思路一-方法二" class="headerlink" title="思路一 (方法二)"></a>思路一 (方法二)</h3><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_010bcb01e28778c22d9.png" alt="5"></p><p>在实际的测试中，我发现了上图中框起来的地方。做过网站的应该都清楚rss，RSS订阅能更快地获取信息，网站提供RSS输出，有利于让用户获取网站内容的最新更新。网络用户可以在客户端借助于支持RSS的聚合工具软件，在不打开网站内容页面的情况下阅读支持RSS输出的网站内容。</p><p>我们打开这个页面之后发现，我们想要的内容都在这里面（种子链接详情页，种子标题）而且关键词也没有加密，这样的话，我们就可以直接请求这个xml的文件，然后从里面提取我们想要的内容就可以了呀：</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_0115a18b6657988d3b2.png" alt="6"></p><p>为了防止偶然，我们有换了一个关键词，发现完全OK。这样的话，我们只需要构造这个请求就可以获取我们想要的东西。</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_0122179ed0a2862f16d.png" alt="7"></p><blockquote><p>方法一和方法二都能够拿到数据，方法一采用的是硬肛的方法，而方法二明显巧妙了许多。方法二比方法一要简单很多，不仅没有加密而且数据规则，方便提取。所以在行动前多思考，我相信方法一并不是所有人都能够想的出来，还要分析请求。所以一定要多思考、多分析。</p></blockquote><h2 id="思路二"><a href="#思路二" class="headerlink" title="思路二"></a>思路二</h2><p>这是我搭建代理池的时候遇到的问题，代理池需要大量的代理，代理网站越多，获取到的代理就越多，国内免费的代理就那么几个，而且都被别人爬烂了，质量十分的低。而国外的就不一样，由于墙的关系，很大一部分人都被拦住了，而我刚好有国外的服务器，搭建了爬虫环境，所以这一层对我没用。</p><p>这里推荐一个国外的代理网站: <a href="http://spys.one/en/free-proxy-list/" target="_blank" rel="external">http://spys.one/en/free-proxy-list/</a></p><p>如下图，500个免费的ip，可用性高，更新速度快：</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_009a1e5281a8c1baff0.png" alt="8"></p><p>正当我准备把这500个热乎乎的ip收入囊中的时候，问题来了，我发现它的端口号是加了密的，WTF？！！！如下图：</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_01385f5ec8a79c70e71.png" alt="9"></p><p>上图是复制出来的源代码，通过正则表达式匹配到的ip，红框圈起来的就是端口号，这是什么鬼？！难道就这么放弃吗？放弃是不可能放弃的，这辈子也不会放弃的。就因为它有难度，所以挡住了绝大多数的爬虫，如果我能爬下来，那岂不是美滋滋。</p><p>继续分析， 在源代码里发现了这个，如下图：</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_014f14ea6d284ff15ff.png" alt="10"></p><p>这一段看起来骨骼惊奇的代码，一看就不简单。</p><p>把它复制出来，然后稍微整理一下，如下图：</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_015cd2d49946cbf0a18.png" alt="11"></p><ul><li>1-20行是赋值出来的字符串</li><li>22行是端口加密的字符串</li></ul><p>通过比较发现它们之间果然有“见不得人的勾当” ，比如</p><font color="red">(g7g7f6^g7q7)  对应于上面的g7g7f6=8^g7q7; g7q7=1221；</font><p>通过python运算8^1221^1221得到结果8，如上图右半部分。</p><p>其中的+不是加法运算，而是连接运算。它把每一个数字连接起来，最终组合成端口号：上图中组合出来的端口号为8080。</p><p>请仔细观察上图中的这一部分：</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_0160215bd77f688b8d0.png" alt="12"></p><p>它是从等号右面从0-9，比如：<br>8^1221^1221 = 8<br>0^7693^7693 = 0<br>……</p><p>从上面可以发现，^后面的都可以不要，只需要判断端口加密字符串中包含上面哪一个字符串，如果包含b2t0l2那么它就是0，如果包含z6l2w3，那么它就是1。</p><p>这样就可以把上面的内容整合成下面的样子，通过正则匹配可以轻松的截取出来：</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_017fbb58517bb89ff81.png" alt="13"></p><p>这样的话，我们就可以很轻松的算出端口号。</p><p>你以为这样就结束了？！ 并没有</p><hr><p>我发现上面的字符串并不是固定的，它是动态变化的，这一次请求是：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">r8d4=<span class="number">7693</span>;h8c3=<span class="number">1203</span>;c3i9=<span class="number">8380</span>;w3v2=<span class="number">3981</span>;i9u1=<span class="number">5597</span>;y5j0=<span class="number">9372</span>;d4h8=<span class="number">9969</span>;z6l2=<span class="number">1350</span>;g7q7=<span class="number">1221</span>;k1o5=<span class="number">7185</span>;b2t0l2=<span class="number">0</span>^r8d4;z6l2w3=<span class="number">1</span>^h8c3;n4a1v2=<span class="number">2</span>^c3i9;p6w3q7=<span class="number">3</span>^w3v2;u1p6n4=<span class="number">4</span>^i9u1;l2x4k1=<span class="number">5</span>^y5j0;s9r8y5=<span class="number">6</span>^d4h8;k1q7g7=<span class="number">7</span>^z6l2;g7g7f6=<span class="number">8</span>^g7q7;f6h8a1=<span class="number">9</span>^k1o5;</div></pre></td></tr></table></figure></p><p>下一次请求就变成：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">w3l2=<span class="number">2592</span>;s9o5=<span class="number">3288</span>;u1h8=<span class="number">6497</span>;j0s9=<span class="number">7893</span>;e5c3=<span class="number">1467</span>;d4r8=<span class="number">8884</span>;k1k1=<span class="number">8142</span>;x4q7=<span class="number">4302</span>;b2w3=<span class="number">2286</span>;m3d4=<span class="number">1753</span>;p6x4c3=<span class="number">0</span>^w3l2;g7a1y5=<span class="number">1</span>^s9o5;n4f6j0=<span class="number">2</span>^u1h8;q7r8n4=<span class="number">3</span>^j0s9;r8b2e5=<span class="number">4</span>^e5c3;s9q7s9=<span class="number">5</span>^d4r8;d4k1m3=<span class="number">6</span>^k1k1;o5s9w3=<span class="number">7</span>^x4q7;u1h8o5=<span class="number">8</span>^b2w3;v2j0r8=<span class="number">9</span>^m3d4;</div></pre></td></tr></table></figure></p><p>这显然是不一样的。WTF？！！！</p><font color="red">这….</font><p>说过不放弃，就是不放弃。继续分析，发现虽然它加密字符串每次都在变，但是我们已经知道了它的构造方法，那么我们就跟着动态变化就是了。我们每次请求的时候，动态提取解密字符串，然后拿着它去解密端口号。</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_01814a53cedde8c4b97.png" alt="14"></p><p>通过上面的方法，我们可以得到一个列表如下图：</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_0194b4a9f4e76025221.png" alt="15"></p><p>列表的对应位置刚好是解密字符串的值。</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_020cda4ae4dcd68f7d9.png" alt="16"></p><ul><li>ips 是通过正则表达式匹配到的ip</li><li>base是通过正则表达式提取的解密字符串列表</li><li>ports是通过正则表达式提取的端口的加密字符串</li><li>aa 是把加密的端口号通过+划分开</li><li>内层for循环解密端口号</li><li>xs 是组装好的端口号</li><li>result是ip加端口号</li></ul><p>上面的代码运行结果如下：</p><p><img src="https://moetu.fastmirror.org/images/2018/05/23/_02165d264ec9feedf81.png" alt="17"></p><p>项目地址：<a href="https://github.com/Tactful-biao/scrapy/blob/master/ip.py" target="_blank" rel="external">github地址</a></p><blockquote><p>上面是我最近遇到的两个比较有意思的问题，以及我的解决思路，希望能给你带来一定的启发。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;思路在编程过程中十分的重要，一个人能否写出高质量的代码，关键就在于是否有一个清晰的思路。这里分享两个我最近一段时间遇到的两个比较有意思的解决问题的思路。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;两个思路&quot;&gt;&lt;a href=&quot;#两个思路&quot;
      
    
    </summary>
    
      <category term="python" scheme="https://www.bbiao.me/categories/python/"/>
    
    
      <category term="爬虫" scheme="https://www.bbiao.me/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="思路" scheme="https://www.bbiao.me/tags/%E6%80%9D%E8%B7%AF/"/>
    
  </entry>
  
  <entry>
    <title>校园内网实现穿梭</title>
    <link href="https://www.bbiao.me/2018/05/04/%E6%A0%A1%E5%9B%AD%E5%86%85%E7%BD%91%E5%AE%9E%E7%8E%B0%E7%A9%BF%E6%A2%AD/"/>
    <id>https://www.bbiao.me/2018/05/04/校园内网实现穿梭/</id>
    <published>2018-05-04T01:15:01.000Z</published>
    <updated>2018-05-04T01:51:49.541Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>好的学校都是相同的，垃圾的学校垃圾到方方面面。</p></blockquote><p>学校的校园网分宿舍区和办公区，办公区是老师待的地方(免费, 不限速),同学上课的时候会在，宿舍区是收费的(30-49元一个月，4M网络，不能开共享)。</p><p>贫穷丰富了我的想象力，无论是宿舍区还是办公区它都是统统在校园网络下面，这样的话可不可以从宿舍区跳到办公区呢？答案是肯定的。</p><h2 id="shadowsocks-的使用"><a href="#shadowsocks-的使用" class="headerlink" title="shadowsocks 的使用"></a>shadowsocks 的使用</h2><p>Shadowsocks: 一种基于Socks5代理方式的加密传输协定，也可以指实作这个协定的各种传输套件。目前套件使用Python、C、C++、C#、Go语言等程式语言开发，大部分主要实作（iOS平台的除外）采用Apache许可证、GPL、MIT许可证等多种自由软体许可协定开放原始码。shadowsocks分为服务器端和客户端，在使用之前，需要先将服务器端部署到服务器上面，然后通过客户端连接并建立本地代理。(维基百科解释)</p><h3 id="安装shadowsocks"><a href="#安装shadowsocks" class="headerlink" title="安装shadowsocks"></a>安装shadowsocks</h3><p>本机环境： Ubuntu 16.04 （其他发行版也同样）<br>安装shadowsocks： pip install shadowsocks<br>创建一个配置文件 vi /etc/shadowsocks.json<br>{<br>    “server”:”0.0.0.0”,<br>    “server_port”:指定端口,<br>    “local_port”:1080,<br>    “password”: 密码,<br>    “timeout”:600,<br>    “method”:”aes-256-cfb”<br>}<br>这一部分具体可以参考我的这篇文章<a href="https://bbiao.me/2017/07/16/centos7%E6%90%AD%E5%BB%BAshadowsocks%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/" target="_blank" rel="external">centos7-搭建shadowsocks科学上网</a></p><p>使用： ssserver -c /etc/shadowsocks.json<br>使用客户端就可以进行连接了。<br><a href="http://oxwgzg29g.bkt.clouddn.com/shadowsocks--universal-4.5.7.apk" target="_blank" rel="external">安卓手机apk下载地址</a></p><p>为了不每次开机都手动启动，可以把它加入到开机自启里面：vi /etc/rc.local</p><p>在 exit 0前面加入：ssserver -c /etc/shadowsocks.json -d start<br>更改rc.local的权限：sudo chmod +x /etc/rc.local<br>重启就可以自动启动了。</p><h2 id="实现穿越"><a href="#实现穿越" class="headerlink" title="实现穿越"></a>实现穿越</h2><p>学校是给每个学院划分了ip的，这个ip在整个校园内是唯一的，在校园内任何一台机器上都可以访问到该ip，这简称为内网。实现穿越的前提是你要在办公区有一台持续在线的机器。因为我在实验室(实验室在办公区)，所以我就用我自己的电脑作为跳转。</p><p>校园网是有认证机制的(宿舍区和办公区都是一样的)，为了节省资源，默认3-5分钟不用网络，网络就自动断开了，所以我们要写一个简单的脚本，使网络不断开.<br>两种思路：</p><ul><li>每三分钟判断一下是否还有网络，若没网则脚本登录</li><li>每时每刻都使用网络，使其不断网</li></ul><p>###第一种思路<br>尝试了一下不太好实现，它的密码是经过特殊处理的，不过我们使用其他的方法进行了实现：<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">from</span> selenium import webdriver</div><div class="line"></div><div class="line">driver = webdriver.PhantomJS()</div><div class="line">driver.<span class="built_in">get</span>(<span class="string">'http://192.168.118.51/srun_portal_pc?ac_id=4'</span>)</div><div class="line">driver.find_element_by_name(<span class="string">'username'</span>).send_keys(<span class="string">''</span>)  <span class="comment"># 账号</span></div><div class="line">driver.find_element_by_name(<span class="string">'password'</span>).send_keys(<span class="string">''</span>)  <span class="comment"># 密码</span></div><div class="line">driver.find_element_by_xpath(<span class="string">'//*[@id="login-form"]/div[3]/div[1]/button'</span>).click()</div><div class="line">driver.<span class="built_in">close</span>()</div></pre></td></tr></table></figure></p><p>主要使用了python的selenium模块，代码很简单。</p><h3 id="第二种思路"><a href="#第二种思路" class="headerlink" title="第二种思路"></a>第二种思路</h3><p>这种思路也很简单，就是没隔3分钟请求一下网络(百度的状态码)，这样就不会断网，为了节省电脑资源，每到凌晨3点，就停止运行，或者执行关机命令也行：</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">import</span> <span class="built_in">time</span></div><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line">url = <span class="string">'http://www.baidu.com'</span></div><div class="line"></div><div class="line"><span class="keyword">while</span> True:</div><div class="line">    status = requests.get(url).status_code</div><div class="line">    <span class="keyword">if</span> status <span class="keyword">is</span> <span class="number">200</span>:</div><div class="line">        <span class="built_in">time</span>.sleep(<span class="number">180</span>)</div><div class="line">        <span class="keyword">if</span> <span class="built_in">time</span>.strftime(<span class="string">'%H'</span>) == <span class="string">'03'</span>:</div><div class="line">            <span class="built_in">time</span>.sleep(<span class="number">6</span> * <span class="number">60</span> * <span class="number">60</span>)</div><div class="line">            #os.popen(<span class="string">'shutdown -h now'</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        break</div></pre></td></tr></table></figure><h3 id="最后一步"><a href="#最后一步" class="headerlink" title="最后一步"></a>最后一步</h3><p>最后下载上面的安卓客户端，配置IP地址，端口，密码。再连上宿舍区的网络，打开代理，就可以实现在宿舍区跳到办公区进行上网了。用到了一点点简单的计算机网络的知识，整体思路也不难。欢迎留言与我交流。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;好的学校都是相同的，垃圾的学校垃圾到方方面面。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;学校的校园网分宿舍区和办公区，办公区是老师待的地方(免费, 不限速),同学上课的时候会在，宿舍区是收费的(30-49元一个月，4M网络，不能开共享)。&lt;/p&gt;
      
    
    </summary>
    
      <category term="python" scheme="https://www.bbiao.me/categories/python/"/>
    
    
      <category term="python" scheme="https://www.bbiao.me/tags/python/"/>
    
      <category term="校园网" scheme="https://www.bbiao.me/tags/%E6%A0%A1%E5%9B%AD%E7%BD%91/"/>
    
      <category term="shadowsocks" scheme="https://www.bbiao.me/tags/shadowsocks/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy的学习</title>
    <link href="https://www.bbiao.me/2018/03/22/Scrapy%E7%9A%84%E5%AD%A6%E4%B9%A0/"/>
    <id>https://www.bbiao.me/2018/03/22/Scrapy的学习/</id>
    <published>2018-03-22T13:57:52.000Z</published>
    <updated>2018-03-22T14:06:19.715Z</updated>
    
    <content type="html"><![CDATA[<p>Scrapy 是一个爬虫的框架，十分的强大，前前后后也写了很多的爬虫了，这里打算学习一下这个框架，刚开始学，感觉还是有点难度的，这里简单记录一下，以后慢慢补充。</p><h2 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h2><p>看一下scrapy原理图:</p><p><img src="https://i.loli.net/2018/03/22/5ab3b756d90b2.png" alt="1"></p><p>Scrapy Engine: 这是引擎，负责Spiders、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等等！（像不像人的身体？）</p><p>Scheduler(调度器): 它负责接受引擎发送过来的requests请求，并按照一定的方式进行整理排列，入队、并等待Scrapy Engine(引擎)来请求时，交给引擎。</p><p>Downloader（下载器）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spiders来处理，</p><p>Spiders：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)，</p><p>Item Pipeline：它负责处理Spiders中获取到的Item，并进行处理，比如去重，持久化存储（存数据库，写入文件，总之就是保存数据用的）</p><p>Downloader Middlewares（下载中间件）：你可以当作是一个可以自定义扩展下载功能的组件</p><p>Spider Middlewares（Spider中间件）：你可以理解为是一个可以自定扩展和操作引擎和Spiders中间‘通信‘的功能组件（比如进入Spiders的Responses;和从Spiders出去的Requests）</p><h3 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h3><p>数据在整个Scrapy的流向：</p><p>程序运行的时候，</p><p>引擎：Hi！Spider, 你要处理哪一个网站？</p><p>Spiders：我要处理23wx.com</p><p>引擎：你把第一个需要的处理的URL给我吧。</p><p>Spiders：给你第一个URL是XXXXXXX.com</p><p>引擎：Hi！调度器，我这有request你帮我排序入队一下。</p><p>调度器：好的，正在处理你等一下。</p><p>引擎：Hi！调度器，把你处理好的request给我，</p><p>调度器：给你，这是我处理好的request</p><p>引擎：Hi！下载器，你按照下载中间件的设置帮我下载一下这个request</p><p>下载器：好的！给你，这是下载好的东西。（如果失败：不好意思，这个request下载失败，然后引擎告诉调度器，这个request下载失败了，你记录一下，我们待会儿再下载。）</p><p>引擎：Hi！Spiders，这是下载好的东西，并且已经按照Spider中间件处理过了，你处理一下（注意！这儿responses默认是交给def parse这个函数处理的）</p><p>Spiders：（处理完毕数据之后对于需要跟进的URL），Hi！引擎，这是我需要跟进的URL，将它的responses交给函数 def  xxxx(self, responses)处理。还有这是我获取到的Item。</p><p>引擎：Hi ！Item Pipeline 我这儿有个item你帮我处理一下！调度器！这是我需要的URL你帮我处理下。然后从第四步开始循环，直到获取到你需要的信息，</p><p>注意！只有当调度器中不存在任何request了，整个程序才会停止，（也就是说，对于下载失败的ＵＲＬ，Scrapy会重新下载。）</p><p>以上就是Scrapy整个流程了。</p><font color="red">注: 内容参考自<a href="https://cuiqingcai.com/3472.html" target="_blank" rel="external">https://cuiqingcai.com/3472.html</a>, 感觉写的不错，特此记录一下。</font>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Scrapy 是一个爬虫的框架，十分的强大，前前后后也写了很多的爬虫了，这里打算学习一下这个框架，刚开始学，感觉还是有点难度的，这里简单记录一下，以后慢慢补充。&lt;/p&gt;
&lt;h2 id=&quot;Scrapy&quot;&gt;&lt;a href=&quot;#Scrapy&quot; class=&quot;headerlink&quot;
      
    
    </summary>
    
      <category term="python3" scheme="https://www.bbiao.me/categories/python3/"/>
    
    
      <category term="爬虫" scheme="https://www.bbiao.me/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="框架" scheme="https://www.bbiao.me/tags/%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
  <entry>
    <title>毛概模拟练习脚本</title>
    <link href="https://www.bbiao.me/2018/03/13/%E6%AF%9B%E6%A6%82%E6%A8%A1%E6%8B%9F%E7%BB%83%E4%B9%A0%E8%84%9A%E6%9C%AC/"/>
    <id>https://www.bbiao.me/2018/03/13/毛概模拟练习脚本/</id>
    <published>2018-03-13T13:49:12.000Z</published>
    <updated>2018-03-14T04:42:54.390Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>学生时代有很多的课(感兴趣的、不感兴趣的)，由于我本人所在的大学并不怎么好，无论老师的讲课方式，还是学校的管理方式，都不是我喜欢的方式，特别是我去中科大交流一年回来之后，没有比较就没有伤害，去了之后才知道什么是真正的大学。<br>╮(╯▽╰)╭  ，不说这些了，说了也回不去了，还是踏踏实实的面对现实吧。</p><p>由于我个人是计算机专业的，所以对历史课这些需要背的文科性质的科目都不是很感冒。但是这又是必修的课，又没办法。很多事都不是我们能够改变的，既然不能改变，那就学着适应。</p><p>学校可能也考虑到学生学习这些课的无奈，所以在学期结束之前，就是考试之前，老师会给一个题库，考题几乎都是从题库中出的，只需要把题库中的题目全都背会，那样拿个高分是完全没有问题的。</p><p>但是作为一名程序员，操作能力肯定比阅读能力要高，所以我打算把题库做成程序的形式，可以做成应答式的。这样也不用背，只需要做几遍就可以记住了，这样还能检测自己哪些题目是掌握的，哪些是没掌握的。考过驾照用过驾考宝典做过题的朋友应该能够明白。</p><p>老师给的题库是PDF格式的，而且给了十二个(一章一个)。如下：<br><img src="https://i.loli.net/2018/03/13/5aa7df66be74c.png" alt="1"></p><p>打开其中一个可以看到内容如下：</p><p><img src="https://i.loli.net/2018/03/13/5aa7e1ec2d92d.png" alt="2"></p><p><img src="https://i.loli.net/2018/03/13/5aa7e224d5dd8.png" alt="3"></p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><ul><li>把pdf解析成txt文本文件</li><li>对txt文件内容进行筛选</li><li>把题目和答案相分离</li><li>代码实现</li><li>打包成可执行文件</li></ul><h2 id="逐步实现"><a href="#逐步实现" class="headerlink" title="逐步实现"></a>逐步实现</h2><h3 id="把pdf解析成txt文本文件"><a href="#把pdf解析成txt文本文件" class="headerlink" title="把pdf解析成txt文本文件"></a>把pdf解析成txt文本文件</h3><blockquote><p><u>为什么把pdf解析成txt文件？<u></u></u></p></blockquote><p>因为pdf是不允许操作的，所以把pdf格式的试题转换成可操作的文本文件。</p><p>python有一个专门解析pdf的库，可以把pdf的文件解析成txt的文件</p><p>需要的库是： pdfminer</p><p>使用pip3 install pdfminer</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> importlib</div><div class="line">importlib.reload(sys)</div><div class="line"></div><div class="line"><span class="keyword">from</span> pdfminer.pdfparser <span class="keyword">import</span> PDFParser,PDFDocument</div><div class="line"><span class="keyword">from</span> pdfminer.pdfinterp <span class="keyword">import</span> PDFResourceManager,PDFPageInterpreter</div><div class="line"><span class="keyword">from</span> pdfminer.converter <span class="keyword">import</span> PDFPageAggregator</div><div class="line"><span class="keyword">from</span> pdfminer.layout <span class="keyword">import</span> LTTextBoxHorizontal,LAParams</div><div class="line"><span class="keyword">from</span> pdfminer.pdfinterp <span class="keyword">import</span> PDFTextExtractionNotAllowed</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">13</span>):</div><div class="line">        fp = open(<span class="string">'概/'</span> + str(x) + <span class="string">'.pdf'</span>, <span class="string">"rb"</span>)</div><div class="line">        parser = PDFParser(fp)</div><div class="line">        doc = PDFDocument()</div><div class="line">        parser.set_document(doc)</div><div class="line">        doc.set_parser(parser)</div><div class="line"></div><div class="line">        doc.initialize()</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> doc.is_extractable:</div><div class="line">            <span class="keyword">raise</span> PDFTextExtractionNotAllowed</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            rsrcmgr = PDFResourceManager()</div><div class="line">            laparams = LAParams()</div><div class="line">            device = PDFPageAggregator(rsrcmgr, laparams=laparams)</div><div class="line">            interpreter = PDFPageInterpreter(rsrcmgr, device)</div><div class="line"></div><div class="line">            <span class="keyword">for</span> page <span class="keyword">in</span> doc.get_pages():</div><div class="line">                interpreter.process_page(page)</div><div class="line">                layout = device.get_result()</div><div class="line">                <span class="keyword">for</span> x <span class="keyword">in</span> layout:</div><div class="line">                    <span class="keyword">if</span>(isinstance(x, LTTextBoxHorizontal)):</div><div class="line">                        <span class="keyword">with</span> open(<span class="string">r'概/1.txt'</span>, <span class="string">'a'</span>) <span class="keyword">as</span> f:</div><div class="line">                            results = x.get_text()</div><div class="line">                            print(results)</div><div class="line">                            f.write(results + <span class="string">'\n'</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    parse()</div></pre></td></tr></table></figure><h3 id="对txt文件内容进行筛选"><a href="#对txt文件内容进行筛选" class="headerlink" title="对txt文件内容进行筛选"></a>对txt文件内容进行筛选</h3><p>使用上面的代码就可以把pdf的文件成功转成txt格式，并且都保存到1.txt文件中，这样我们就把12个pdf文件，成功整合成一个txt文件(再次表示，python就是强大)，如下：<br><img src="https://i.loli.net/2018/03/13/5aa7e866545c6.png" alt="4"></p><p>可以看到我们已经把pdf的文件转变成txt了，但是目前的数据还不规整，比如 每一章的标题，题目类型，以及空格以及题目的换行等等都不是我们想要的，我们想要的只有题目、选项、以及答案。可以使用文本编辑器的查找替换功能就可以实现：</p><p><img src="https://i.loli.net/2018/03/13/5aa7eae4dee13.png" alt="5"></p><p>可以看到使用正则表达式可以很简单的实现我们的目的，其他的筛选方式就不再细说了。关于pdfminer库的使用，感兴趣的可以自己百度了解一下。</p><h3 id="把题目和答案相分离"><a href="#把题目和答案相分离" class="headerlink" title="把题目和答案相分离"></a>把题目和答案相分离</h3><p>最终我们把数据清理成下图这样，我们把单选题、多选题和判断题分开了。并把其对应的答案也分开了：</p><p><img src="https://i.loli.net/2018/03/13/5aa7ec034ad8d.png" alt="6"></p><p><img src="https://i.loli.net/2018/03/13/5aa7ed0dc6d8e.png" alt="7"></p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>先看一下，最终的效果(Windows下的可执行文件的效果)：</p><p><img src="https://i.loli.net/2018/03/13/5aa7eed952f88.png" alt="8"></p><p><img src="https://i.loli.net/2018/03/13/5aa7eee6f0ecb.png" alt="9"></p><p>Linux下的效果如下：</p><p><img src="https://i.loli.net/2018/03/13/5aa7ef90aa028.png" alt="10"></p><p>代码中实现了如下细节:</p><ul><li>Windows下高亮(正确显示绿色、错误显示红色)</li><li>题目随机显示</li><li>回答之后显示题目所属章节(用于比对，查找具体知识点)</li><li>统计正确率</li><li>做对的题目不再显示，错误题目循环显示</li><li>答案大小写不敏感</li><li>错误输出异常处理</li></ul><p>Windows下高亮:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> ctypes</div><div class="line"></div><div class="line">STD_INPUT_HANDLE = <span class="number">-10</span></div><div class="line">STD_OUTPUT_HANDLE= <span class="number">-11</span></div><div class="line">STD_ERROR_HANDLE = <span class="number">-12</span></div><div class="line"></div><div class="line">FOREGROUND_GREEN  = <span class="number">0x02</span></div><div class="line">FOREGROUND_RED = <span class="number">0x04</span></div><div class="line"></div><div class="line">std_out_handle = ctypes.windll.kernel32.GetStdHandle(STD_OUTPUT_HANDLE)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_cmd_color</span><span class="params">(color, handle=std_out_handle)</span>:</span></div><div class="line">  bool = ctypes.windll.kernel32.SetConsoleTextAttribute(handle, color)</div><div class="line">  <span class="keyword">return</span> bool</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_color</span><span class="params">()</span>:</span>  <span class="comment"># 设置cmd颜色种类</span></div><div class="line">  set_cmd_color(FOREGROUND_RED | FOREGROUND_GREEN)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_color_text</span><span class="params">(color, text)</span>:</span>  <span class="comment"># 颜色输出的函数</span></div><div class="line">  set_cmd_color(color)</div><div class="line">  sys.stdout.write(<span class="string">'%s\n'</span> % text)</div><div class="line">  reset_color()</div><div class="line"></div><div class="line">print_color_text(FROEGROUND_GREEN, <span class="string">'你好'</span>)</div></pre></td></tr></table></figure></p><p>题目随机显示、答案大小写不敏感<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">import <span class="built_in">random</span></div><div class="line"></div><div class="line">seed = <span class="built_in">random</span>.randint(<span class="number">0</span>, <span class="built_in">len</span>(content)<span class="number">-1</span>)  # content是题目所放列表，也就是随机数范围在<span class="number">1</span>到最后一题</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> content[seed]:</div><div class="line">    <span class="built_in">print</span>(i)</div><div class="line">    <span class="keyword">if</span> i[<span class="number">0</span>] is <span class="string">'D'</span>:</div><div class="line">        ans=<span class="built_in">input</span>(<span class="string">'请输入您的答案(输入Q退出):'</span>)</div><div class="line">        <span class="keyword">if</span> ans.<span class="built_in">upper</span>() != answer[seed]:</div><div class="line">            <span class="built_in">print</span>(<span class="string">'回答错误'</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="built_in">print</span>(<span class="string">'回答正确'</span>)</div></pre></td></tr></table></figure></p><p>做对的题目不再显示，错误题目循环显示:<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">wrong = []</div><div class="line"><span class="literal">right</span> = []</div><div class="line"></div><div class="line">right_content = [<span class="number">0</span>]</div><div class="line">wrong_content = [<span class="built_in">len</span>(content)]</div><div class="line"></div><div class="line"><span class="keyword">if</span> seed <span class="keyword">not</span> <span class="keyword">in</span> <span class="literal">right</span>:</div><div class="line">    <span class="keyword">if</span> ans.<span class="built_in">upper</span>() = answer[seed]:</div><div class="line">        <span class="literal">right</span>.append(seed)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        pass</div></pre></td></tr></table></figure></p><h3 id="生成可执行文件"><a href="#生成可执行文件" class="headerlink" title="生成可执行文件"></a>生成可执行文件</h3><p>考虑到惠及大多数同学，所以打算把程序打包成可执行文件，这样大家在电脑上直接点击就可以直接用了，不需要每个人都需要安装python环境。<br>打包需要在Windows环境下，因为在Linux下打包不会生成可执行文件，需要用到的包是<strong>pyinstaller</strong>.</p><p>使用命令：</p><blockquote><p>pyinstaller -F -w -i icon.ico maogai.py     # 关于pyinstall的使用已经参数不懂得可以自己了解一下。</p></blockquote><font color="red">注：打包的时候需要把题目整合进一个py文件中，因为如果题目放在文件中，在打包的时候路径问题不好解决。</font><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><p>完整代码在我的github上。</p><ul><li>包括Linux版</li><li>Windows版</li><li>题目</li><li>可执行文件</li></ul><p><a href="https://github.com/Tactful-biao/Scriptlet" target="_blank" rel="external">项目地址</a></p><blockquote><p>如果你感觉文章对你有所帮助，文章下方有打赏按钮，请您随意打赏，以支持作者继续创作。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;学生时代有很多的课(感兴趣的、不感兴趣的)，由于我本人所在的大学并不怎么好，无论老师的讲课方式，还是学校的管理方式，都不是我喜欢的方式，特别
      
    
    </summary>
    
      <category term="脚本" scheme="https://www.bbiao.me/categories/%E8%84%9A%E6%9C%AC/"/>
    
    
      <category term="python" scheme="https://www.bbiao.me/tags/python/"/>
    
      <category term="脚本" scheme="https://www.bbiao.me/tags/%E8%84%9A%E6%9C%AC/"/>
    
  </entry>
  
  <entry>
    <title>查询脚本</title>
    <link href="https://www.bbiao.me/2018/03/09/%E6%9F%A5%E8%AF%A2%E8%84%9A%E6%9C%AC/"/>
    <id>https://www.bbiao.me/2018/03/09/查询脚本/</id>
    <published>2018-03-09T12:42:47.000Z</published>
    <updated>2018-03-15T14:30:23.619Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>前段时间和同学一块接了一个简单的小项目，是做查询。要求是通过网页的方式进行查询。项目不是很难，于是我们就接下了，他负责前端和后端，我负责获取数据，于是就有了这些脚本。用python写脚本真的不能再方便了。</p><h2 id="两种形式"><a href="#两种形式" class="headerlink" title="两种形式"></a>两种形式</h2><p>主要是做查询，其实数据还是从别人那里去拿，那客户为什么不直接去别人那里去拿呢？是这样的，比如说，你想查英语四六级成绩，你就需要到四六级查询的网站去查，你想要查询计算机等级成绩，就需要到计算机等级网站去查询，客户要做的就是把各种查询都集中到他的网站上去，这样无论你是查询英语四六级还是查询计算机等级，都可以到他那里查询就可以了。而且他还可以把用户的成绩保存到本地，进行分析。 我们这里只记录技术。既然做查询，就会遇到两种情况，一种是带验证码的查询，一种是不带验证码的，不带验证码的很简单，只需要找到查询成绩的源链接，然后把用户名之类的数据发送个它就可以拿到数据了，带验证码的就比较复杂了，验证码也分为两种，一种是验证码比较简单的，可以通过python的图片识图库，或者使用百度识图的api进行识别，如果验证码比较复杂，那就只能把验证码爬取下来，然后人工识别，然后把验证码输入，再把验证码和用户名之类的账号发送过去就可以拿到数据了。</p><h3 id="不带验证码"><a href="#不带验证码" class="headerlink" title="不带验证码"></a>不带验证码</h3><p>之前<a href="http://www.chsi.com.cn/cet/" target="_blank" rel="external">英语四六级查询的网址</a> 是不需要验证码的，但是这一次刚出的四六级成绩就需要验证码了。。。，这里不需要验证码的就不在介绍了，这种情况十分简单，只需要找到获取数据的链接，然后构造请求就可以了。而且现在绝大多数的查询网站都加入了验证码，目的就是防止爬虫的爬取。</p><h3 id="带验证码"><a href="#带验证码" class="headerlink" title="带验证码"></a>带验证码</h3><p>这里，我们着重介绍一下，带验证码的情况，这里我们以大学英语四六级的查询网站为例子进行介绍：<br>网站查询页面如图:<br><img src="https://i.loli.net/2018/03/15/5aaa814abb66b.png" alt="2"></p><p>这里我们使用测试<font color="blue">账号 440550172213914 姓名: 叶晖</font></p><p>  <font color="red">注：账号来自网络，如有侵权，请联系博主邮箱(biao@bbiao.me)</font></p><p>可以通过Chrome浏览器F12–&gt;Network 查看到含有成绩数据的链接，如下：</p><p><img src="https://i.loli.net/2018/03/15/5aaa81c36b7be.png" alt="3"></p><p>我们来分析一下这个链接的请求头部，浏览器的请求也是通过请求头部，我们只需要把头部模拟出来，我们也可以拿到数据。<br>我们可以看到请求的头部信息如下：</p><p><img src="https://i.loli.net/2018/03/15/5aaa8206bc777.png" alt="4"></p><p>可以看到请求参数有三个，分别是：</p><ul><li>zkzh     准考证号</li><li>xm       姓名</li><li>yzm      验证码</li></ul><p>在请求的时候我们只需要带上这三个参数即可。准考证号和姓名我们要查询成绩的话肯定知道，但是验证码每次都不一样，该怎么处理呢？</p><p>我们先来观察一下验证码的信息，再看一下得到成绩的链接的信息，我们对比一下：</p><p><img src="https://i.loli.net/2018/03/15/5aaa82960ffc5.png" alt="5"></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="attribute">JSESSIONID</span>=23E5521A52C11D78EED651E737FB9470; <span class="attribute">JSESSIONID</span>=C251FAB3C55A4C79EE66E879761D9664; <span class="attribute">acw_tc</span>=AQAAAIfwSFom6w4A15YccFVF1Cu3ZBX5; <span class="attribute">aliyungf_tc</span>=AQAAAOJW3hFvAQwAGfuKPbMBKKVJhaor; <span class="attribute">__utmt</span>=1; <span class="attribute">__utma</span>=65168252.783548276.1517454574.1520600353.1521123535.16; <span class="attribute">__utmb</span>=65168252.6.10.1521123535; <span class="attribute">__utmc</span>=65168252; <span class="attribute">__utmz</span>=65168252.1520600353.15.5.utmcsr=baidu|utmccn=(organic)|utmcmd=organic</div><div class="line">Host:www.chsi.com.cn</div></pre></td></tr></table></figure><p><img src="https://i.loli.net/2018/03/15/5aaa830dc05a7.png" alt="6"></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Cookie:<span class="attribute">JSESSIONID</span>=23E5521A52C11D78EED651E737FB9470; <span class="attribute">JSESSIONID</span>=42C04019A0FE6E3FA0F8B412FD5EC7F6; <span class="attribute">acw_tc</span>=AQAAAIfwSFom6w4A15YccFVF1Cu3ZBX5; <span class="attribute">aliyungf_tc</span>=AQAAAOJW3hFvAQwAGfuKPbMBKKVJhaor; <span class="attribute">__utmt</span>=1; <span class="attribute">__utma</span>=65168252.783548276.1517454574.1520241698.1520600353.15; <span class="attribute">__utmb</span>=65168252.12.10.1520600353; <span class="attribute">__utmc</span>=65168252; <span class="attribute">__utmz</span>=65168252.1520600353.15.5.utmcsr=baidu|utmccn=(organic)|utmcmd=organic</div></pre></td></tr></table></figure><p>上面分别是验证码的请求头部信息以及其cookies，和请求成绩的请求头部信息和cookies，可以看到他们之间的cookies是一样的，验证码和准考证号以及姓名是通过cookies连接起来的，这样每次都随机生成一张验证码，然后生成cookies，当请求成绩的时候也使用相同的cookies，这样就相当于把cookies和准考证号以及验证码等信息绑定在一起了。不明白什么是cookies的朋友可以自己去百度一下。</p><p>这样我们的解决思路就是：</p><ul><li>请求验证码，并把验证码保存到本地</li><li>把cookies保存下来</li><li>人工输入验证码</li><li>把输入的验证码以及账号信息发送给成绩查询链接</li></ul><p>经过分析可以得到，验证码的链接是: <a href="http://www.chsi.com.cn/cet/ValidatorIMG.JPG?ID=1361.864138277713" target="_blank" rel="external">http://www.chsi.com.cn/cet/ValidatorIMG.JPG?ID=1361.864138277713</a></p><p>经过测试得知，验证码链接后面的ID对应的数字是多少对验证码的请求没有影响，可以直接打开该链接，并且每次刷新都会得到不同的验证码。</p><h4 id="请求验证码"><a href="#请求验证码" class="headerlink" title="请求验证码"></a>请求验证码</h4><figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line">from PIL import Image</div><div class="line"></div><div class="line">url = <span class="string">'http://www.chsi.com.cn/cet/ValidatorIMG.JPG?ID=1361.864138277713'</span></div><div class="line">yzm = requests.<span class="built_in">get</span>(url).content</div><div class="line"></div><div class="line">with <span class="keyword">open</span>(<span class="string">'yzm.jpg'</span>, <span class="string">'wb+'</span>) <span class="keyword">as</span> jp<span class="variable">g:</span></div><div class="line">    jpg.<span class="keyword">write</span>(yzm)</div><div class="line"></div><div class="line"><span class="keyword">im</span> = Image.<span class="keyword">open</span>(<span class="string">'yzm.jpg'</span>)</div><div class="line"><span class="keyword">im</span>.show()</div></pre></td></tr></table></figure><p>通过requests库可以很简单的实现对验证码的请求，使用with open把图片保存到本地，使用PIL库中的Image函数，可以把图片打开并显示。</p><h4 id="把cookies保存下来"><a href="#把cookies保存下来" class="headerlink" title="把cookies保存下来"></a>把cookies保存下来</h4><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line"></div><div class="line">s = requests.session()</div><div class="line">url = <span class="string">'http://www.chsi.com.cn/cet/ValidatorIMG.JPG?ID=1361.864138277713'</span></div><div class="line">yzm = s.<span class="built_in">get</span>(url).content</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'yzm.jpg'</span>, <span class="string">'wb+'</span>) <span class="keyword">as</span> jpg:</div><div class="line">    jpg.<span class="built_in">write</span>(yzm)</div><div class="line"></div><div class="line">cookies = <span class="string">'__utmt=1; '</span> \</div><div class="line">         <span class="string">'__utma=65168252.468897637.1517456114.1517456114.1520232128.2; '</span> \</div><div class="line">         <span class="string">'__utmb=65168252.7.10.1520232128; '</span> \</div><div class="line">         <span class="string">'__utmc=65168252; '</span> \</div><div class="line">         <span class="string">'__utmz=65168252.1517456114.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); '</span></div><div class="line"></div><div class="line">ck = s.cookies.get_dict()</div><div class="line"><span class="keyword">for</span> key, <span class="built_in">value</span> <span class="keyword">in</span> zip(ck.<span class="built_in">keys</span>(), ck.values()):</div><div class="line">    cookies += key + <span class="string">'='</span> + <span class="built_in">value</span> + <span class="string">';'</span></div><div class="line"></div><div class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'cookies'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> ck:</div><div class="line">    ck.<span class="built_in">write</span>(cookies)</div></pre></td></tr></table></figure><p>这里我们对上面的代码稍作修改就可以了，这里我们使用了requests.session()，session是用来保持会话的，我们通过分析浏览器请求的cookies得到，cookies中包含很多的参数，其中我们定义的cookies变量中已经添加了一部分固定的参数，因为我们动态获取cookies的时候得到的是变化的参数，其中固定的参数是没有的，我们只需要把动态得到的参数也加入到cookies变量里面，就成功构造出来完整的参数了，这里我们为什么要把cookies保存到本地呢？其实不保存到本地是完全可以的，我们之所以保存到本地还是以为前面说的和同学合作的项目，因为数据要展示到网页上，用户在前台首先需要输入账号信息以及验证码信息，然后后台才能拿着这些信息去验证，也就是说这是两步进行的，第一步是先请求验证码，保存到本地，然后前端把验证码嵌入到网页中，然后用户点击查询，这时候如果想要使验证码和账号信息进行绑定就需要用到cookies，因为查询是分两步的，所以需要把cookies写入文件保存到本地，这样用户点击查询之后，后台就可以带着请求数据和保存下来的cookies去验证，获取成绩。</p><h4 id="把输入的验证码以及账号信息发送给成绩查询链接"><a href="#把输入的验证码以及账号信息发送给成绩查询链接" class="headerlink" title="把输入的验证码以及账号信息发送给成绩查询链接"></a>把输入的验证码以及账号信息发送给成绩查询链接</h4><p>验证码的问题解决了，后面就是拿着信息去获取成绩：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CET</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.s = requests.session()</div><div class="line">        self.cookies = <span class="string">'__utmt=1; '</span> \</div><div class="line">         <span class="string">'__utma=65168252.468897637.1517456114.1517456114.1520232128.2; '</span> \</div><div class="line">         <span class="string">'__utmb=65168252.7.10.1520232128; '</span> \</div><div class="line">         <span class="string">'__utmc=65168252; '</span> \</div><div class="line">         <span class="string">'__utmz=65168252.1517456114.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); '</span></div><div class="line"></div><div class="line">        self.headers = &#123;</div><div class="line">            <span class="string">'Accept'</span>: <span class="string">'*/*'</span>,</div><div class="line">            <span class="string">'Accept-Encoding'</span>: <span class="string">'gzip, deflate'</span>,</div><div class="line">            <span class="string">'Accept-Language'</span>: <span class="string">'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2'</span>,</div><div class="line">            <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>,</div><div class="line">            <span class="string">'Host'</span>: <span class="string">'www.chsi.com.cn'</span>,</div><div class="line">            <span class="string">'Referer'</span>: <span class="string">'http://www.chsi.com.cn/cet/'</span>,</div><div class="line">            <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span>,</div><div class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:58.0) Gecko/20100101 Firefox/58.0'</span></div><div class="line">        &#125;</div><div class="line">        self.get_yzm()</div><div class="line">        self.get_data()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_yzm</span><span class="params">(self)</span>:</span></div><div class="line">        yzm_url = <span class="string">'http://www.chsi.com.cn/cet/ValidatorIMG.JPG?ID=3677.4286808430857'</span></div><div class="line">        yz = self.s.get(yzm_url, headers=self.headers).content</div><div class="line">        <span class="keyword">with</span> open(<span class="string">'yzm.jpg'</span>, <span class="string">'wb+'</span>) <span class="keyword">as</span> jpg:</div><div class="line">            jpg.write(yz)</div><div class="line">        ck = self.s.cookies.get_dict()</div><div class="line">        <span class="keyword">for</span> key, value <span class="keyword">in</span> zip(ck.keys(), ck.values()):</div><div class="line">            self.cookies += key + <span class="string">"="</span> + value + <span class="string">';'</span></div><div class="line">        <span class="keyword">with</span> open(<span class="string">'cookies'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> ck:</div><div class="line">            ck.write(self.cookies)</div><div class="line">        im = Image.open(<span class="string">'yzm.jpg'</span>)</div><div class="line">        im.show()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(self)</span>:</span></div><div class="line">        datas = []</div><div class="line">        <span class="keyword">with</span> open(<span class="string">'cookies'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> ck:</div><div class="line">            cookie = ck.read()</div><div class="line">        yzm = input(<span class="string">"输入验证码:"</span>)</div><div class="line">        url_ = <span class="string">'http://www.chsi.com.cn/cet/query?'</span></div><div class="line">        params = &#123;</div><div class="line">            <span class="string">'zkzh'</span>: num,</div><div class="line">            <span class="string">'xm'</span>: name,</div><div class="line">            <span class="string">'yzm'</span>: yzm</div><div class="line">        &#125;</div><div class="line">        self.headers[<span class="string">'Cookie'</span>] = cookie</div><div class="line">        url = url_ + parse.urlencode(params)</div><div class="line">        html = requests.get(url, headers=self.headers).text</div><div class="line">        print(html)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    name = input(<span class="string">'请输入姓名: '</span>)</div><div class="line">    num = input(<span class="string">'请输入准考证号: '</span>)</div><div class="line">    cet = CET()</div></pre></td></tr></table></figure><p>这里可以获取响应的HTML数据，最后一步是对数据进行提取。</p><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CET</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.s = requests.session()</div><div class="line">        self.cookies = <span class="string">'__utmt=1; '</span> \</div><div class="line">         <span class="string">'__utma=65168252.468897637.1517456114.1517456114.1520232128.2; '</span> \</div><div class="line">         <span class="string">'__utmb=65168252.7.10.1520232128; '</span> \</div><div class="line">         <span class="string">'__utmc=65168252; '</span> \</div><div class="line">         <span class="string">'__utmz=65168252.1517456114.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); '</span></div><div class="line"></div><div class="line">        self.headers = &#123;</div><div class="line">            <span class="string">'Accept'</span>: <span class="string">'*/*'</span>,</div><div class="line">            <span class="string">'Accept-Encoding'</span>: <span class="string">'gzip, deflate'</span>,</div><div class="line">            <span class="string">'Accept-Language'</span>: <span class="string">'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2'</span>,</div><div class="line">            <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>,</div><div class="line">            <span class="string">'Host'</span>: <span class="string">'www.chsi.com.cn'</span>,</div><div class="line">            <span class="string">'Referer'</span>: <span class="string">'http://www.chsi.com.cn/cet/'</span>,</div><div class="line">            <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span>,</div><div class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:58.0) Gecko/20100101 Firefox/58.0'</span></div><div class="line">        &#125;</div><div class="line">        self.get_yzm()</div><div class="line">        self.get_data()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_yzm</span><span class="params">(self)</span>:</span></div><div class="line">        yzm_url = <span class="string">'http://www.chsi.com.cn/cet/ValidatorIMG.JPG?ID=3677.4286808430857'</span></div><div class="line">        yz = self.s.get(yzm_url, headers=self.headers).content</div><div class="line">        <span class="keyword">with</span> open(<span class="string">'yzm.jpg'</span>, <span class="string">'wb+'</span>) <span class="keyword">as</span> jpg:</div><div class="line">            jpg.write(yz)</div><div class="line">        ck = self.s.cookies.get_dict()</div><div class="line">        <span class="keyword">for</span> key, value <span class="keyword">in</span> zip(ck.keys(), ck.values()):</div><div class="line">            self.cookies += key + <span class="string">"="</span> + value + <span class="string">';'</span></div><div class="line">        <span class="keyword">with</span> open(<span class="string">'cookies'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> ck:</div><div class="line">            ck.write(self.cookies)</div><div class="line">        im = Image.open(<span class="string">'yzm.jpg'</span>)</div><div class="line">        im.show()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(self)</span>:</span></div><div class="line">        datas = []</div><div class="line">        <span class="keyword">with</span> open(<span class="string">'cookies'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> ck:</div><div class="line">            cookie = ck.read()</div><div class="line">        yzm = input(<span class="string">"输入验证码:"</span>)</div><div class="line">        url_ = <span class="string">'http://www.chsi.com.cn/cet/query?'</span></div><div class="line">        params = &#123;</div><div class="line">            <span class="string">'zkzh'</span>: num,</div><div class="line">            <span class="string">'xm'</span>: name,</div><div class="line">            <span class="string">'yzm'</span>: yzm</div><div class="line">        &#125;</div><div class="line">        self.headers[<span class="string">'Cookie'</span>] = cookie</div><div class="line">        url = url_ + parse.urlencode(params)</div><div class="line">        html = requests.get(url, headers=self.headers).text</div><div class="line">        <span class="string">'''提取数据'''</span></div><div class="line">        soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">        content = soup.find(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'m_cnt_m'</span>&#125;)</div><div class="line">        <span class="keyword">if</span> content.find(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'error alignC'</span>&#125;):</div><div class="line">            print(request.quote(<span class="string">'验证码不正确'</span>).encode(<span class="string">'utf-8'</span>))</div><div class="line"></div><div class="line">        <span class="keyword">elif</span> content.find(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'error alignC marginT20'</span>&#125;):</div><div class="line">            print(request.quote(<span class="string">'无法找到对应的分数，请确认您输入的准考证号及姓名无误。'</span>).encode(<span class="string">'utf-8'</span>))</div><div class="line"></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            result_item = content.findAll(<span class="string">"td"</span>)</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> result_item:</div><div class="line">                datas.append(i.getText())</div><div class="line"></div><div class="line">            print(<span class="string">'姓   名: '</span> + datas[<span class="number">0</span>].strip())</div><div class="line">            print(<span class="string">'学   校: '</span> + datas[<span class="number">1</span>].strip())</div><div class="line">            print(<span class="string">'考试级别: '</span> + datas[<span class="number">2</span>].strip())</div><div class="line">            print(<span class="string">'\n\t\t笔 试 成 绩\t\t\n'</span>)</div><div class="line">            print(<span class="string">'准考证号: '</span> + datas[<span class="number">3</span>].strip())</div><div class="line">            print(<span class="string">'总    分: '</span> + datas[<span class="number">4</span>].strip())</div><div class="line">            print(<span class="string">'\t\t听     力: '</span> + datas[<span class="number">6</span>].strip())</div><div class="line">            print(<span class="string">'\t\t阅     读: '</span> + datas[<span class="number">8</span>].strip())</div><div class="line">            print(<span class="string">'\t\t写作和翻译: '</span> + datas[<span class="number">10</span>].strip())</div><div class="line">            print(<span class="string">'\n\t\t口 语 成 绩\t\t\n'</span>)</div><div class="line">            print(<span class="string">'准考证号: '</span> + datas[<span class="number">11</span>].strip())</div><div class="line">            print(<span class="string">'等   级: '</span> + datas[<span class="number">12</span>].replace(<span class="string">'\r\n '</span>, <span class="string">''</span>))</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    name = input(<span class="string">'请输入姓名: '</span>)</div><div class="line">    num = input(<span class="string">'请输入准考证号: '</span>)</div><div class="line">    cet = CET()</div></pre></td></tr></table></figure><p>在<a href="https://github.com/Tactful-biao/scrapy/tree/master/%E6%9F%A5%E8%AF%A2" target="_blank" rel="external">我的GitHub</a>上还有其他的几个相关的脚本，这里就不在一一介绍了，大同小异。大家如果感兴趣的话，可以去看看，如果感觉还可以的话，给个Star哦！！</p><blockquote><p>如果你感觉文章对你有所帮助，文章下方有打赏按钮，请您随意打赏，以支持作者继续创作。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;前段时间和同学一块接了一个简单的小项目，是做查询。要求是通过网页的方式进行查询。项目不是很难，于是我们就接下了，他负责前端和后端，我负责获取
      
    
    </summary>
    
      <category term="Python" scheme="https://www.bbiao.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://www.bbiao.me/tags/Python/"/>
    
      <category term="脚本" scheme="https://www.bbiao.me/tags/%E8%84%9A%E6%9C%AC/"/>
    
  </entry>
  
  <entry>
    <title>对QQ空间数据进行分析</title>
    <link href="https://www.bbiao.me/2017/12/06/%E5%AF%B9QQ%E7%A9%BA%E9%97%B4%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90/"/>
    <id>https://www.bbiao.me/2017/12/06/对QQ空间数据进行分析/</id>
    <published>2017-12-06T10:00:56.000Z</published>
    <updated>2018-03-15T14:10:56.528Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>前几天爬取了所有QQ好友的QQ空间，数据一直在数据库里面存着，当然要拿出来瞎鸡儿分析一波了。根据爬取的内容，对如下内容进行了分析:</p><ul><li>好友发说说月份统计</li><li>好友所属星座统计</li><li>好友手机设备统计</li><li>好友发说说小时统计</li><li>对好友的好友进行统计</li></ul><p>上面这些只是分析了好友的说说和个人信息，没有对留言进行分析，但是后面关于留言有很多有意思的东西，我们一步一步来。</p><h2 id="开始分析"><a href="#开始分析" class="headerlink" title="开始分析"></a>开始分析</h2><h3 id="好友发说说月份统计"><a href="#好友发说说月份统计" class="headerlink" title="好友发说说月份统计"></a>好友发说说月份统计</h3><p>先上代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</div><div class="line">db = client[<span class="string">'QQ'</span>]</div><div class="line">table = db[<span class="string">'mood'</span>]</div><div class="line"></div><div class="line">date = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line">months = [<span class="string">'01'</span>, <span class="string">'02'</span>, <span class="string">'03'</span>, <span class="string">'04'</span>, <span class="string">'05'</span>, <span class="string">'06'</span>, <span class="string">'07'</span>, <span class="string">'08'</span>, <span class="string">'09'</span>, <span class="string">'10'</span>, <span class="string">'11'</span>, <span class="string">'12'</span>]</div><div class="line">months_han = [<span class="string">'一月份'</span>, <span class="string">'二月份'</span>, <span class="string">'三月份'</span>, <span class="string">'四月份'</span>, <span class="string">'五月份'</span>, <span class="string">'六月份'</span>, <span class="string">'七月份'</span>, <span class="string">'八月份'</span>, <span class="string">'九月份'</span>, <span class="string">'十月份'</span>, <span class="string">'十一月份'</span>, <span class="string">'十二月份'</span>]</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> table.find():</div><div class="line">    <span class="keyword">if</span> i[<span class="string">'CreateTime'</span>][<span class="number">5</span>:<span class="number">7</span>] <span class="keyword">in</span> months:</div><div class="line">        date[months.index(i[<span class="string">'CreateTime'</span>][<span class="number">5</span>:<span class="number">7</span>])] += <span class="number">1</span></div><div class="line"></div><div class="line">colors = [<span class="string">'blue'</span>, <span class="string">'green'</span>, <span class="string">'yellowgreen'</span>, <span class="string">'gold'</span>, <span class="string">'lightskyblue'</span>, <span class="string">'lightcoral'</span>,</div><div class="line">          <span class="string">'lightgray'</span>, <span class="string">'lime'</span>, <span class="string">'skyblue'</span>, <span class="string">'lightcyan'</span>, <span class="string">'red'</span>,  <span class="string">'dimgray'</span>]</div><div class="line">explode = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</div><div class="line">plt.title(<span class="string">u'QQ好友发说说月份统计'</span>)</div><div class="line">plt.pie(date, explode=explode, labels=months_han, colors=colors, autopct=<span class="string">'%1.1f%%'</span>, shadow=<span class="keyword">True</span>, startangle=<span class="number">90</span>)</div><div class="line">plt.axis(<span class="string">'equal'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><blockquote><p>相关模块:</p></blockquote><ul><li>mongodb</li><li>matplotlib</li></ul><p>因为我们的数据是存储到mongodb中的，所以我们这里需要从mongodb中读取数据，这里我们还用到了matplotlib,<br>这是python很著名的一个用于画图的模块，不了解的朋友可以自己去查一下。</p><p>代码很简单，大致思路是从数据库中读取说说的创建时间，通过切片提取月份，分别与months列表进行对比，如果匹配<br>上了对应的data位置就加一，这样就可以统计我们的好友发说说的月份都在哪了。</p><p>统计结果如下：</p><p><img src="https://i.loli.net/2018/03/15/5aaa7cf9f3f64.png" alt="1"></p><p>画的是饼图，关于饼图的画法，我希望对数据分析感兴趣的朋友可以自己去查一下，我不希望我的文章向教程一样，我说是什么样的，<br>你就认为是什么样子的，我希望你看过我写的文章能够给你带来收获和启发，而不是让大家按照代码运行一遍。</p><h3 id="好友所属星座统计"><a href="#好友所属星座统计" class="headerlink" title="好友所属星座统计"></a>好友所属星座统计</h3><p>代码如下:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">import pymongo</div><div class="line">import matplotlib<span class="selector-class">.pyplot</span> as plt</div><div class="line"></div><div class="line">client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</div><div class="line">db = client[<span class="string">'QQ'</span>]</div><div class="line"><span class="selector-tag">table</span> = db[<span class="string">'information'</span>]</div><div class="line"></div><div class="line">constellation = [<span class="string">'白羊座'</span>, <span class="string">'金牛座'</span>, <span class="string">'双子座'</span>, <span class="string">'巨蟹座'</span>, <span class="string">'狮子座'</span>, <span class="string">'处女座'</span>, <span class="string">'天秤座'</span>, <span class="string">'天蝎座'</span>, <span class="string">'射手座'</span>, <span class="string">'摩羯座'</span>, <span class="string">'水瓶座'</span>, <span class="string">'双鱼座'</span>, <span class="string">'未填写'</span>]</div><div class="line">count = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line"></div><div class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> <span class="selector-tag">table</span>.find():</div><div class="line">    <span class="keyword">if</span> <span class="number">50</span> &gt; <span class="selector-tag">i</span>[<span class="string">'age'</span>] &gt; <span class="number">10</span>:</div><div class="line">        count[constellation.index(<span class="selector-tag">i</span>[<span class="string">'constellation'</span>])] += <span class="number">1</span></div><div class="line"></div><div class="line">colors = [<span class="string">'blue'</span>, <span class="string">'green'</span>, <span class="string">'yellowgreen'</span>, <span class="string">'gold'</span>, <span class="string">'lightskyblue'</span>, <span class="string">'lightcoral'</span>,</div><div class="line">          <span class="string">'lightgray'</span>, <span class="string">'lime'</span>, <span class="string">'skyblue'</span>, <span class="string">'lightcyan'</span>, <span class="string">'red'</span>,  <span class="string">'dimgray'</span>, <span class="string">'orchid'</span>]</div><div class="line">explode = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</div><div class="line">plt.title(u<span class="string">'QQ好友星座分布图'</span>)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">()</span></span></div><div class="line">plt.pie(count, explode=explode, labels=constellation, colors=colors, autopct=<span class="string">'%1.1f%%'</span>, shadow=True, startangle=<span class="number">90</span>)</div><div class="line">plt.axis(<span class="string">'equal'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>这里的统计和上面月份的统计大同小异，这里对数据进行了简单的筛选，通过年龄进行筛选，大家知道有部分人不喜欢在空间填写自己的<br>个人档信息，但是不写的话，腾讯就默认给一个值，默认是1990-0-0，所以我们可以通过年龄来筛选掉这部分人，如果不筛选掉这样就<br>会造成数据的结果与真实结果偏移比较大。</p><p>这里提一下，explode参数可以保证画出来的结果是一个圆形，如果你想重点突出某一块数据，你可以把某一块数据在explode对应的位置<br>的0改为1即可。</p><p>效果图如下：</p><p><img src="https://i.loli.net/2018/03/15/5aaa7d1280689.png" alt="2"></p><h3 id="好友手机设备统计"><a href="#好友手机设备统计" class="headerlink" title="好友手机设备统计"></a>好友手机设备统计</h3><p>代码如下：</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot as plt</div><div class="line"></div><div class="line">client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</div><div class="line">db = client[<span class="string">'QQ'</span>]</div><div class="line">table = db[<span class="string">'mood'</span>]</div><div class="line"></div><div class="line"><span class="keyword">source</span> = []</div><div class="line">names = []</div><div class="line">devices = [<span class="string">'iPhone'</span>, <span class="string">'华为'</span>, <span class="string">'小米'</span>, <span class="string">'OPPO'</span>, <span class="string">'Samsung'</span>, <span class="string">'vivo'</span>, <span class="string">'魅族'</span>, <span class="string">'其他'</span>]</div><div class="line">device = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line"><span class="keyword">for</span> i in table.<span class="keyword">find</span>():</div><div class="line">    <span class="keyword">if</span> i[<span class="string">'CreateTime'</span>][<span class="number">0</span>:<span class="number">4</span>] == <span class="string">'2017'</span> and i[<span class="string">'source'</span>] != <span class="string">''</span>:</div><div class="line">        <span class="keyword">if</span> i[<span class="string">'name'</span>] not in names:</div><div class="line">            <span class="keyword">source</span>.<span class="keyword">append</span>(i[<span class="string">'source'</span>])</div><div class="line">            names.<span class="keyword">append</span>(i[<span class="string">'name'</span>])</div><div class="line"></div><div class="line"><span class="keyword">for</span> ii in range(len(<span class="keyword">source</span>)):</div><div class="line">    <span class="keyword">if</span> re.<span class="keyword">findall</span>(<span class="string">'.*iPhone.*'</span>, <span class="keyword">source</span>[ii]):</div><div class="line">        device[<span class="number">0</span>] += <span class="number">1</span></div><div class="line">    elif re.<span class="keyword">findall</span>(<span class="string">'.*华为.*'</span>, <span class="keyword">source</span>[ii]) or re.<span class="keyword">findall</span>(<span class="string">'.*HUAWEI.*'</span>, <span class="keyword">source</span>[ii]) or re.<span class="keyword">findall</span>(<span class="string">'.*荣耀.*'</span>, <span class="keyword">source</span>[ii]):</div><div class="line">        device[<span class="number">1</span>] += <span class="number">1</span></div><div class="line">    elif re.<span class="keyword">findall</span>(<span class="string">'.*小米.*'</span>, <span class="keyword">source</span>[ii]) or re.<span class="keyword">findall</span>(<span class="string">'.*红米.*'</span>, <span class="keyword">source</span>[ii]):</div><div class="line">        device[<span class="number">2</span>] += <span class="number">1</span></div><div class="line">    elif re.<span class="keyword">findall</span>(<span class="string">'.*OPPO.*'</span>, <span class="keyword">source</span>[ii]):</div><div class="line">        device[<span class="number">3</span>] += <span class="number">1</span></div><div class="line">    elif re.<span class="keyword">findall</span>(<span class="string">'.*Samsung.*'</span>, <span class="keyword">source</span>[ii]) or re.<span class="keyword">findall</span>(<span class="string">'.*SAMSUNG.*'</span>, <span class="keyword">source</span>[ii]) or re.<span class="keyword">findall</span>(<span class="string">'.*三星.*'</span>, <span class="keyword">source</span>[ii]):</div><div class="line">        device[<span class="number">4</span>] += <span class="number">1</span></div><div class="line">    elif re.<span class="keyword">findall</span>(<span class="string">'.*vivo.*'</span>, <span class="keyword">source</span>[ii]):</div><div class="line">        device[<span class="number">5</span>] += <span class="number">1</span></div><div class="line">    elif re.<span class="keyword">findall</span>(<span class="string">'.*魅族.*'</span>, <span class="keyword">source</span>[ii]) or re.<span class="keyword">findall</span>(<span class="string">'.*魅蓝.*'</span>, <span class="keyword">source</span>[ii]):</div><div class="line">        device[<span class="number">6</span>] += <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        device[<span class="number">7</span>] += <span class="number">1</span></div><div class="line"></div><div class="line">colors = [<span class="string">'blue'</span>, <span class="string">'green'</span>, <span class="string">'yellowgreen'</span>, <span class="string">'gold'</span>, <span class="string">'lightskyblue'</span>, <span class="string">'lightcoral'</span>,</div><div class="line">          <span class="string">'lightgray'</span>, <span class="string">'lime'</span>]</div><div class="line">explode = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</div><div class="line">plt.title(u<span class="string">'QQ好友手机设备分布图'</span>)</div><div class="line"><span class="keyword">print</span>()</div><div class="line">plt.pie(device, explode=explode, labels=devices, colors=colors, autopct=<span class="string">'%1.1f%%'</span>, shadow=<span class="keyword">True</span>, startangle=<span class="number">90</span>)</div><div class="line">plt.axis(<span class="string">'equal'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>这里对于好友手机中的大部分机型进行了统计，当然同一款手机可能有不同系列的，这里我们统归于一类产品，比如iPhone系列，小米系列等，我们使用了正则表达式，<br>原因是因为空间里面为黄钻用户提供了个性化小尾巴的功能，也就是说他们的手机设备可以随意添加内容，大家看一下就知道了：</p><p><img src="https://i.loli.net/2018/03/15/5aaa7d417c155.png" alt="3"></p><p>虽然他们可以瞎鸡儿个性，但是他们再个性也没办法把他们的手机型号给抹掉，只能添加一些描述性的文字，这样我们就可以使用正则表达式去匹配他们的设备，如果<br>他们包含我们统计的关键字，那么就计数。</p><p>效果图如下：</p><p><img src="https://i.loli.net/2018/03/15/5aaa7d5b594ba.png" alt="4"></p><h3 id="好友发说说小时统计"><a href="#好友发说说小时统计" class="headerlink" title="好友发说说小时统计"></a>好友发说说小时统计</h3><p>代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</div><div class="line">db = client[<span class="string">'QQ'</span>]</div><div class="line">table = db[<span class="string">'mood'</span>]</div><div class="line"></div><div class="line">y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line">hour_han = [<span class="string">'零点'</span>, <span class="string">'一点'</span>, <span class="string">'两点'</span>, <span class="string">'三点'</span>, <span class="string">'四点'</span>, <span class="string">'五点'</span>, <span class="string">'六点'</span>, <span class="string">'七点'</span>, <span class="string">'八点'</span>, <span class="string">'九点'</span>, <span class="string">'十点'</span>, <span class="string">'十一点'</span>, <span class="string">'十二点'</span>, <span class="string">'十三点'</span>,</div><div class="line">            <span class="string">'十四点'</span>, <span class="string">'十五点'</span>, <span class="string">'十六点'</span>, <span class="string">'十七点'</span>, <span class="string">'十八点'</span>, <span class="string">'十九点'</span>, <span class="string">'二十点'</span>, <span class="string">'二十一点'</span>, <span class="string">'二十二点'</span>, <span class="string">'二十三点'</span>]</div><div class="line">hours = [<span class="string">'00'</span>, <span class="string">'01'</span>, <span class="string">'02'</span>, <span class="string">'03'</span>, <span class="string">'04'</span>, <span class="string">'05'</span>, <span class="string">'06'</span>, <span class="string">'07'</span>, <span class="string">'08'</span>, <span class="string">'09'</span>, <span class="string">'10'</span>, <span class="string">'11'</span>, <span class="string">'12'</span>, <span class="string">'13'</span>,</div><div class="line">         <span class="string">'14'</span>, <span class="string">'15'</span>, <span class="string">'16'</span>, <span class="string">'17'</span>, <span class="string">'18'</span>, <span class="string">'19'</span>, <span class="string">'20'</span>, <span class="string">'21'</span>, <span class="string">'22'</span>, <span class="string">'23'</span>]</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> table.find():</div><div class="line">    <span class="keyword">if</span> i[<span class="string">'CreateTime'</span>][<span class="number">11</span>:<span class="number">13</span>] <span class="keyword">in</span> hours:</div><div class="line">        y[hours.index(i[<span class="string">'CreateTime'</span>][<span class="number">11</span>:<span class="number">13</span>])] += <span class="number">1</span></div><div class="line"></div><div class="line">colors = [<span class="string">'b'</span>, <span class="string">'g'</span>, <span class="string">'yellowgreen'</span>, <span class="string">'gold'</span>, <span class="string">'lightskyblue'</span>, <span class="string">'lightcoral'</span>, <span class="string">'c'</span>, <span class="string">'turquoise'</span>, <span class="string">'m'</span>, <span class="string">'gray'</span>, <span class="string">'r'</span>,</div><div class="line">          <span class="string">'lightgray'</span>, <span class="string">'lime'</span>, <span class="string">'skyblue'</span>, <span class="string">'purple'</span>, <span class="string">'peru'</span>,  <span class="string">'dimgray'</span>, <span class="string">'orange'</span>, <span class="string">'olive'</span>, <span class="string">'g'</span>, <span class="string">'wheat'</span>, <span class="string">'c'</span>, <span class="string">'y'</span>, <span class="string">'indigo'</span>]</div><div class="line"></div><div class="line">size = <span class="number">24</span></div><div class="line">x = np.arange(size)</div><div class="line">plt.title(<span class="string">u'QQ好友发说说小时统计'</span>)</div><div class="line">plt.bar(range(len(y)), y, width=<span class="number">0.8</span>, tick_label=hours, color=colors)</div><div class="line"><span class="keyword">for</span> a, b <span class="keyword">in</span> zip(x, y):</div><div class="line">    plt.text(a, b+<span class="number">0.8</span>, <span class="string">'%d条'</span> % b, ha=<span class="string">'center'</span>, va=<span class="string">'bottom'</span>, fontsize=<span class="number">10</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>这里我们引入了新的库 numpy，一个强大的科学计算的库，这里我们画的是柱状图，所以包含x轴和y轴，设置x轴的范围是24，y轴是我们统计的各小时发的说说数据</p><p>关于柱状图bar的参数详情这里也不做介绍。</p><p>效果图如下:</p><p><img src="https://i.loli.net/2018/03/15/5aaa7d6d3b4f3.png" alt="5"></p><p>当然如果你感觉柱状图不能够满足你，没关系我们看看折线图，可以清楚的看到大家在晚上十点发的<br>说说最多，其次是十一点，果然吃饱饭睡觉之前大家都喜欢玩玩手机。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">import pymongo</div><div class="line">import numpy as np</div><div class="line">import matplotlib<span class="selector-class">.pyplot</span> as plt</div><div class="line"></div><div class="line">client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</div><div class="line">db = client[<span class="string">'QQ'</span>]</div><div class="line"><span class="selector-tag">table</span> = db[<span class="string">'mood'</span>]</div><div class="line"></div><div class="line">y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line">hours = [<span class="string">'00'</span>, <span class="string">'01'</span>, <span class="string">'02'</span>, <span class="string">'03'</span>, <span class="string">'04'</span>, <span class="string">'05'</span>, <span class="string">'06'</span>, <span class="string">'07'</span>, <span class="string">'08'</span>, <span class="string">'09'</span>, <span class="string">'10'</span>, <span class="string">'11'</span>, <span class="string">'12'</span>, <span class="string">'13'</span>,</div><div class="line">         <span class="string">'14'</span>, <span class="string">'15'</span>, <span class="string">'16'</span>, <span class="string">'17'</span>, <span class="string">'18'</span>, <span class="string">'19'</span>, <span class="string">'20'</span>, <span class="string">'21'</span>, <span class="string">'22'</span>, <span class="string">'23'</span>]</div><div class="line"><span class="selector-tag">a</span> = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>]</div><div class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> <span class="selector-tag">table</span>.find():</div><div class="line">    <span class="keyword">if</span> <span class="selector-tag">i</span>[<span class="string">'CreateTime'</span>][<span class="number">11</span>:<span class="number">13</span>] <span class="keyword">in</span> hours:</div><div class="line">        y[hours.index(<span class="selector-tag">i</span>[<span class="string">'CreateTime'</span>][<span class="number">11</span>:<span class="number">13</span>])] += <span class="number">1</span></div><div class="line"></div><div class="line">plt.title(u<span class="string">'QQ好友发说说小时统计'</span>)</div><div class="line">plt.xlabel(<span class="string">'时间(h)'</span>)</div><div class="line">plt.ylabel(<span class="string">'说说条数'</span>)</div><div class="line">plt.plot(hours, y, <span class="string">'b'</span>)</div><div class="line">plt.xticks(<span class="selector-tag">a</span>, hours, rotation=<span class="number">0</span>)</div><div class="line">plt.grid()</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>效果图如下:</p><p><img src="https://i.loli.net/2018/03/15/5aaa7d7f7cbde.png" alt="6"></p><h3 id="对好友的好友进行统计"><a href="#对好友的好友进行统计" class="headerlink" title="对好友的好友进行统计"></a>对好友的好友进行统计</h3><p>下面才是有意思的地方，我们可以对好友的好友进行统计，这也是我爬取点赞的人的原因。<br>这里我们对好友的QQ关系进行简单的整理，我们可以从他们的留言和点赞的人中提取它们的好友关系，代码如下：</p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">import pymongo</div><div class="line">import <span class="built_in">random</span></div><div class="line"></div><div class="line">client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</div><div class="line">db = client[<span class="string">'QQ'</span>]</div><div class="line">table = db[<span class="string">'mood'</span>]</div><div class="line">board = db[<span class="string">'board'</span>]</div><div class="line">guanxi = db[<span class="string">'guanxi'</span>]</div><div class="line"></div><div class="line">qq_nu = []</div><div class="line">def get_info():</div><div class="line">    <span class="keyword">for</span> ii <span class="built_in">in</span> table.<span class="built_in">find</span>():</div><div class="line">        <span class="keyword">if</span> ii[<span class="string">'likers'</span>] != <span class="number">0</span>:</div><div class="line">            <span class="keyword">for</span> iii <span class="built_in">in</span> range(len(ii[<span class="string">'likers'</span>])):</div><div class="line">                <span class="keyword">if</span> <span class="built_in">str</span>(ii[<span class="string">'likers'</span>][iii][<span class="string">'fuin'</span>]) <span class="built_in">not</span> <span class="built_in">in</span> qq_nu:</div><div class="line">                    qq_nu.<span class="built_in">append</span>(<span class="built_in">str</span>(ii[<span class="string">'likers'</span>][iii][<span class="string">'fuin'</span>]))</div><div class="line">                    data = &#123;</div><div class="line">                        <span class="string">'_id'</span>: <span class="built_in">str</span>(ii[<span class="string">'likers'</span>][iii][<span class="string">'fuin'</span>]) + <span class="built_in">str</span>(<span class="built_in">random</span>.<span class="built_in">random</span>() * <span class="number">10</span>).replace(<span class="string">'.'</span>, <span class="string">''</span>),</div><div class="line">                        <span class="string">'owner'</span>: ii[<span class="string">'name'</span>],</div><div class="line">                        <span class="string">'name'</span>: <span class="built_in">str</span>(ii[<span class="string">'likers'</span>][iii][<span class="string">'nick'</span>]),</div><div class="line">                        <span class="string">'QQ'</span>: <span class="built_in">str</span>(ii[<span class="string">'likers'</span>][iii][<span class="string">'fuin'</span>])</div><div class="line">                    &#125;</div><div class="line">                    <span class="keyword">try</span>:</div><div class="line">                        <span class="keyword">if</span> guanxi.insert(data):</div><div class="line">                            print(<span class="string">'插入到数据库成功！'</span>)</div><div class="line">                    except:</div><div class="line">                        print(<span class="string">'插入到数据库失败！'</span>)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> x <span class="built_in">in</span> board.<span class="built_in">find</span>():</div><div class="line">        <span class="keyword">if</span> <span class="built_in">str</span>(x[<span class="string">'qq'</span>]) <span class="built_in">not</span> <span class="built_in">in</span> qq_nu:</div><div class="line">            qq_nu.<span class="built_in">append</span>(<span class="built_in">str</span>(x[<span class="string">'qq'</span>]))</div><div class="line">            datas = &#123;</div><div class="line">                <span class="string">'_id'</span>: <span class="built_in">str</span>(x[<span class="string">'qq'</span>])+<span class="string">'_'</span>+<span class="built_in">str</span>(<span class="built_in">random</span>.<span class="built_in">random</span>() * <span class="number">10</span>).replace(<span class="string">'.'</span>, <span class="string">''</span>),</div><div class="line">                <span class="string">'owner'</span>: x[<span class="string">'owner'</span>],</div><div class="line">                <span class="string">'name'</span>: <span class="built_in">str</span>(x[<span class="string">'name'</span>]),</div><div class="line">                <span class="string">'QQ'</span>: <span class="built_in">str</span>(x[<span class="string">'qq'</span>])</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">try</span>:</div><div class="line">                <span class="keyword">if</span> guanxi.insert(datas):</div><div class="line">                    print(<span class="string">'插入到数据库成功！'</span>)</div><div class="line">            except:</div><div class="line">                print(<span class="string">'插入到数据库失败！'</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> <span class="variable">__name__</span> == <span class="string">'__main__'</span>:</div><div class="line">    get_info()</div></pre></td></tr></table></figure><p>存储到数据库的结果如下:</p><p><img src="https://i.loli.net/2018/03/15/5aaa7d96da159.png" alt="7"></p><ul><li>owner 字段是自己的好友</li><li>name 字段是提取的好友的好友</li><li>QQ 字段是提取的好友的好友的QQ</li></ul><p>提取完之后我们就可以为所欲为。当然我们可以再此的基础上对好友的好友再进行爬取，前提是好友的好友没有设置<br>访问权限，我实际测试了一下，发现24000个好友的好友，只有6700个是空间没有设置权限的，到目前为止爬了130万的<br>留言(还在继续爬着)，说说还没开始爬取。爬虫是放在服务器上24小时不停的爬取的。</p><h2 id="其他玩法"><a href="#其他玩法" class="headerlink" title="其他玩法"></a>其他玩法</h2><p>当然数据的玩法有很多，就看你选择怎么玩,比如像这样:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"></div><div class="line">client = pymongo.MongoClient(host=<span class="string">'localhost'</span>, port=<span class="number">27017</span>)</div><div class="line">db = client[<span class="string">'QQ'</span>]</div><div class="line">table = db[<span class="string">'board'</span>]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_info</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">for</span> ii <span class="keyword">in</span> table.find():</div><div class="line">        <span class="keyword">if</span> <span class="string">'我爱你'</span> <span class="keyword">in</span> ii[<span class="string">'content'</span>]:</div><div class="line">            print(<span class="string">'留言板的主人:'</span>, ii[<span class="string">'owner'</span>], <span class="string">'留言者:'</span>, ii[<span class="string">'name'</span>], <span class="string">'留言时间:'</span>, ii[<span class="string">'time'</span>], <span class="string">'留言内容:'</span>, ii[<span class="string">'content'</span>])</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    get_info()</div></pre></td></tr></table></figure></p><p><img src="https://i.loli.net/2018/03/15/5aaa7da4350f0.png" alt="8"></p><p>如果我把这些都发到空间里面，估计好友都会把我拉黑了吧。。。</p><p>当然还有很多有趣的玩法，剩下的留给大家自己去探索吧, 文章就写到这里。</p><blockquote><p>如果你感觉文章对你有所帮助，文章下方有打赏按钮，您随意打赏。支持作者继续创作。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;前几天爬取了所有QQ好友的QQ空间，数据一直在数据库里面存着，当然要拿出来瞎鸡儿分析一波了。根据爬取的内容，对如下内容进行了分析:&lt;/p&gt;

      
    
    </summary>
    
      <category term="Python" scheme="https://www.bbiao.me/categories/Python/"/>
    
    
      <category term="QQ空间" scheme="https://www.bbiao.me/tags/QQ%E7%A9%BA%E9%97%B4/"/>
    
      <category term="分析" scheme="https://www.bbiao.me/tags/%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>爬取QQ空间</title>
    <link href="https://www.bbiao.me/2017/11/23/%E7%88%AC%E5%8F%96QQ%E7%A9%BA%E9%97%B4/"/>
    <id>https://www.bbiao.me/2017/11/23/爬取QQ空间/</id>
    <published>2017-11-23T08:04:00.000Z</published>
    <updated>2017-12-09T10:45:27.472Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2></blockquote><p>垂涎QQ空间已经很久了，空间里有大量的数据。如果能把它爬取下来肯定是十分有趣的。网上各大网站的爬虫都有，但是关于QQ空间的爬虫却十分的少，主要是腾讯的请求都是加了密的，腾讯光加密算法的js就写了6000行，想要构造它的请求链接比较困难。不过只要功夫深 铁杵磨成针。</p><blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2></blockquote><p>博主每次写和程序相关的文章都会把思路写出来，因为这一步是关键部分，只有思路清晰才能够把程序写好。</p><ul><li>构造请求链接</li><li>先获取所有的好友</li><li>获取说说</li><li>获取留言</li><li>获取个人信息</li><li>把数据存到数据库</li></ul><p>以上就是整个过程中的大思路，然后在逐步把大思路化解成小的具体的问题去解决。</p><blockquote><h2 id="逐步实现"><a href="#逐步实现" class="headerlink" title="逐步实现"></a>逐步实现</h2></blockquote><p>首先介绍一下所用到的环境已经一些相关的库：</p><blockquote><h3 id="环境与第三方库函数"><a href="#环境与第三方库函数" class="headerlink" title="环境与第三方库函数"></a>环境与第三方库函数</h3></blockquote><p>环境：Ubuntu 16.04<br>IDE： PyCharm<br>python 3.5</p><ul><li>selenium : 用于模拟登录获取cookies</li><li>requests : 用于保存会话</li><li>json     : 用于把数据进行清理，整合</li><li>urllib   : 用于对链接的构造</li><li>pymongo  : 用于对mongodb数据库的调用</li></ul><blockquote><h3 id="模拟登录"><a href="#模拟登录" class="headerlink" title="模拟登录"></a>模拟登录</h3></blockquote><p>首先QQ空间是需要登录的，我们利用selenium进行模拟登录，关于什么是selenium，以及它的具体怎么去使用它，在这里我就不再介绍了，这里直接开始使用。首先先分析QQ空间的登录界面(<a href="http://user.qzone.qq.com/" target="_blank" rel="external">http://user.qzone.qq.com/</a>):<br><img src="http://oxwgzg29g.bkt.clouddn.com/21.png" alt="1"></p><p>我们使用账号密码登录，通过如下代码可以进行模拟登录：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></div><div class="line">    <span class="keyword">self</span>.driver.switch_to.frame(<span class="string">'login_frame'</span>)</div><div class="line">    <span class="keyword">self</span>.driver.find_element_by_id(<span class="string">'switcher_plogin'</span>).click()</div><div class="line">    <span class="keyword">self</span>.driver.find_element_by_id(<span class="string">'u'</span>).clear()</div><div class="line">    <span class="keyword">self</span>.driver.find_element_by_id(<span class="string">'u'</span>).send_keys(<span class="keyword">self</span>.__username)</div><div class="line">    <span class="keyword">self</span>.driver.find_element_by_id(<span class="string">'p'</span>).clear()</div><div class="line">    <span class="keyword">self</span>.driver.find_element_by_id(<span class="string">'p'</span>).send_keys(<span class="keyword">self</span>.__password)</div><div class="line">    <span class="keyword">self</span>.driver.find_element_by_id(<span class="string">'login_button'</span>).click()</div><div class="line">    time.sleep(<span class="number">3</span>)</div><div class="line">    <span class="keyword">self</span>.driver.implicitly_wait(<span class="number">3</span>)</div><div class="line">    <span class="keyword">self</span>.driver.get(<span class="string">'http://user.qzone.qq.com/&#123;&#125;'</span>.format(<span class="keyword">self</span>.__username))</div><div class="line">    cookie = <span class="string">''</span></div><div class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> <span class="keyword">self</span>.driver.get_cookies()<span class="symbol">:</span></div><div class="line">        cookie += item[<span class="string">"name"</span>] + <span class="string">'='</span> + item[<span class="string">'value'</span>] + <span class="string">';'</span></div><div class="line">    <span class="keyword">self</span>.cookies = cookie</div><div class="line">    <span class="keyword">self</span>.headers[<span class="string">'Cookie'</span>] = <span class="keyword">self</span>.cookies</div><div class="line">    <span class="keyword">self</span>.driver.quit()</div></pre></td></tr></table></figure></p><p>上面的代码首先我们转换到login_frame，然后找到id为switcher_plogin的按钮进行点击，然后再找到id为u(用户名)的输入框，先进行clear操作，把已有的内容清空，然后把账号通过send_keys的方式输入进去，密码框同理，然后再找到id为login_button的按钮点击，就完成了登录。<br>过程就跟我们自己登录是一样的，我们首先打开空间，然后点击账号密码登录，在用户名处输入QQ号，在密码处输入密码，然后点击登录。<br>登录之后我们等待6秒，让网页加载一段时间，然后我们获取cookies，<br>在Chrome浏览器下，F12找到Network，就可以找到Cookies选项，我们通过循环依次把所有cookies的内容加到我们的cookies字典中，这样我们就构造好了我们的cookies。<br><img src="http://oxwgzg29g.bkt.clouddn.com/22.png" alt="2"></p><p>到这里我们的登录任务就完成了，因为我们登录的目的就是为了获取cookies，后面的操作我们就可以利用cookies来进行操作，关于什么是cookies，它的作用是什么，这里不做介绍。</p><blockquote><h3 id="构造链接"><a href="#构造链接" class="headerlink" title="构造链接"></a>构造链接</h3></blockquote><p>有过爬虫经验的朋友肯定知道，构造链接在爬虫过程中是经常会遇到的，特别是数据需要动态加载的时候。我们先来看看空间的数据都在哪里放着，首先我们先打开一个个人说说的界面：<br>该空间是我随意输入的一个QQ号，非好友,可以看到QQ空间说说的链接是：<a href="https://user.qzone.qq.com/qq账号/311：" target="_blank" rel="external">https://user.qzone.qq.com/qq账号/311：</a><br><img src="http://oxwgzg29g.bkt.clouddn.com/23.png" alt="3"></p><p>我们再看一下它的数据来源，在chrome中可以找到一个如下链接的地址，点击它可以看到它就是当前页面的说说内容，我们就可以对它直接进行请求，然后从它这里面提取我们想要的数据：<br>emotion_cgi_msglist_v6?uin=85517362&amp;ftype=0&amp;sort=0&amp;pos=0&amp;num=20</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/24.png" alt="4"></p><p>我们仔细来分析一下它的请求链接：</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">https:</span><span class="comment">//user.qzone.qq.com/proxy/domain/taotao.qq.com/cgi-bin/emotion_cgi_msglist_v6?</span></div><div class="line">uin=<span class="number">85517362</span></div><div class="line"><span class="variable">&amp;ftype</span>=<span class="number">0</span></div><div class="line"><span class="variable">&amp;sort</span>=<span class="number">0</span></div><div class="line"><span class="variable">&amp;pos</span>=<span class="number">0</span></div><div class="line"><span class="variable">&amp;num</span>=<span class="number">20</span></div><div class="line"><span class="variable">&amp;replynum</span>=<span class="number">100</span></div><div class="line"><span class="variable">&amp;g_tk</span>=<span class="number">1794161944</span></div><div class="line"><span class="variable">&amp;callback</span>=_preloadCallback<span class="variable">&amp;code_version</span>=<span class="number">1</span></div><div class="line"><span class="variable">&amp;format</span>=jsonp</div><div class="line"><span class="variable">&amp;need_private_comment</span>=<span class="number">1</span></div><div class="line"><span class="variable">&amp;qzonetoken</span>=<span class="number">40</span>cf0abf3ea3e735c2a145ab3e5cfe9f4a60861039047c2a3353a748345c814d8fdd65e31afce3e5</div><div class="line"><span class="variable">&amp;g_tk</span>=<span class="number">1794161944</span></div></pre></td></tr></table></figure><p>可以看到请求链接中包含了这么多的参数，这里面主要需要注意的是这个g_tk,一看就知道这是一个通过加密算法加密之后得到的一个值，而这里面肯定还有一个是控制翻页的参数，通过翻页之后比较参数可以发现这个控制翻页的参数是pos，每次增加20，uin是请求的对象的qq，qzonetoken这个参数可以不用，哪些参数必须要有，哪些参数可以没有，可以直接双击该js，在新的页面打开该js，你可以直接在上面链接中进行删除，如果内容没有出错，则表示该参数不是必要的。下面我们来介绍一下g_tk怎么构造：<br>在chrome的Network下的js中找到qzfl_v8_2.1.65.js，打开它，在第5600行左右找到如下内容：<br><img src="http://oxwgzg29g.bkt.clouddn.com/25.png" alt="5"></p><figure class="highlight qml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">QZFL.pluginsDefine.getACSRFToken = <span class="function"><span class="keyword">function</span>(<span class="params">url</span>) </span>&#123;</div><div class="line">  <span class="built_in">url</span> = QZFL.util.URI(<span class="built_in">url</span>);</div><div class="line">  <span class="keyword">var</span> skey;</div><div class="line">  <span class="keyword">if</span> (<span class="built_in">url</span>) &#123;</div><div class="line">    <span class="keyword">if</span> (<span class="built_in">url</span>.host &amp;&amp; <span class="built_in">url</span>.host.indexOf(<span class="string">"qzone.qq.com"</span>) &gt; <span class="number">0</span>) &#123;</div><div class="line">      <span class="title">try</span> &#123;</div><div class="line">        skey = <span class="built_in">parent</span>.QZFL.cookie.get(<span class="string">"p_skey"</span>);</div><div class="line">      &#125; <span class="keyword">catch</span> (err) &#123;</div><div class="line">        skey = QZFL.cookie.get(<span class="string">"p_skey"</span>);</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="title">else</span> &#123;</div><div class="line">      <span class="keyword">if</span> (<span class="built_in">url</span>.host &amp;&amp; <span class="built_in">url</span>.host.indexOf(<span class="string">"qq.com"</span>) &gt; <span class="number">0</span>) &#123;</div><div class="line">        skey = QZFL.cookie.get(<span class="string">"skey"</span>);</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (!skey) &#123;</div><div class="line">    skey = QZFL.cookie.get(<span class="string">"p_skey"</span>) || (QZFL.cookie.get(<span class="string">"skey"</span>) || (QZFL.cookie.get(<span class="string">"rv2"</span>) || <span class="string">""</span>));</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> <span class="built_in">arguments</span>.callee._DJB(skey);</div><div class="line">&#125;;</div><div class="line">QZFL.pluginsDefine.getACSRFToken._DJB = <span class="function"><span class="keyword">function</span>(<span class="params">str</span>) </span>&#123;</div><div class="line">  <span class="keyword">var</span> hash = <span class="number">5381</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">var</span> i = <span class="number">0</span>, len = str.length;i &lt; len;++i) &#123;</div><div class="line">    hash += (hash &lt;&lt; <span class="number">5</span>) + str.charCodeAt(i);</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> hash &amp; <span class="number">2147483647</span>;</div><div class="line">&#125;;</div></pre></td></tr></table></figure><p>这里就是获取g_tk的算法部分，转换成python版本就是：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">p_skey = self<span class="selector-class">.cookies</span>[self<span class="selector-class">.cookies</span><span class="selector-class">.find</span>(<span class="string">'p_skey='</span>) + <span class="number">7</span>: self<span class="selector-class">.cookies</span><span class="selector-class">.find</span>(<span class="string">';'</span>, self<span class="selector-class">.cookies</span><span class="selector-class">.find</span>(<span class="string">'p_skey='</span>))]</div><div class="line">h = <span class="number">5381</span></div><div class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> p_skey:</div><div class="line">    h += (h &lt;&lt; <span class="number">5</span>) + ord(i)</div><div class="line">    print(<span class="string">'g_tk'</span>, h &amp; <span class="number">2147483647</span>)</div><div class="line">    self<span class="selector-class">.g_tk</span> = h &amp; <span class="number">2147483647</span></div></pre></td></tr></table></figure></p><p>这个就是整个程序的核心部分，只要拿到了这个g_tk，其他的就不是什么困难的地方了。</p><blockquote><h3 id="获取所有好友的账号"><a href="#获取所有好友的账号" class="headerlink" title="获取所有好友的账号"></a>获取所有好友的账号</h3></blockquote><p>我们肯定不能一个一个好友输入账号进行爬取，那不是一个程序员应该干的事。</p><p>我们点击QQ空间的设置，在权限设置的地方可以看到有多少个好友可以访问你的空间，既然可以看到那就说明这个数据是加载了的，我们找到这个数据就可以了：</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/26.png" alt="6"></p><p>找到一个get_entryuinlist.cgi?开头的链接，里面包含我们的好友信息：</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/27.png" alt="7"></p><p>请求链接构造如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_friends_url</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></div><div class="line">    url = <span class="string">'https://h5.qzone.qq.com/proxy/domain/base.qzone.qq.com/cgi-bin/right/get_entryuinlist.cgi?'</span></div><div class="line">    params = &#123;</div><div class="line">        <span class="string">'uin'</span>: <span class="keyword">self</span>.__username,</div><div class="line">        <span class="string">'ver'</span>: <span class="number">1</span>,</div><div class="line">        <span class="string">'fupdate'</span>: <span class="number">1</span>,</div><div class="line">        <span class="string">'action'</span>: <span class="number">1</span>,</div><div class="line">        <span class="string">'g_tk'</span>: <span class="keyword">self</span>.g_tk</div><div class="line">    &#125;</div><div class="line">    url = url + parse.urlencode(params)</div><div class="line">    <span class="keyword">return</span> url</div></pre></td></tr></table></figure><p>获取好友信息如下：</p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">def get_friends(self):</div><div class="line">    offset = <span class="number">0</span></div><div class="line">    t = <span class="literal">True</span></div><div class="line">    url = self.get_friends_url()</div><div class="line">    friends_list = []</div><div class="line">    <span class="built_in">name</span> = []</div><div class="line">    qq_num = []</div><div class="line">    <span class="keyword">while</span> (t):</div><div class="line">        url_ = url + <span class="string">'&amp;offset='</span> + <span class="built_in">str</span>(offset)</div><div class="line">        page = self.req.get(url=url_, headers=self.headers)</div><div class="line">        <span class="keyword">if</span> <span class="string">'\"end\":1'</span> <span class="built_in">in</span> page.<span class="built_in">text</span>:</div><div class="line">            t = <span class="literal">False</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            data = page.<span class="built_in">text</span>[<span class="number">95</span>:-<span class="number">5</span>]</div><div class="line">            a = json.loads(data)</div><div class="line">            <span class="keyword">for</span> j <span class="built_in">in</span> a:</div><div class="line">                friends_list.<span class="built_in">append</span>(j)</div><div class="line">            offset += <span class="number">50</span></div><div class="line">    <span class="keyword">for</span> ii <span class="built_in">in</span> range(len(friends_list)):</div><div class="line">        <span class="built_in">name</span>.<span class="built_in">append</span>(friends_list[ii][<span class="string">'label'</span>])</div><div class="line">        qq_num.<span class="built_in">append</span>(friends_list[ii][<span class="string">'data'</span>])</div></pre></td></tr></table></figure><p>好友数据不是一下全加载出来的，所以这里有一个翻页的处理，通过offset参数，每次增加20，我们把请求的数据通过切片，再通过json规整一下，把好友的qq加入到qq_name列表中，昵称加入到name列表中。通过”end”:1判断所有的好友是否爬取完成！</p><p>在数据分析的构成中，偶然发现一个链接里面包含所有好友的账号和姓名，这样的话连翻页都不需要进行处理，但是这是我之后发现的，就没有再对它进行处理，我想这种方法肯定更加方便，有兴趣的朋友可以尝试一下，如果有更好的方法欢迎与博主进行交流。如下图：</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/29.png" alt="9"></p><blockquote><h3 id="获取所有的好友的说说"><a href="#获取所有的好友的说说" class="headerlink" title="获取所有的好友的说说"></a>获取所有的好友的说说</h3></blockquote><p>先上代码：</p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line">def get_mood(self):</div><div class="line">    url = <span class="string">'https://h5.qzone.qq.com/proxy/domain/taotao.qq.com/cgi-bin/emotion_cgi_msglist_v6?'</span></div><div class="line">    <span class="built_in">params</span> = &#123;</div><div class="line">        <span class="string">'inCharset'</span>: <span class="string">'utf-8'</span>,</div><div class="line">        <span class="string">'outCharset'</span>: <span class="string">'utf-8'</span>,</div><div class="line">        <span class="string">'sort'</span>: <span class="number">0</span>,</div><div class="line">        <span class="string">'num'</span>: <span class="number">20</span>,</div><div class="line">        <span class="string">'repllyunm'</span>: <span class="number">100</span>,</div><div class="line">        <span class="string">'cgi_host'</span>: <span class="string">'http://taotao.qq.com/cgi-bin/emotion_cgi_msglist_v6'</span>,</div><div class="line">        <span class="string">'callback'</span>: <span class="string">'_preloadCallback'</span>,</div><div class="line">        <span class="string">'code_version'</span>: <span class="number">1</span>,</div><div class="line">        <span class="string">'format'</span>: <span class="string">'jsonp'</span>,</div><div class="line">        <span class="string">'need_private_comment'</span>: <span class="number">1</span>,</div><div class="line">        <span class="string">'g_tk'</span>: self.g_tk</div><div class="line">    &#125;</div><div class="line">    url = url + parse.urlencode(<span class="built_in">params</span>)</div><div class="line">    <span class="keyword">for</span> q <span class="built_in">in</span> self.qq_num:</div><div class="line">        t1 = <span class="literal">True</span></div><div class="line">        url_ = url + <span class="string">'&amp;uin='</span> + <span class="built_in">str</span>(q)</div><div class="line">        pos = <span class="number">0</span></div><div class="line">        black = self.db[<span class="string">'black'</span>]</div><div class="line">        shuoshuo = self.db[<span class="string">'shuoshuo'</span>]</div><div class="line">        <span class="keyword">while</span>(t1):</div><div class="line">            url__ = url_ + <span class="string">'&amp;pos='</span> + <span class="built_in">str</span>(pos)</div><div class="line">            mood = self.req.get(url=url__, headers=self.headers)</div><div class="line">            <span class="keyword">if</span> <span class="string">'\"msglist\":null'</span> <span class="built_in">in</span> mood.<span class="built_in">text</span> <span class="built_in">or</span> <span class="string">"\"</span>message\<span class="string">":\"</span>对不起,主人设置了保密,您没有权限查看\<span class="string">""</span> <span class="built_in">in</span> mood.<span class="built_in">text</span>:</div><div class="line">                t1 = <span class="literal">False</span></div><div class="line">                <span class="keyword">if</span> <span class="string">'\"message\":\"对不起,主人设置了保密,您没有权限查看\"'</span> <span class="built_in">in</span> mood.<span class="built_in">text</span>:</div><div class="line">                    data = &#123;</div><div class="line">                        <span class="string">'name'</span>: self.<span class="built_in">name</span>[self.qq_num.index(q)],</div><div class="line">                        <span class="string">'qq'</span>: q</div><div class="line">                    &#125;</div><div class="line">                    black.insert(data)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                shuo = mood.<span class="built_in">text</span>[<span class="number">17</span>: -<span class="number">2</span>]</div><div class="line">                <span class="keyword">try</span>:</div><div class="line">                    js = json.loads(shuo)</div><div class="line">                    <span class="keyword">for</span> s <span class="built_in">in</span> js[<span class="string">'msglist'</span>]:</div><div class="line">                        <span class="keyword">if</span> <span class="built_in">not</span> s[<span class="string">'commentlist'</span>]:</div><div class="line">                            s[<span class="string">'commentlist'</span>] = <span class="built_in">list</span>()</div><div class="line">                        <span class="keyword">if</span> <span class="string">'pic'</span> <span class="built_in">in</span> s:</div><div class="line">                            pics = []</div><div class="line">                            <span class="keyword">for</span> j <span class="built_in">in</span> range(len(s[<span class="string">'pic'</span>])):</div><div class="line">                                pics.<span class="built_in">append</span>(s[<span class="string">'pic'</span>][j][<span class="string">'url2'</span>])</div><div class="line">                            data = &#123;</div><div class="line">                                <span class="string">'name'</span>: <span class="built_in">str</span>(s[<span class="string">'name'</span>]),</div><div class="line">                                <span class="string">'_id'</span>: <span class="built_in">str</span>(s[<span class="string">'uin'</span>]) + <span class="string">'_'</span> + <span class="built_in">str</span>(<span class="built_in">random</span>.<span class="built_in">random</span>() * <span class="number">10</span>).replace(<span class="string">'.'</span>, <span class="string">''</span>),</div><div class="line">                                <span class="string">'CreateTime'</span>: s[<span class="string">'createTime'</span>],</div><div class="line">                                <span class="string">'source'</span>: s[<span class="string">'source_name'</span>],</div><div class="line">                                <span class="string">'content'</span>: s[<span class="string">'content'</span>],</div><div class="line">                                <span class="string">'forward'</span>: int(s[<span class="string">'fwdnum'</span>]),</div><div class="line">                                <span class="string">'comment_content'</span>: <span class="built_in">str</span>(</div><div class="line">                                    [(x[<span class="string">'content'</span>], x[<span class="string">'createTime2'</span>], x[<span class="string">'name'</span>], x[<span class="string">'uin'</span>]) <span class="keyword">for</span> x <span class="built_in">in</span></div><div class="line">                                     <span class="built_in">list</span>(s[<span class="string">'commentlist'</span>])]),</div><div class="line">                                <span class="string">'comment'</span>: int(s[<span class="string">'cmtnum'</span>]),</div><div class="line">                                <span class="string">'pic'</span>: pics</div><div class="line">                            &#125;</div><div class="line">                            <span class="keyword">if</span> shuoshuo.insert(data):</div><div class="line">                                print(<span class="string">'%s 的说说写入到数据库成功！'</span> % self.<span class="built_in">name</span>[self.qq_num.index(q)])</div><div class="line">                    pos += <span class="number">20</span></div><div class="line">                except:</div><div class="line">                    print(<span class="string">'%s 的说说写入到数据库失败！'</span> % self.<span class="built_in">name</span>[self.qq_num.index(q)])</div></pre></td></tr></table></figure><p>上面的代码首先对链接进行了构造，通过pos进行翻页处理，判断一下空间是否设置了权限，如果设置了权限就把该好友的姓名和qq加入到数据库(black)中,然后退出循环，如果没有的话，把数据进行切片，再通过json规整，然后再对数据进行提取，最后把提取的数据加入到数据库中。</p><p>爬取到的原始数据如下：</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/28.png" alt="8"></p><p>每一条请求包含二十条左右的说说，对于数据的处理我不进行细致的讲解，这也没什么好讲的，只要细心一点，都是可以的，这里我把思路说一下，我先对数据进行了切片处理，把前面没用的部分去除掉，然后切片后的数据是一个符合json格式的数据，通过json.loads()方法，把数据整合一下，然后再逐个的对数据进行提取，最终得到我们想要的东西，这里我们提取了name(好友的姓名)，发说说的设备(source)，发说说的时间(create_time), 转发数, 评论数, 说说内容，说说配图链接，以及评论内容。</p><blockquote><h3 id="其他内容的爬取"><a href="#其他内容的爬取" class="headerlink" title="其他内容的爬取"></a>其他内容的爬取</h3></blockquote><p>当然还爬取了留言，以及个人信息，但是其他的内容都是大同小异，主要包括链接构造，已经数据处理，链接构造中最主要的g_tk上面已经给出了获取方法，其他的翻页也只是一个参数的改变，这里主要是给出思路，具体的还需要自己去实践，你照着我给的代码把爬虫运行一遍意义并不大，你能够从中学到东西才是最关键的。</p><blockquote><p>留言的爬取</p></blockquote><p>先来到留言板的页面，链接格式是<code>https://user.qzone.qq.com/好友的qq/334</code>，打开之后，找到get_msgb?开头的js，然后可以看到它里面包含的就是当前页面所有的留言信息，当然，我们只需要对它进行请求即可。如下图：</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/31.png" alt="11"></p><p>通过链接构造，start控制翻页，每次增加10，对数据进行处理等过程和爬取说说的过程十分的相似。</p><blockquote><p>个人信息的爬取</p></blockquote><p><img src="http://oxwgzg29g.bkt.clouddn.com/32.png" alt="12"></p><p>通过点击个人档，找到个人资料，可以找到数据的请求链接cgi_userinfo?，里面包含个人档中的所有信息，当然很多人的信息都是不全的，而且准确性也不高，这里面需要注意的地方有，性别是使用代码进行转化的，比如1表示男，2表示女，0表示其他。婚姻状态用1表示单身，2表示已婚，3表示保密，0表示未填写。星座是按照时间顺序依次用0-11表示十二个星座，-1表示未填写。</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/34.png" alt="13"></p><blockquote><h3 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h3></blockquote><p>数据是存储在MongoDB中的，表结构以及内容如下：<br><img src="http://oxwgzg29g.bkt.clouddn.com/30.png" alt="10"></p><p>black表是无权限访问的好友，board表是留言板，information是个人信息的爬取，shuoshuo当然是说说的爬取，爬取了250个好友的9万条留言，3万7千条说说没有使用多线程，用了两个多小时。</p><blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2></blockquote><p>这次对QQ空间的爬取总体来说收获还是很大的，文章中只是大体介绍了一下过程，具体细节我希望各位能够亲自动手去做一下，授人以鱼不如授人以渔，要想学好程序，不动手是肯定不行的。当然，博主水平有限，语言表达也不是很好，有些地方表达的不清楚，有错误的地方希望各位多多指教。不了解不清楚的地方可以在文章下方或者留言板中给博主留言！</p><p>另外数据爬取下来的主要目的是为了分析，不然爬取数据的意义并不大，要能够从数据中提取出有价值的东西来，这样才有意义，后面博主可能会再发一篇关于空间数据分析的文章。等有时间再写吧。</p><blockquote><h3 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h3></blockquote><p><a href="https://github.com/Tactful-biao/scrapy/tree/master/Qzone" target="_blank" rel="external">My GitHub</a><br>项目我会一直完善的，最新的爬取功能我会添加到github上，欢迎各位的star哦！</p><blockquote><p>如果你感觉文章对你有所帮助，文章下方有打赏按钮，您随意打赏。支持作者继续创作。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;/blockquote&gt;
&lt;p&gt;垂涎QQ空间已经很久了，空间里有大量的数据。如果能把它爬取下来肯定是十分有趣的。网上各
      
    
    </summary>
    
      <category term="Python" scheme="https://www.bbiao.me/categories/Python/"/>
    
    
      <category term="爬虫" scheme="https://www.bbiao.me/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="python" scheme="https://www.bbiao.me/tags/python/"/>
    
      <category term="Qzone" scheme="https://www.bbiao.me/tags/Qzone/"/>
    
  </entry>
  
  <entry>
    <title>监票小脚本</title>
    <link href="https://www.bbiao.me/2017/10/29/%E7%9B%91%E7%A5%A8%E5%B0%8F%E8%84%9A%E6%9C%AC/"/>
    <id>https://www.bbiao.me/2017/10/29/监票小脚本/</id>
    <published>2017-10-29T07:07:28.000Z</published>
    <updated>2017-10-31T14:08:29.264Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2></blockquote><p>学习的时候看到了Python发邮件的知识，就实际操作了一下，发现使用Python发邮件真的是特别的简单，就想着跟自己之前的知识结合一下，本来是想要写一个脚本用来检测自己的博客是否出现异常，如果发现出现异常就给自己发邮件。但是自己的博客是放在github上的，静态博客很难出现异常，所以单单为了写脚本而写脚本意义就不大了，正好想到前几天写了一个命令行查票小程序，就想着能不能跟这个结合起来。于是就有了现在的这篇文章，通过爬虫去不断的(可以自己设置时间)爬取12306的余票信息，如果有票就给自己发邮件就可以了。</p><blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2></blockquote><ul><li>爬取12306的余票信息</li><li>从爬取的信息中提取我们想要的票数信息</li><li>判断是否是我们想要的结果</li><li>发送邮件</li></ul><blockquote><h2 id="逐步实现"><a href="#逐步实现" class="headerlink" title="逐步实现"></a>逐步实现</h2></blockquote><p>这里我们就开始一步一步的去实现我们的目的了！</p><p>先放张效果图:<br><img src="http://oxwgzg29g.bkt.clouddn.com/Photo-2017-10-30-22-48-31_5804.PNG" alt="3"></p><blockquote><h3 id="爬取12306余票信息"><a href="#爬取12306余票信息" class="headerlink" title="爬取12306余票信息"></a>爬取12306余票信息</h3></blockquote><p>这一步可以参考我的这篇文章<a href="http://bbiao.me/2017/10/23/%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9F%A5%E8%AF%A2%E7%81%AB%E8%BD%A6%E7%A5%A8/" target="_blank" rel="external">《命令行查询火车票》</a>, 这里我们直接使用，具体的网址构造等知识不再介绍。<br>这里我们获取的数据包含所有的数据，其中包含不同的火车的座位等级，比如普通火车的硬卧，硬座，和高铁动车的商务座，一等座等情况，这里我判断了一下，如果是高级火车(G、D)就监控它的一等座和二等座，如果是普通火车(T、K、Z等)就监控它的硬座和硬卧。</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">train_date = '<span class="number">2017-11-05</span>'</div><div class="line">from_station = stations.get_telecode('北京')</div><div class="line">to_station = stations.get_telecode('上海')</div><div class="line">url = (</div><div class="line">       'https://kyfw.<span class="number">1230</span>6.cn/otn/leftTicket/query?leftTicketDTO.'</div><div class="line">       'train_date=&#123;&#125;&amp;'</div><div class="line">       'leftTicketDTO.from_station=&#123;&#125;&amp;'</div><div class="line">       'leftTicketDTO.to_station=&#123;&#125;&amp;'</div><div class="line">       'purpose_codes=ADULT'</div><div class="line">       ).format(train_date, from_station, to_station)</div><div class="line">headers = &#123;</div><div class="line">           'Referer': 'https://kyfw.<span class="number">1230</span>6.cn/otn/leftTicket/init',</div><div class="line">           'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.<span class="number">3112.11</span>3 Safari/537.36',</div><div class="line">           'X-Requested-With': 'XMLHttpRequest'</div><div class="line">           &#125;</div><div class="line">data = requests.get(url, headers=headers, verify=False).json()</div><div class="line">result = data['data']['result']</div></pre></td></tr></table></figure><p>通过上面的代码我们就可以得到出发日期为2017年11月5日从北京到上海的所有的列车的信息，返回来的数据是json格式了，下面我们就是对数据进行清洗，提取出我们想要的数据。其中result返回的是列表的形式，其中每一辆列车对应一条数据。可以通过len(result)得到该条路线上总共有几辆列车。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="attr">trains</span> = []</div><div class="line"><span class="attr">hard_sleeper</span> = []</div><div class="line"><span class="attr">hard_seat</span> = []</div><div class="line"><span class="attr">first_class_seat</span> = []</div><div class="line"><span class="attr">second_class_seat</span> = []</div><div class="line"><span class="attr">gord</span> = []</div></pre></td></tr></table></figure><p>定义六个列表，分别用来存放没亮列车的车次、硬卧、硬座、一等座、二等座，gord列表是存放列车车次的首字母的(G、D、T、Z、K)的，而且六个列表的内容是一一对应的，比如说trains[0]表示的是第一列列车的车次，那么hard_sleep[0]则表示第一列列车的硬卧信息，其他的同理。</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i in <span class="built_in">range</span>(<span class="keyword">int</span>(lenth)):</div><div class="line">    <span class="keyword">x</span> = result[i].<span class="keyword">split</span>(<span class="string">'|'</span>)</div><div class="line">    gord.<span class="keyword">append</span>(<span class="keyword">x</span>[<span class="number">3</span>][<span class="number">0</span>])</div><div class="line">    trains.<span class="keyword">append</span>(<span class="keyword">x</span>[<span class="number">3</span>])</div><div class="line">    hard_seat.<span class="keyword">append</span>(<span class="keyword">x</span>[<span class="number">29</span>])</div><div class="line">    hard_sleeper.<span class="keyword">append</span>(<span class="keyword">x</span>[<span class="number">28</span>])</div><div class="line">    first_class_seat.<span class="keyword">append</span>(<span class="keyword">x</span>[<span class="number">31</span>])</div><div class="line">    second_class_seat.<span class="keyword">append</span>(<span class="keyword">x</span>[<span class="number">30</span>])</div></pre></td></tr></table></figure><p>上面的代码中我们首先需要定义一个lenth变量来统计该线路上有几量火车，然后通过split函数把数据分割开来，把杂乱的数据分割成了一个个小的数据块，其中我们需要的内容就在单独的数据块里面，而且又是列表的形式，所以可以使用列表直接对我们需要的数据进行访问，但是这样会把所有的列车相同位置的信息一下提取出来完，这不是我想要的结果，我想要的结果是只返回单独的一辆列车的信息，因为我的最终目的是可以监控具体的某一辆列车的票数信息，因为我们买票的时候不可能把所有的列车都买上票，我们肯定是有目的性的。我们只需要监控我们需要买的那辆车就可以了。<br>所以我又把提取出来的数据依次放入上面的列表中，就相当于对数据进行了二次整理，把我们需要的数据放在一看，这样更好处理。</p><p>把上面的列表打印一下就可以看到如下结果：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[<span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'1</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'G</span>', <span class="symbol">'T</span>', <span class="symbol">'D</span>', <span class="symbol">'D</span>', <span class="symbol">'D</span>']</div><div class="line">[<span class="symbol">'G101</span>', <span class="symbol">'G5</span>', <span class="symbol">'G105</span>', <span class="symbol">'G11</span>', <span class="symbol">'G107</span>', <span class="symbol">'G111</span>', <span class="symbol">'G113</span>', <span class="symbol">'G1</span>', <span class="symbol">'G41</span>', <span class="symbol">'G115</span>', <span class="symbol">'G117</span>', <span class="symbol">'G13</span>', <span class="symbol">'G119</span>', <span class="symbol">'G121</span>', <span class="symbol">'G15</span>', <span class="symbol">'G125</span>', <span class="symbol">'G411</span>', <span class="symbol">'1461</span>', <span class="symbol">'G129</span>', <span class="symbol">'G131</span>', <span class="symbol">'G133</span>', <span class="symbol">'G135</span>', <span class="symbol">'G137</span>', <span class="symbol">'G139</span>', <span class="symbol">'G3</span>', <span class="symbol">'G43</span>', <span class="symbol">'G141</span>', <span class="symbol">'G143</span>', <span class="symbol">'G145</span>', <span class="symbol">'G17</span>', <span class="symbol">'G147</span>', <span class="symbol">'G21</span>', <span class="symbol">'G149</span>', <span class="symbol">'G23</span>', <span class="symbol">'G153</span>', <span class="symbol">'G157</span>', <span class="symbol">'G7</span>', <span class="symbol">'G9</span>', <span class="symbol">'T109</span>', <span class="symbol">'D313</span>', <span class="symbol">'D311</span>', <span class="symbol">'D321</span>']</div><div class="line">['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', <span class="symbol">'有</span>', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', <span class="symbol">'有</span>', '', '', '']</div><div class="line">['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', <span class="symbol">'有</span>', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', <span class="symbol">'无</span>', '', '', '']</div><div class="line">[<span class="symbol">'有</span>', <span class="symbol">'无</span>', <span class="symbol">'17</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'无</span>', <span class="symbol">'有</span>', <span class="symbol">'无</span>', <span class="symbol">'无</span>', <span class="symbol">'有</span>', <span class="symbol">'无</span>', <span class="symbol">'无</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'1</span>', <span class="symbol">'有</span>', <span class="symbol">'无</span>', '', <span class="symbol">'18</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'1</span>', <span class="symbol">'有</span>', <span class="symbol">'无</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'无</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'无</span>', <span class="symbol">'无</span>', '', '', '', '']</div><div class="line">[<span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', '', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'无</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'无</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'有</span>', '', <span class="symbol">'无</span>', '', <span class="symbol">'16</span>']</div></pre></td></tr></table></figure><p>第一行是我们提取的列车的标志，通过它可以判断是高级火车还是普通火车，第二行是所有的车辆的车次信息，第三行是硬座信息，第四行是硬卧，第五行是一等座，第六行是二等座。<br>为什么要判断是高级火车还是普通火车呢？<br>是因为高级火车和普通火车的座位种类不同，如果是高级火车的话是没有硬卧和硬座的，只有一等座和二等座，普通火车就没有一等座和二等座。所以这里需要判断一下，如果是高级车就监控它的一等和二等座，如果是普通火车，就监控它的硬卧和硬座。</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(int(lenth)):</div><div class="line">    <span class="keyword">if</span> (gord[i] == <span class="string">'G'</span> <span class="keyword">or</span> gord == <span class="string">'D'</span>):</div><div class="line">        <span class="literal">one</span> = (first_class_seat[i] != <span class="string">'无'</span> <span class="keyword">and</span> first_class_seat[i] != <span class="string">' '</span>)</div><div class="line">        <span class="literal">two</span> = (second_class_seat[i] != <span class="string">'无'</span> <span class="keyword">and</span> second_class_seat[i] != <span class="string">' '</span>)</div><div class="line">        <span class="keyword">while</span> (<span class="literal">one</span> <span class="keyword">or</span> <span class="literal">two</span>):</div><div class="line">            <span class="keyword">if</span> (<span class="literal">one</span>):</div><div class="line">                print(<span class="string">'车次:%s的车有一等座票！'</span> % trains[i])</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                print(<span class="string">'车次:%s的车有二等座票！'</span> % trains[i])</div><div class="line">            <span class="built_in">time</span>.sleep(<span class="number">3600</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="literal">three</span> = (hard_sleeper[i] != <span class="string">'无'</span> <span class="keyword">and</span> hard_sleeper[i] != <span class="string">' '</span>)</div><div class="line">        <span class="literal">four</span> = (hard_seat[i] != <span class="string">'无'</span> <span class="keyword">and</span> hard_seat[i] != <span class="string">' '</span>)</div><div class="line">        <span class="keyword">while</span> (<span class="literal">three</span> <span class="keyword">or</span> <span class="literal">four</span>):</div><div class="line">            <span class="keyword">if</span> (<span class="literal">three</span>):</div><div class="line">                print(<span class="string">'车次:%s的车有卧铺票！'</span> % trains[i])</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                print(<span class="string">'车次:%s的车有硬座票！'</span> % trains[i])</div><div class="line">                <span class="built_in">time</span>.sleep(<span class="number">3600</span>)</div></pre></td></tr></table></figure><p>因为我们通过lenth可以获得该路线的火车总数，通过一个循环就可以依次遍历每一辆车，通过一个if判断，如果是G或者D开头的车次，则表示该辆车属于高级车，那么就去监控它的一等和二等座，这里我们先打印一下，到最后我们就可以把print换成发送邮件的方式就可以了。<br>如果不是G或者D的话就是普通火车，那样的火车就去监控它的硬座和硬卧，这里又另外定义了one、two、three、four四个变量，这是个变量判断具体是哪种座位有票，如果满足one的条件就是一等座有票，其他的同理。</p><blockquote><h3 id="添加条件"><a href="#添加条件" class="headerlink" title="添加条件"></a>添加条件</h3></blockquote><p>我们上面的代码只能够监控到第一个有票的车辆，无论是高级车还是普通车，如果有一等、二等座或者硬卧、硬座的票就会给我们发邮件，告诉我们是那辆车有那种票，后面的车就不会再监控了，这样显然不能够满足我们的需求，谁也不能保证第一列有票的车就是我们想要的，所以我要添加一个可以监控你指定的列车的功能。</p><p>这个功能也不难，我们需要指定一个车次，然后通过trains列表的索引功能，然后把你指定的列车的名称和列表中的车次进行对比，找到它在列表中的位置，因为我们其他的信息(一等座、二等座、硬卧、硬座)是放在另外的列表中存放的，它们的位置是一一对应的，所以只要找到了该量车次的位置，就相当于知道了它所有的信息。</p><p>然后再判断一下它是高级车还是普通车，剩下的就跟上面的差不多了，这里就不在赘述。<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">appoint = trains.index(appoin)</div><div class="line"><span class="keyword">if</span> (gord[appoint] == <span class="string">'G'</span> <span class="keyword">or</span> gord == <span class="string">'D'</span>):</div><div class="line">    <span class="literal">one</span> = (first_class_seat[appoint] != <span class="string">'无'</span> <span class="keyword">and</span> first_class_seat[appoint] != <span class="string">' '</span>)</div><div class="line">    <span class="literal">two</span> = (second_class_seat[appoint] != <span class="string">'无'</span> <span class="keyword">and</span> second_class_seat[appoint] != <span class="string">' '</span>)</div><div class="line">    <span class="keyword">while</span> (<span class="literal">one</span> <span class="keyword">or</span> <span class="literal">two</span>):</div><div class="line">        <span class="keyword">if</span> (<span class="literal">one</span>):</div><div class="line">            print(<span class="string">'车次:%s的车有一等座票！'</span> % trains[appoint])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            print(<span class="string">'车次:%s的车有二等座票！'</span> % trains[appoint])</div><div class="line">        <span class="built_in">time</span>.sleep(<span class="number">3600</span>)</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    <span class="literal">three</span> = (hard_sleeper[appoint] != <span class="string">'无'</span> <span class="keyword">and</span> hard_sleeper[appoint] != <span class="string">' '</span>)</div><div class="line">    <span class="literal">four</span> = (hard_seat[appoint] != <span class="string">'无'</span> <span class="keyword">and</span> hard_seat[appoint] != <span class="string">' '</span>)</div><div class="line">    <span class="keyword">while</span> (<span class="literal">three</span> <span class="keyword">or</span> <span class="literal">four</span>):</div><div class="line">        <span class="keyword">if</span> (<span class="literal">three</span>):</div><div class="line">            print(<span class="string">'车次:%s的车有卧铺票！'</span> % trains[appoint])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            print(<span class="string">'车次:%s的车有硬座票！'</span> % trains[appoint])</div><div class="line">        <span class="built_in">time</span>.sleep(<span class="number">10</span>)</div></pre></td></tr></table></figure></p><blockquote><h3 id="使用python发送邮件"><a href="#使用python发送邮件" class="headerlink" title="使用python发送邮件"></a>使用python发送邮件</h3></blockquote><p>Python发送邮件有两个模块，一个是email。另外一个是smtplib<br>这部分内容我不打算详细的介绍了, 给大家提供一个学习的地方，不懂得朋友可以到这里去看一下，在QQ邮箱里就可以申请授权码，然后就可以进行邮件的发送：</p><p><a href="http://www.runoob.com/python/python-email.html" target="_blank" rel="external">学习python邮件发送</a></p><blockquote><h3 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h3></blockquote><p>完整的项目我已经放在我的github上了,感兴趣的朋友可以看一下：</p><p><a href="https://github.com/Tactful-biao/scrapy/tree/master/12306" target="_blank" rel="external">监票小脚本</a></p><blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2></blockquote><p>这个小程序总体来说也没有什么难度，特别是结合上面那个命令行查票小程序来看。当然代码还有很多有待改进的地方，比如代码的封装等地方还有待改进的地方，以及对所有的座位类型进行检测，还有更好的交互界面等等，这些都是可以提升的空间，现在这个就相当于是一个模型。以后再慢慢完善！</p><blockquote><p>文章若有出错的地方，请多多指正！如果你感觉文章对你有所帮助，文章下方有打赏按钮，您随意打赏。支持作者继续创作。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;/blockquote&gt;
&lt;p&gt;学习的时候看到了Python发邮件的知识，就实际操作了一下，发现使用Python发邮件
      
    
    </summary>
    
      <category term="Python" scheme="https://www.bbiao.me/categories/Python/"/>
    
    
      <category term="爬虫" scheme="https://www.bbiao.me/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="email" scheme="https://www.bbiao.me/tags/email/"/>
    
  </entry>
  
  <entry>
    <title>命令行查询火车票</title>
    <link href="https://www.bbiao.me/2017/10/23/%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9F%A5%E8%AF%A2%E7%81%AB%E8%BD%A6%E7%A5%A8/"/>
    <id>https://www.bbiao.me/2017/10/23/命令行查询火车票/</id>
    <published>2017-10-23T13:29:03.000Z</published>
    <updated>2018-05-23T01:50:18.674Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2></blockquote><p>最近一直在找一些Python相关的练手项目，只有不断的做项目才能够对知识有更加牢固的掌握。找到了一个关于查询火车票余票的小项目，就拿来练练手，由于项目比较小，所以作为一个练手项目还是十分不错的。</p><blockquote><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2></blockquote><p>编程最重要的就是思路，思路一定要清晰，在思路上花时间是值得的。</p><ul><li>从12306网站上获取它的API接口</li><li>对它的接口进行请求，对获得的数据进行处理</li><li>提取我们想要的内容</li></ul><p>上面就是我们的总的一个思路。下面就是把大的思想划分成小的功能，逐步进行实现。</p><blockquote><h2 id="逐步实现"><a href="#逐步实现" class="headerlink" title="逐步实现"></a>逐步实现</h2></blockquote><p>下面我们就一步步的来实现我们的程序。</p><blockquote><h3 id="用到的模块"><a href="#用到的模块" class="headerlink" title="用到的模块"></a>用到的模块</h3></blockquote><ul><li>requests 模块: 网络请求的库</li><li>docopt 模块: 命令行参数解释器</li><li>prettytable 模块: 将输出内容如表格方式整齐输出</li><li>colorama 模块: 命令行上色函数</li><li>setup 模块: python的构建工具</li><li>系统环境: Ubuntu 16.04</li><li>python 环境: python 3.5</li></ul><p>系统环境可以根据自己的实际来选择不同的操作系统应该都可以正确执行，上面这些模块就是我们接下来主要用到的模块，在下面的文章中，我们不仔细介绍具体模块的使用方法，不理解的可以留言或者自己百度也可以，我相信这样你能记得更牢固。</p><blockquote><h3 id="获得接口并且请求接口"><a href="#获得接口并且请求接口" class="headerlink" title="获得接口并且请求接口"></a>获得接口并且请求接口</h3></blockquote><p>12306 官网:<a href="https://kyfw.12306.cn/otn/leftTicket/init" target="_blank" rel="external">https://kyfw.12306.cn/otn/leftTicket/init</a></p><p>使用Chrome浏览器，打开12306官网，F12，选择出发地、目的地以及日期，点击查询，从下面的NetWork栏，可以看到它的数据接口是：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">https:<span class="regexp">//</span>kyfw.<span class="number">12306</span>.cn<span class="regexp">/otn/</span>leftTicket<span class="regexp">/query?leftTicketDTO.train_date=2017-11-10&amp;leftTicketDTO.from_station=BJP&amp;leftTicketDTO.to_station=SHH&amp;purpose_codes=ADULT</span></div></pre></td></tr></table></figure><p><img src="http://oxwgzg29g.bkt.clouddn.com/11.png" alt="1"></p><p>数据返回的是json格式的，每列火车对应一条json数据。只不过返回的内容十分的嘈杂，我们需要从返回的数据中提取出我们需要的内容。<br>这里我们就得到了我们需要的API。</p><p>我们暂时先不分析这个API，我们先对这个API进行请求看看返回的内容是什么。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line">from requests<span class="selector-class">.packages</span><span class="selector-class">.urllib3</span><span class="selector-class">.exceptions</span> import InsecureRequestWarning</div><div class="line"></div><div class="line">requests<span class="selector-class">.packages</span><span class="selector-class">.urllib3</span><span class="selector-class">.disable_warnings</span>(InsecureRequestWarning)</div><div class="line"></div><div class="line">url = <span class="string">'https://kyfw.12306.cn/otn/leftTicket/query?leftTicketDTO.train_date=2017-11-10&amp;leftTicketDTO.from_station=BJP&amp;leftTicketDTO.to_station=SHH&amp;purpose_codes=ADULT'</span></div><div class="line">s = requests.get(url, verify=False)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(s.json()</span></span>)</div></pre></td></tr></table></figure><p>上面的代码就是对12306的这个余票的API进行了请求，大家知道12306的证书是自己颁发的，浏览器不认。要想对它进行requests请求，必须在请求的时候加上verify=False,默认为True，这样就会忽略证书。另外我们导入的exceptions模块是处理异常信息的，如果不加上的话。会显示如下错误：/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py:852: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: <a href="https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings" target="_blank" rel="external">https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings</a><br>  InsecureRequestWarning)，加上之后就不会显示这个错误了。如果导入这个模块出现问题的话，建议把requests换成版本2.17.3</p><p>我们来看一下我们获得的数据：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"> &#123;</div><div class="line">  <span class="attr">"validateMessagesShowId"</span>: <span class="string">"_validatorMessage"</span>,</div><div class="line">  <span class="attr">"status"</span>: <span class="literal">true</span>,</div><div class="line">  <span class="attr">"httpstatus"</span>: <span class="number">200</span>,</div><div class="line">  <span class="attr">"data"</span>: &#123;</div><div class="line">    <span class="attr">"result"</span>: [</div><div class="line">      <span class="string">"aSRB3lfB6Mz%2FLafQ6X4JF1mIzb%2F5U%2F%2BDiKDDDrF4D%2F0umLHjuqsgduKSCdskDh2dI60zPanx2XWl%0AFMYzqHdjYZn3lp44kgCYPJrkVpruig2zfcLU6p6N%2BoKEE8XXNiFo5VLBNvX4oQ3SYHK6KALA4Z0C%0AyBJmqV8gyK8kZBhsAebwDpYHo0qGPEP6i6MX9oLwXFsOGtjhpZyedQ4Fk6UKqoBvjOHiu84B3GtN%0AbvLUhdL%2F%2Fi6%2F|预订|240000G1010D|G101|VNP|AOH|VNP|AOH|06:43|12:39|05:56|Y|2K4q7e00dHCBKBkBXUoLkJvNCxC5z%2Fx3dqGAR%2BKRNj6oJADq|20171110|3|P2|01|11|0|0|||||||||||有|有|20||O0M090|OM9"</span>,  <span class="string">"flag"</span>: <span class="string">"1"</span>,</div><div class="line">    <span class="string">"map"</span>: &#123;</div><div class="line">      <span class="attr">"AOH"</span>: <span class="string">"上海虹桥"</span>,</div><div class="line">      <span class="attr">"BJP"</span>: <span class="string">"北京"</span>,</div><div class="line">      <span class="attr">"VNP"</span>: <span class="string">"北京南"</span>,</div><div class="line">      <span class="attr">"SHH"</span>: <span class="string">"上海"</span></div><div class="line">    &#125;</div><div class="line">  &#125;,</div><div class="line">  <span class="string">"messages"</span>: [],</div><div class="line">  <span class="string">"validateMessages"</span>: &#123;&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>考虑到篇幅的问题，这里我只选取了一小部分数据，可以看到每一列火车余票的信息都在这样的一条数据里面，扎一看乱七八糟的一片，感觉无从下手。这时候一定要有耐心，要仔细。</p><blockquote><h3 id="分析数据，提取内容"><a href="#分析数据，提取内容" class="headerlink" title="分析数据，提取内容"></a>分析数据，提取内容</h3></blockquote><p>从上面杂乱无章的数据中提取我们想要的内容是一个体力活，需要仔细观察，认真思考。可以看到相邻的段都有一个”|”,这就是一个突破点，Python有一个split方法，可以把数据按指定条件进行分割，我们就用这个方法，把杂乱无章的数据分割成有规律的数据块。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line">from requests<span class="selector-class">.packages</span><span class="selector-class">.urllib3</span><span class="selector-class">.exceptions</span> import InsecureRequestWarning</div><div class="line"></div><div class="line">requests<span class="selector-class">.packages</span><span class="selector-class">.urllib3</span><span class="selector-class">.disable_warnings</span>(InsecureRequestWarning)</div><div class="line"></div><div class="line">url = <span class="string">'https://kyfw.12306.cn/otn/leftTicket/query?leftTicketDTO.train_date=2017-11-10&amp;leftTicketDTO.from_station=BJP&amp;leftTicketDTO.to_station=SHH&amp;purpose_codes=ADULT'</span></div><div class="line">s = requests.get(url, verify=False).json()</div><div class="line">data = s[<span class="string">'data'</span>][<span class="string">'result'</span>]</div><div class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> data:</div><div class="line">    print(<span class="selector-tag">i</span>.split(<span class="string">'|'</span>))</div></pre></td></tr></table></figure><p>我们需要的数据在json格式下，在’data’的’result’里面，因为data包含许多条数据，所以需要使用for循环进行逐条访问，通过上面的代码，就可以把杂乱无章的数据分割成小的数据块，结果如下：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[<span class="symbol">'aSRB3l6i6MvLUhdL%2F%2Fi6%2F</span>', <span class="symbol">'预订</span>', <span class="symbol">'240000G1010D</span>', <span class="symbol">'G101</span>', <span class="symbol">'VNP</span>', <span class="symbol">'AOH</span>', <span class="symbol">'VNP</span>', <span class="symbol">'AOH</span>', <span class="symbol">'06:43</span>', <span class="symbol">'12:39</span>', <span class="symbol">'05:56</span>', <span class="symbol">'Y</span>', <span class="symbol">'2K4q7e00dHCBKBkBXUoLkJvNCxC5z%2Fx3dqGAR%2BKRNj6oJADq</span>', <span class="symbol">'20171110</span>', <span class="symbol">'3</span>', <span class="symbol">'P2</span>', <span class="symbol">'01</span>', <span class="symbol">'11</span>', <span class="symbol">'0</span>', <span class="symbol">'0</span>', '', '', '', '', '', '', '', '', '', '', <span class="symbol">'有</span>', <span class="symbol">'有</span>', <span class="symbol">'20</span>', '', <span class="symbol">'O0M090</span>', <span class="symbol">'OM9</span>']</div><div class="line">[<span class="symbol">'vP6ccFiPo7CIHX9rsUDgg3y60dq5</span>', <span class="symbol">'预订</span>', <span class="symbol">'24000000G503</span>', <span class="symbol">'G5</span>', <span class="symbol">'VNP</span>', <span class="symbol">'AOH</span>', <span class="symbol">'VNP</span>', <span class="symbol">'AOH</span>', <span class="symbol">'07:00</span>', <span class="symbol">'11:34</span>', <span class="symbol">'04:34</span>', <span class="symbol">'Y</span>', <span class="symbol">'E%2BKOgellBQOuoFy3ooqgD1IiyztPgk2GP%2BrKNB8zgp8NLzS9</span>', <span class="symbol">'20171110</span>', <span class="symbol">'3</span>', <span class="symbol">'P2</span>', <span class="symbol">'01</span>', <span class="symbol">'05</span>', <span class="symbol">'0</span>', <span class="symbol">'0</span>', '', '', '', '', '', '', '', '', '', '', <span class="symbol">'有</span>', <span class="symbol">'9</span>', <span class="symbol">'9</span>', '', <span class="symbol">'O0M090</span>', <span class="symbol">'OM9</span>']</div><div class="line">[<span class="symbol">'ykb6BDSb5WqzjAheDW8FxhnuUkqLO</span>', <span class="symbol">'预订</span>', <span class="symbol">'240000G1050K</span>', <span class="symbol">'G105</span>', <span class="symbol">'VNP</span>', <span class="symbol">'AOH</span>', <span class="symbol">'VNP</span>', <span class="symbol">'AOH</span>', <span class="symbol">'07:35</span>', <span class="symbol">'13:11</span>', <span class="symbol">'05:36</span>', <span class="symbol">'Y</span>', <span class="symbol">'zZHDvkuUo0QIfwYrdqP%2FndR3PG%2FVmePNTN8vgl85mkQkHs8f</span>', <span class="symbol">'20171110</span>', <span class="symbol">'3</span>', <span class="symbol">'P4</span>', <span class="symbol">'01</span>', <span class="symbol">'09</span>', <span class="symbol">'0</span>', <span class="symbol">'0</span>', '', '', '', '', '', '', '', '', '', '', <span class="symbol">'有</span>', <span class="symbol">'19</span>', <span class="symbol">'5</span>', '', <span class="symbol">'O0M090</span>', <span class="symbol">'OM9</span>']</div></pre></td></tr></table></figure><p>考虑到篇幅我把部分数据截短了，现在再来看我们的数据是不是清晰了许多，我们需要的数据在哪里都可以清楚的看到。</p><blockquote><h3 id="分析与构造网址"><a href="#分析与构造网址" class="headerlink" title="分析与构造网址"></a>分析与构造网址</h3></blockquote><p>上面我们请求的网址是固定的，在实际应用中这样肯定是不行的，如果程序只能查询固定地方的余票信息，那么这个程序就是一个“死”的，下面我们就构造动态请求，可以根据你的需要进行请求。</p><p>首先我们来分析一下这个网址：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">https:<span class="regexp">//</span>kyfw.<span class="number">12306</span>.cn<span class="regexp">/otn/</span>leftTicket<span class="regexp">/query?leftTicketDTO.train_date=2017-11-10&amp;leftTicketDTO.from_station=BJP&amp;leftTicketDTO.to_station=SHH&amp;purpose_codes=ADULT</span></div></pre></td></tr></table></figure><p>这个网址分四部分：</p><ol><li>固定的部分:<code>https://kyfw.12306.cn/otn/leftTicket/query?leftTicketDTO.</code></li><li>出行日期部分: train_date=2017-11-10&amp;leftTicketDTO.</li><li>始发站部分: from_station=BJP&amp;leftTicketDTO.</li><li>终点站部分: to_station=SHH&amp;purpose_codes=ADULT</li></ol><p>可以把url分这四部分，我们需要动态构造的是2,3,4部分。出发日期格式很简单，主要是始发站和终点站部分，它把城市名称转化成了相应的code，我们需要知道它的转换规则，它的转换规则在它的网站上肯定是有的。我们去找一下。</p><p>果然在请求的js里面有一个stations_name 的js，打开它就是所有城市对应的code：</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/12.png" alt="3"></p><p><img src="http://oxwgzg29g.bkt.clouddn.com/13.png" alt="4"></p><p>我们写一个简单的脚本，把这些内容提取出来并保存到列表里面：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    url = <span class="string">'https://kyfw.12306.cn/otn/resources/js/framework/station_name.js?station_version=1.9018'</span></div><div class="line">    r = requests.get(url, verify=<span class="keyword">False</span>)</div><div class="line">    pattern = <span class="string">u'([\u4e00-\u9fa5]+)\|([A-Z]+)'</span></div><div class="line">    stations = dict(re.findall(pattern, r.text))</div><div class="line">    print(stations.keys())</div><div class="line">    print(stations.values())</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure></p><p>运行之后会显示两个字典，每个字典内包含一个列表一个是城市名称，另外一个是城市代码，是一一对应关系。这样我们就把城市和对应的代码提取出来了。</p><p>我们可以通过传入参数(日期，出发站，终点站)就可以实现动态查找。</p><blockquote><h3 id="docopt的使用"><a href="#docopt的使用" class="headerlink" title="docopt的使用"></a>docopt的使用</h3></blockquote><p>docopt 是Python的命令行参数解释器，具体使用方法可以参考<a href="https://www.tuicool.com/articles/36zyQnu" target="_blank" rel="external">这篇教程</a>，这里不做过多介绍。<br>可以使用该命令进行安装:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip3 <span class="keyword">install</span> docopt</div></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""Train tickets query from CLI.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Usage:</span></div><div class="line"><span class="string">  test.py [-dgktz] &lt;from&gt; &lt;to&gt; &lt;date&gt;</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Options:</span></div><div class="line"><span class="string">  -h --help    Show this screen.</span></div><div class="line"><span class="string">  -d            动车</span></div><div class="line"><span class="string">  -g            高铁</span></div><div class="line"><span class="string">  -k            快速</span></div><div class="line"><span class="string">  -t            特快</span></div><div class="line"><span class="string">  -z            直达</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">Example:</span></div><div class="line"><span class="string">  test.py -gd 北京 上海 2017-11-01</span></div><div class="line"><span class="string">"""</span></div><div class="line"><span class="keyword">from</span> docopt <span class="keyword">import</span> docopt</div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">import</span> stations</div><div class="line"><span class="keyword">from</span> requests.packages.urllib3.exceptions <span class="keyword">import</span> InsecureRequestWarning</div><div class="line"></div><div class="line">requests.packages.urllib3.disable_warnings(InsecureRequestWarning)</div><div class="line"></div><div class="line">canshu = docopt(__doc__)</div><div class="line">start = stations.get_telecode(canshu[<span class="string">'&lt;from&gt;'</span>])</div><div class="line">stop = stations.get_telecode(canshu[<span class="string">'&lt;to&gt;'</span>])</div><div class="line">url = (</div><div class="line">        <span class="string">'https://kyfw.12306.cn/otn/leftTicket/query?leftTicketDTO.'</span></div><div class="line">        <span class="string">'train_date=&#123;&#125;&amp;'</span></div><div class="line">        <span class="string">'leftTicketDTO.from_station=&#123;&#125;&amp;'</span></div><div class="line">        <span class="string">'leftTicketDTO.to_station=&#123;&#125;&amp;'</span></div><div class="line">        <span class="string">'purpose_codes=ADULT'</span></div><div class="line">    ).format(canshu[<span class="string">'&lt;date&gt;'</span>], start, stop)</div><div class="line">s = requests.get(url, verify=<span class="keyword">False</span>).json()</div><div class="line">data = s[<span class="string">'data'</span>][<span class="string">'result'</span>]</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> data:</div><div class="line">    print(i.split(<span class="string">'|'</span>))</div></pre></td></tr></table></figure><p>stations模块是我们自己定义的一个python模块，内容就是我们之前获得的城市名称以及它所对应的城市代码。</p><p>通过这种方法就可以实现动态的获取数据，上面的程序传入参数之后运行后的结果如下：</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/18.png" alt="6"></p><p>上面获得的数据虽然通过我们的分割但是还是处于待处理阶段，不过处理就十分简单了，通过Python的切片操作，直接把需要的数据通过切片的方式获取就可以了。</p><p>我们需要获取的数据有“车次 车站 时间 历时 特等座 一等座 二等座 软卧 硬卧 软座 硬座 无座”，这些内容的票数信息在上面的数据中都有，我们主需要把对应的内容提取出来就可以了。</p><blockquote><h3 id="prettytable-的使用"><a href="#prettytable-的使用" class="headerlink" title="prettytable 的使用"></a>prettytable 的使用</h3></blockquote><p>Python通过PrettyTable模块可以将输出内容如表格方式整齐地输出。</p><p>安装：<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip3 <span class="keyword">install</span> prettytable</div></pre></td></tr></table></figure></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">from prettytable import PrettyTable</div><div class="line"><span class="selector-tag">table</span> = PrettyTable([<span class="string">"animal"</span>, <span class="string">"ferocity"</span>])</div><div class="line"><span class="selector-tag">table</span>.add_row([<span class="string">"wolverine"</span>, <span class="number">100</span>])</div><div class="line"><span class="selector-tag">table</span>.add_row([<span class="string">"grizzly"</span>, <span class="number">87</span>])</div><div class="line"><span class="selector-tag">table</span>.add_row([<span class="string">"Rabbit of Caerbannog"</span>, <span class="number">110</span>])</div><div class="line"><span class="selector-tag">table</span>.add_row([<span class="string">"cat"</span>, -<span class="number">1</span>])</div><div class="line"><span class="selector-tag">table</span>.add_row([<span class="string">"platypus"</span>, <span class="number">23</span>])</div><div class="line"><span class="selector-tag">table</span>.add_row([<span class="string">"dolphin"</span>, <span class="number">63</span>])</div><div class="line"><span class="selector-tag">table</span>.add_row([<span class="string">"albatross"</span>, <span class="number">44</span>])</div><div class="line"><span class="selector-tag">table</span>.sort_key(<span class="string">"ferocity"</span>)</div><div class="line"><span class="selector-tag">table</span><span class="selector-class">.reversesort</span> = True</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(table)</span></span></div></pre></td></tr></table></figure><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">''<span class="emphasis">'效果图</span></div><div class="line"><span class="emphasis">+----------------------+----------+</span></div><div class="line"><span class="emphasis">|        animal        | ferocity |</span></div><div class="line"><span class="emphasis">+----------------------+----------+</span></div><div class="line"><span class="emphasis">| Rabbit of Caerbannog |   110    |</span></div><div class="line"><span class="emphasis">|      wolverine       |   100    |</span></div><div class="line"><span class="emphasis">|       grizzly        |    87    |</span></div><div class="line"><span class="emphasis">|       dolphin        |    63    |</span></div><div class="line"><span class="emphasis">|      albatross       |    44    |</span></div><div class="line"><span class="emphasis">|       platypus       |    23    |</span></div><div class="line"><span class="emphasis">|         cat          |    -1    |</span></div><div class="line"><span class="emphasis">+----------------------+----------+</span></div></pre></td></tr></table></figure><p>具体用法可以自己去了解一下，自己动手丰衣足食。</p><blockquote><h3 id="colorama-的使用"><a href="#colorama-的使用" class="headerlink" title="colorama 的使用"></a>colorama 的使用</h3></blockquote><p>colorama是一个python专门用来在控制台、命令行输出彩色文字的模块，可以跨平台使用。</p><p>安装:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip3 <span class="keyword">install</span> colorama</div></pre></td></tr></table></figure></p><p>可用格式常数:<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">Fore:</span> <span class="keyword">BLACK, </span>RED, GREEN, YELLOW, <span class="keyword">BLUE, </span>MAGENTA, CYAN, WHITE, RESET.</div><div class="line"><span class="keyword">Back: </span><span class="keyword">BLACK, </span>RED, GREEN, YELLOW, <span class="keyword">BLUE, </span>MAGENTA, CYAN, WHITE, RESET.</div><div class="line"><span class="symbol">Style:</span> <span class="keyword">DIM, </span><span class="keyword">NORMAL, </span><span class="keyword">BRIGHT, </span>RESET_ALL</div></pre></td></tr></table></figure></p><p>跨平台印刷彩色文本可以使用彩色光的常数简称ANSI转义序列:<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> colorama import Fore,Back,Style</div><div class="line"><span class="builtin-name">print</span> (Fore.RED + <span class="string">"some red text"</span>)</div><div class="line"><span class="builtin-name">print</span> (Back.GREEN + <span class="string">"and with a green background"</span>)</div><div class="line"><span class="builtin-name">print</span> (Style.DIM + <span class="string">"and in dim text"</span>)</div><div class="line"><span class="builtin-name">print</span> (Style.RESET_ALL)</div><div class="line"><span class="builtin-name">print</span> (<span class="string">"back to normal now!!"</span>)</div></pre></td></tr></table></figure></p><p>同样具体用法自行了解。</p><blockquote><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2></blockquote><p>在命令行下运行结果如下：<br><img src="http://oxwgzg29g.bkt.clouddn.com/19.png" alt="7"></p><p>每次都要输入python3 而且还都要在tickets.py所在目录才能执行这个程序十分的不方便，有没有简便的方法呢？答案是肯定的。</p><p>先来看下效果图:</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/20.png" alt="8"></p><blockquote><h3 id="setup的使用"><a href="#setup的使用" class="headerlink" title="setup的使用"></a>setup的使用</h3></blockquote><p>setup依赖于setuptools包，通过<strong>pip3 install setuptools</strong>进行安装。<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">from setuptools <span class="built_in">import</span> setup, find_packages</div><div class="line"></div><div class="line">setup(</div><div class="line"> <span class="attr">name</span> = <span class="string">"test"</span>,</div><div class="line"> <span class="attr">version</span> = <span class="string">"1.0"</span>,</div><div class="line"> <span class="attr">keywords</span> = (<span class="string">"test"</span>, <span class="string">"xxx"</span>),</div><div class="line"> <span class="attr">description</span> = <span class="string">"eds sdk"</span>,</div><div class="line"> <span class="attr">long_description</span> = <span class="string">"eds sdk for python"</span>,</div><div class="line"> <span class="attr">license</span> = <span class="string">"MIT Licence"</span>,</div><div class="line"></div><div class="line"> <span class="attr">url</span> = <span class="string">"http://test.com"</span>,</div><div class="line"> <span class="attr">author</span> = <span class="string">"test"</span>,</div><div class="line"> <span class="attr">author_email</span> = <span class="string">"test@gmail.com"</span>,</div><div class="line"></div><div class="line"> <span class="attr">packages</span> = find_packages(),</div><div class="line"> <span class="attr">include_package_data</span> = True,</div><div class="line"> <span class="attr">platforms</span> = <span class="string">"any"</span>,</div><div class="line"> <span class="attr">install_requires</span> = [],</div><div class="line"></div><div class="line"> <span class="attr">scripts</span> = [],</div><div class="line"> <span class="attr">entry_points</span> = &#123;</div><div class="line">  'console_scripts': [</div><div class="line">   '<span class="attr">test</span> = test.help:main'</div><div class="line">  ]</div><div class="line"> &#125;</div><div class="line">)</div></pre></td></tr></table></figure></p><blockquote><p> setup.py各参数介绍：<br>–name 包名称<br>–version (-V) 包版本<br>–author 程序的作者<br>–author_email 程序的作者的邮箱地址<br>–maintainer 维护者<br>–maintainer_email 维护者的邮箱地址<br>–url 程序的官网地址<br>–license 程序的授权信息<br>–description 程序的简单描述<br>–long_description 程序的详细描述<br>–platforms 程序适用的软件平台列表<br>–classifiers 程序的所属分类列表<br>–keywords 程序的关键字列表<br>–packages 需要处理的包目录（包含<strong>init</strong>.py的文件夹）<br>–py_modules 需要打包的python文件列表<br>–download_url 程序的下载地址<br>–cmdclass<br>–data_files 打包时需要打包的数据文件，如图片，配置文件等<br>–scripts 安装时需要执行的脚步列表<br>–package_dir 告诉setuptools哪些目录下的文件被映射到哪个源码包。一个例子：package_dir = {‘’: ‘lib’}，表示“root package”中的模块都在lib 目录中。<br>–requires 定义依赖哪些模块<br>–provides定义可以为哪些模块提供依赖<br>–find_packages() 对于简单工程来说，手动增加packages参数很容易，刚刚我们用到了这个函数，它默认在和setup.py同一目录下搜索各个含有 <strong>init</strong>.py的包。</p></blockquote><p>–install_requires = [“requests”] 需要安装的依赖包<br>–entry_points 动态发现服务和插件</p><p>entry_points 中console_scripts 指明了命令行工具的名称；在“redis_run = RedisRun.redis_run:main”中，等号前面指明了工具包的名称，等号后面的内容指明了程序的入口地址。</p><p>当然具体用法还需要自己去理解去实际，这里给出setup的官网，可以参考一下：<br><a href="https://docs.python.org/3/distutils/setupscript.html" target="_blank" rel="external">官方网站</a></p><p>我们需要用到的实例<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">from setuptools <span class="keyword">import</span> <span class="built_in">setup</span></div><div class="line"></div><div class="line"><span class="built_in">setup</span>(</div><div class="line">    name=<span class="string">'tickets'</span>,</div><div class="line">    py_modules=[<span class="string">'tickets'</span>, <span class="string">'stations'</span>],</div><div class="line">    install_requires=[<span class="string">'requests'</span>, <span class="string">'docopt'</span>, <span class="string">'prettytable'</span>, <span class="string">'colorama'</span>],</div><div class="line">    entry_points=&#123;</div><div class="line">        <span class="string">'console_scripts'</span>: [<span class="string">'tickets=tickets:main'</span>]</div><div class="line">    &#125;</div><div class="line">)</div></pre></td></tr></table></figure></p><p>运行python3 setup.py install 即可。</p><p>然后就可以在任何位置使用tickets [-dgktz] <from> <to> <date>进行命令行查票了。</date></to></from></p><blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2></blockquote><p>这个程序不是很难，通过这个程序使我掌握了docopt、prettytable、colorama、setup的使用，感觉收获还是很大的。<br>希望通过这个程序对你也有所提升。</p><blockquote><h3 id="项目地址"><a href="#项目地址" class="headerlink" title="项目地址"></a>项目地址</h3></blockquote><p>完整代码我放在我的GitHub上了，感兴趣的朋友可以参考一下,记得点star哦！</p><p><a href="https://github.com/Tactful-biao/scrapy/tree/master/12306" target="_blank" rel="external">项目地址</a></p><blockquote><p>文章如有出错的地方，请多多指正！如果你感觉文章对你有所帮助，文章下方有打赏按钮，您随意打赏。支持作者继续创作。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;/blockquote&gt;
&lt;p&gt;最近一直在找一些Python相关的练手项目，只有不断的做项目才能够对知识有更加牢固的掌
      
    
    </summary>
    
      <category term="Python" scheme="https://www.bbiao.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://www.bbiao.me/tags/Python/"/>
    
      <category term="12306" scheme="https://www.bbiao.me/tags/12306/"/>
    
  </entry>
  
  <entry>
    <title>抵御sshd暴力破解</title>
    <link href="https://www.bbiao.me/2017/10/23/%E6%8A%B5%E5%BE%A1sshd%E6%9A%B4%E5%8A%9B%E7%A0%B4%E8%A7%A3/"/>
    <id>https://www.bbiao.me/2017/10/23/抵御sshd暴力破解/</id>
    <published>2017-10-23T09:39:43.000Z</published>
    <updated>2017-10-23T10:38:50.092Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2></blockquote><p>最近发现服务器总是连接不上，打开控制台发现CPU一直处于很高的负载状态，导致我正常的连接都连接不上，这时候就该思考一下原因了，我打开系统下/var/log/secure(CentOS7)之后看到了如下画面：</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/14.png" alt="1"></p><p>大家都知道当你的服务器开启了远程连接服务之后，你的连接就相当于公开了，如果你不添加任何过滤的话，任何人都可以对你的服务器进行连接，当然想要连接成功的话肯定是要秘钥或者密码的。但是如果是密码的话就可以进行暴力枚举，虽然效率很低，但是总会有一些弱密码存在。群众里面有坏人，总有刁民想害朕。<br>我大致浏览了一下服务器日志，发现每天都有大量的连接，有的能够持续好几个小时的尝试登录，这样就导致我的正常登录存在问题。</p><p>我是设置了秘钥的，并且禁止了密码登录，但是还是有很多的ip尝试进行密码登录，虽然登录不上，但是一直建立连接也会影响服务器性能，主要是我打算在手机上远程控制服务器，这样的话我是打算把密码验证打开的，所以我要杜绝这种暴力登录的尝试，虽然我对我的密码有信心，但是一想到自己的服务器在一直被别人破解着密码，心里就不是很爽。</p><blockquote><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2></blockquote><p><a href="https://baike.baidu.com/item/DenyHosts/7529952?fr=aladdin" target="_blank" rel="external">DenyHosts</a> :DenyHosts是Python语言写的一个程序，它会分析sshd的日志文件（/var/log/secure），当发现重 复的攻击时就会记录IP到/etc/hosts.deny文件，从而达到自动屏IP的功能。<br>我们通过给服务器配置和安装DenyHosts把那些尝试破解我们的服务器的ip都添加到hosts.deny中，这样这些ip就不能再对我们进行尝试连接了。</p><blockquote><h3 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h3></blockquote><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">wget http:<span class="comment">//ncu.dl.sourceforge.net/sourceforge/denyhosts/DenyHosts-2.6.tar.gz</span></div><div class="line">tar -xzvf DenyHosts-<span class="number">2.6</span><span class="selector-class">.tar</span><span class="selector-class">.gz</span></div><div class="line">cd DenyHosts-<span class="number">2.6</span></div><div class="line">python setup<span class="selector-class">.py</span> install　　  <span class="comment">//安装Denyhost</span></div><div class="line">cd /usr/share/denyhosts/    <span class="comment">//切换目录进入/usr/share/denyhosts目录</span></div><div class="line">cp denyhosts<span class="selector-class">.cfg-dist</span> denyhosts<span class="selector-class">.cfg</span>   <span class="comment">//备份配置文件</span></div></pre></td></tr></table></figure><blockquote><h3 id="参数配置"><a href="#参数配置" class="headerlink" title="参数配置"></a>参数配置</h3></blockquote><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">vi</span> <span class="selector-tag">denyhosts</span><span class="selector-class">.cfg</span></div></pre></td></tr></table></figure><figure class="highlight clean"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"> ############ THESE SETTINGS ARE REQUIRED ############</div><div class="line"></div><div class="line">SECURE_LOG = /var/log/secure</div><div class="line"></div><div class="line">HOSTS_DENY = /etc/hosts.deny</div><div class="line"></div><div class="line">PURGE_DENY = <span class="number">1</span>w #过多久后清除已经禁止的，其中w代表周，d代表天，h代表小时，s代表秒，m代表分钟</div><div class="line"></div><div class="line">BLOCK_SERVICE  = sshd</div><div class="line"></div><div class="line">DENY_THRESHOLD_INVALID = <span class="number">3</span> #允许无效用户失败的次数</div><div class="line"></div><div class="line">DENY_THRESHOLD_VALID = <span class="number">5</span> #允许普通用户登陆失败的次数</div><div class="line"></div><div class="line">DENY_THRESHOLD_ROOT = <span class="number">5</span> #允许root登陆失败的次数</div><div class="line"></div><div class="line">DENY_THRESHOLD_RESTRICTED = <span class="number">1</span></div><div class="line"></div><div class="line">WORK_DIR = /usr/share/denyhosts/data</div><div class="line"></div><div class="line">SUSPICIOUS_LOGIN_REPORT_ALLOWED_HOSTS=YES</div><div class="line"></div><div class="line">HOSTNAME_LOOKUP=YES</div><div class="line"></div><div class="line">LOCK_FILE = /var/lock/subsys/denyhosts</div><div class="line"></div><div class="line"> ############ THESE SETTINGS ARE OPTIONAL ############</div><div class="line"></div><div class="line">ADMIN_EMAIL = denyhosts@<span class="number">163.</span>com #若有ip被禁用发邮件通知</div><div class="line"></div><div class="line">SMTP_HOST = localhost</div><div class="line"></div><div class="line">SMTP_PORT = <span class="number">25</span></div><div class="line"></div><div class="line">SMTP_FROM = DenyHosts &lt;<span class="number">192.168</span><span class="number">.0</span><span class="number">.1</span>@localhost&gt;</div><div class="line"></div><div class="line">SMTP_SUBJECT = DenyHosts Report</div><div class="line"></div><div class="line">AGE_RESET_VALID=<span class="number">1</span>d #有效用户登录失败计数归零的时间</div><div class="line"></div><div class="line">AGE_RESET_ROOT=<span class="number">1</span>d #root用户登录失败计数归零的时间</div><div class="line"></div><div class="line">AGE_RESET_RESTRICTED=<span class="number">1</span>d</div><div class="line"></div><div class="line">AGE_RESET_INVALID=<span class="number">10</span>d #无效用户登录失败计数归零的时间</div><div class="line"></div><div class="line">######### THESE SETTINGS ARE SPECIFIC TO DAEMON MODE  ##########</div><div class="line"></div><div class="line">DAEMON_LOG = /var/log/denyhosts</div><div class="line"></div><div class="line">DAEMON_SLEEP = <span class="number">30</span>s</div><div class="line"></div><div class="line">DAEMON_PURGE = <span class="number">1</span>h</div></pre></td></tr></table></figure><blockquote><h3 id="启动文件配置"><a href="#启动文件配置" class="headerlink" title="启动文件配置"></a>启动文件配置</h3></blockquote><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">cp daemon-<span class="section">control</span>-dist daemon-<span class="section">control</span></div><div class="line">chown root daemon-<span class="section">control</span></div><div class="line">chmod <span class="number">700</span> daemon-<span class="section">control</span></div><div class="line">./daemon-<span class="section">control</span> start         <span class="comment">//启动DenyHosts</span></div><div class="line"> </div><div class="line">ln -s /usr/share/denyhosts/daemon-<span class="section">control</span> /etc/init.d/denyhosts    <span class="comment">//建立符号链接</span></div><div class="line">chkconfig --add denyhosts　　　　　　　　　　　　　　　　　　　　  <span class="comment">//增加denyhosts服务进程</span></div><div class="line">chkconfig  denyhosts on　　　　　　　　　　　　　　　　　　　　　　<span class="comment">//设置开机启动denyhosts</span></div><div class="line">chkconfig --<span class="type">list</span> denyhosts      <span class="comment">// 查看是否生效</span></div></pre></td></tr></table></figure><p>如果显示如下则表示生效：<br><img src="http://oxwgzg29g.bkt.clouddn.com/15.png" alt="3"></p><p>到目前为止我们的所有配置都已经配置完成了，我们来看一下日志：<br><img src="http://oxwgzg29g.bkt.clouddn.com/16.png" alt="4"></p><p>可以看到这个59.63.188.3的ip尝试暴力登录我们的服务器，在尝试登录5次之后，就被拒绝连接了。<br>我们再来看一下我们的hosts.deny文件：</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/17.png" alt="5"></p><p>可以在最后看到该IP已经在我们的屏蔽列表里面了。</p><blockquote><p>文章若有出错的地方，请多多指正！如果你感觉文章对你有所帮助，文章下方有打赏按钮，您随意打赏。支持作者继续创作。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;/blockquote&gt;
&lt;p&gt;最近发现服务器总是连接不上，打开控制台发现CPU一直处于很高的负载状态，导致我正常的连
      
    
    </summary>
    
      <category term="解决问题" scheme="https://www.bbiao.me/categories/%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/"/>
    
    
      <category term="ssh" scheme="https://www.bbiao.me/tags/ssh/"/>
    
      <category term="暴力破解" scheme="https://www.bbiao.me/tags/%E6%9A%B4%E5%8A%9B%E7%A0%B4%E8%A7%A3/"/>
    
      <category term="denyhosts" scheme="https://www.bbiao.me/tags/denyhosts/"/>
    
  </entry>
  
  <entry>
    <title>搭建自己的网盘 owncloud</title>
    <link href="https://www.bbiao.me/2017/10/16/%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E7%BD%91%E7%9B%98-owncloud/"/>
    <id>https://www.bbiao.me/2017/10/16/搭建自己的网盘-owncloud/</id>
    <published>2017-10-16T04:16:40.000Z</published>
    <updated>2017-10-23T13:30:34.702Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2></blockquote><p>现在网盘，云存储等一些虚拟存储技术越来越发达，但是不是空间受限制就是网速受限制，而且随着网络监管越来越严，自己的数据放在别人那里终究不保险，还动不动就被和谐。既然这样何不自己撘一个网盘？</p><p>网盘的搭建并没有大家想的那么难，我们并不需要实现底层的东西，直接利用现成的东西就可以搭建完成。</p><blockquote><h3 id="自己搭建网盘有什么好处？"><a href="#自己搭建网盘有什么好处？" class="headerlink" title="自己搭建网盘有什么好处？"></a>自己搭建网盘有什么好处？</h3></blockquote><ol><li>自己的东西用着放心</li><li>自己搭建的，网速不受限制</li><li>我的地盘，我做主，自己想存什么东西就存什么东西(违法的东西不要存)！</li></ol><blockquote><h3 id="都需要用到哪些东西"><a href="#都需要用到哪些东西" class="headerlink" title="都需要用到哪些东西"></a>都需要用到哪些东西</h3></blockquote><ul><li>需要有一台服务器(用来存储)</li><li><a href="https://baike.baidu.com/item/ownCloud%E4%B8%AA%E4%BA%BA%E4%BA%91%E6%9C%8D%E5%8A%A1/16116850?fr=aladdin" target="_blank" rel="external">OwnCloud 个人云服务</a></li><li>LAMP环境：Linux-Apache-MySQL-PHP</li><li><a href="https://baike.baidu.com/item/ssl/320778?fr=aladdin" target="_blank" rel="external">SSL</a>: 安全套接字</li></ul><p>上面就是我们搭建个人云服务所需要的环境已经一些配置要求，不理解的可以百度，或者给博主留言，这里就不再细讲了。</p><blockquote><h2 id="安装流程"><a href="#安装流程" class="headerlink" title="安装流程"></a>安装流程</h2></blockquote><h3 id="第一步：选择服务器"><a href="#第一步：选择服务器" class="headerlink" title="第一步：选择服务器"></a>第一步：选择服务器</h3><p>我个人使用的是Digital Ocean上购买的虚拟主机，5$一个月20G SSD，1T流量。个人用我感觉已经够了，当然如果你的需求较高，当然是有更高的配置，同时价钱也会更加的高。如果你想要在Digital Ocean上购买虚拟主机，可以参考我的这篇文章《<a href="http://bbiao.me/2017/07/15/digitalocean%E6%B3%A8%E5%86%8C%E5%92%8C%E8%99%9A%E6%8B%9F%E4%B8%BB%E6%9C%BA%E7%9A%84%E5%88%9B%E5%BB%BA/" target="_blank" rel="external">digitalocean注册和虚拟主机的创建</a>》</p><p>我选择的是CentOS 7.4 进行搭建的，关于虚拟主机的创建等在这里不再介绍了，在上面提到的文章中都有。我们直接开始配置。</p><h3 id="第二步：LAMP-环境的搭建"><a href="#第二步：LAMP-环境的搭建" class="headerlink" title="第二步：LAMP 环境的搭建"></a>第二步：LAMP 环境的搭建</h3><p>连接上服务器之后使用:<font color="Lime">yum update</font></p><blockquote><h4 id="安装Apache"><a href="#安装Apache" class="headerlink" title="安装Apache"></a>安装Apache</h4></blockquote><p>通过yum源进行安装：<font color="Lime">yum -y install httpd</font></p><p>然后启动我们的Apache：<font color="Lime">systemctl start httpd.service</font></p><p>输入：<font color="Lime"><a href="http://你的服务器的ip地址/" target="_blank" rel="external">http://你的服务器的ip地址/</a></font></p><p>可以通过该命令查看你的IP地址：<br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attribute">ip</span> addr show eth0 | grep inet | awk <span class="string">'&#123; print <span class="variable">$2</span>; &#125;'</span> | sed <span class="string">'s/\/.*$//'</span></div></pre></td></tr></table></figure></p><p>如果看到如下画面则表示Apache安装成功</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/1.PNG" alt="4"></p><p>最后一步是把启动设置为开机自动启动：<font color="Lime">systemctl enable httpd.service</font></p><blockquote><h4 id="安装MySQL"><a href="#安装MySQL" class="headerlink" title="安装MySQL"></a>安装MySQL</h4></blockquote><p>使用如下命令：<font color="Lime">yum -y install mariadb-server mariadb</font></p><p>重启一下MySQL服务：<font color="Lime">systemctl start mariadb</font></p><p>紧接着对MySQL进行一些安全设置：<font color="Lime">mysql_secure_installation</font></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Enter current password <span class="keyword">for</span> root (enter <span class="keyword">for</span> none):</div><div class="line">OK, successfully used password, moving on<span class="built_in">..</span>.</div><div class="line"></div><div class="line">Setting the root password ensures that nobody can log into the MariaDB</div><div class="line">root<span class="built_in"> user </span>without the proper authorization.</div><div class="line"></div><div class="line">New password: password</div><div class="line">Re-enter new password: password</div><div class="line">Password updated successfully!</div><div class="line">Reloading privilege tables<span class="built_in">..</span></div><div class="line"> Success!</div></pre></td></tr></table></figure><p> 你可以看到上面的这些设置，其中只有在password的地方输入你自己喜欢的密码，其他的地方之间按回车就可以了。</p><p>最后一步是把MySQL也写入开机自启:<font color="Lime">systemctl enable mariadb.service</font></p><blockquote><h4 id="安装PHP-7"><a href="#安装PHP-7" class="headerlink" title="安装PHP 7"></a>安装PHP 7</h4></blockquote><p>先导入php 7的安装源：<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">rpm -Uvh <span class="symbol">https:</span>/<span class="regexp">/dl.fedoraproject.org/pub</span><span class="regexp">/epel/epel</span>-release-latest-<span class="number">7</span>.noarch.rpm</div><div class="line">rpm -Uvh <span class="symbol">https:</span>/<span class="regexp">/mirror.webtatic.com/yum</span><span class="regexp">/el7/webtatic</span>-release.rpm</div></pre></td></tr></table></figure></p><p>安装php 7：<font color="Lime">yum -y install php70w</font></p><p>安装php 7 的一些模板：<font color="Lime">yum -y install php70w-mysql php70w-xml php70w-soap php70w-xmlrpc php70w-mbstring php70w-json php70w-gd php70w-mcrypt</font></p><p>查看php的版本: <font color="Lime">php -v</font><br>显示如下，则表示安装成功：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">PHP 7.0.24 (cli) (built: Sep 30 2017 10:10:28) ( NTS )</div><div class="line">Copyright (c) 1997-2017 The PHP Group</div><div class="line">Zend Engine v3.0.0, Copyright (c) 1998-2017 Zend Technologies</div><div class="line">    with Zend OPcache v7.0.24, Copyright (c) 1999-2017, by Zend Technologies</div></pre></td></tr></table></figure></p><p>重启一下Apache服务：<font color="Lime">systemctl restart httpd.service</font></p><blockquote><h2 id="配置SSL证书"><a href="#配置SSL证书" class="headerlink" title="配置SSL证书"></a>配置SSL证书</h2></blockquote><p>大家都知道，无论什么服务安全性永远是第一位，如果不能够提供安全保障，肯定不会有人使用。OwnCloud也是，你的访问必须要能够提供安全保障，才能够使用，所有必须要配置SSL。SSL一般有专门的认证机构，只有这些官方的认证机构认证的网址才会显示正确，当然还有一种就是自认证方式，我自己给我自己认证，我同样能够提供安全连接，但是浏览器是不认的。如果没有域名并且不想通过官方认证的话，我们也可以用，下面我们就使用这种方法进行配置我们的字签名证书：</p><blockquote><h3 id="第一步：安装mod-ssl"><a href="#第一步：安装mod-ssl" class="headerlink" title="第一步：安装mod_ssl"></a>第一步：安装mod_ssl</h3></blockquote><p>安装命令：<font color="Lime">yum -y install mod_ssl</font></p><blockquote><h3 id="第二步：-创建新证书"><a href="#第二步：-创建新证书" class="headerlink" title="第二步： 创建新证书"></a>第二步： 创建新证书</h3></blockquote><p>创建文件夹：<font color="Lime">mkdir /etc/ssl/private</font></p><p>修改权限(此文件夹必须严格保密，所以权限设置成这样)：<font color="Lime">chmod 700 /etc/ssl/private</font></p><p>创建SSL秘钥和证书文件OpenSSL：<font color="Lime">openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/ssl/private/apache-selfsigned.key -out /etc/ssl/certs/apache-selfsigned.crt</font></p><blockquote><ul><li>openssl：这是创建和管理OpenSSL证书，密钥和其他文件的基本命令行工具。</li><li>req -x509：这指定我们要使用X.509证书签名请求（CSR）管理。“X.509”是SSL和TLS坚持用于密钥和证书管理的公钥基础架构标准。<br>节点：这将告诉OpenSSL跳过使用密码保护证书的选项。当服务器启动时，我们需要Apache无需用户干预才能读取该文件。密码短语会阻止这种情况发生，因为每次重启后都必须输入密码。</li><li>days 365：此选项设置证书被认为有效的时间长度。我们在这里设置了一年。</li><li>newkey rsa：2048：这指定我们要同时生成一个新的证书和一个新的密钥。我们没有创建在上一步中签署证书所需的密钥，因此我们需要与证书一起创建证书。该rsa:2048部分告诉它制作一个2048位长的RSA密钥。</li><li>keyout：此行告诉OpenSSL放置我们正在创建的生成的私钥文件。</li><li>out：这告诉OpenSSL放置我们正在创建的证书的位置。</li></ul></blockquote><p>完整的提示列表将如下所示：</p><figure class="highlight less"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">Country</span> <span class="selector-tag">Name</span> (<span class="number">2</span> letter code) <span class="selector-attr">[XX]</span><span class="selector-pseudo">:US</span></div><div class="line"><span class="selector-tag">State</span> <span class="selector-tag">or</span> <span class="selector-tag">Province</span> <span class="selector-tag">Name</span> (full name) <span class="selector-attr">[]</span><span class="selector-pseudo">:Example</span></div><div class="line"><span class="selector-tag">Locality</span> <span class="selector-tag">Name</span> (eg, city) <span class="selector-attr">[Default City]</span><span class="selector-pseudo">:Example</span> </div><div class="line"><span class="selector-tag">Organization</span> <span class="selector-tag">Name</span> (eg, company) <span class="selector-attr">[Default Company Ltd]</span><span class="selector-pseudo">:Example</span> <span class="selector-tag">Inc</span></div><div class="line"><span class="selector-tag">Organizational</span> <span class="selector-tag">Unit</span> <span class="selector-tag">Name</span> (eg, section) <span class="selector-attr">[]</span><span class="selector-pseudo">:Example</span> <span class="selector-tag">Dept</span></div><div class="line"><span class="selector-tag">Common</span> <span class="selector-tag">Name</span> (eg, your name or your server's hostname) <span class="selector-attr">[]</span><span class="selector-pseudo">:example.com</span></div><div class="line"><span class="selector-tag">Email</span> <span class="selector-tag">Address</span> <span class="selector-attr">[]</span><span class="selector-pseudo">:webmaster</span>@<span class="selector-tag">example</span><span class="selector-class">.com</span></div></pre></td></tr></table></figure><p>上面的内容可以不写，一直回车，内容就为空。</p><p>您创建的两个文件将被放置在/etc/ssl目录的相应子目录中。</p><p>当我们使用OpenSSL时，我们还应该创建一个强大的Diffie-Hellman组<br>接着输入：<font color="Lime">openssl dhparam -out /etc/ssl/certs/dhparam.pem 2048</font></p><p>由于CentOS 7附带的Apache版本不包括该SSLOpenSSLConfCmd指令，所以我们必须手动将生成的文件附加到我们的自签名证书的末尾.<br>输入：<font color="Lime">cat /etc/ssl/certs/dhparam.pem | sudo tee -a /etc/ssl/certs/apache-selfsigned.crt</font></p><blockquote><h3 id="第三步：设置证书"><a href="#第三步：设置证书" class="headerlink" title="第三步：设置证书"></a>第三步：设置证书</h3></blockquote><p>打开Apache的SSL配置文件：<font color="Lime">vi /etc/httpd/conf.d/ssl.conf</font></p><p>找到开头的部分<virtualhost _default_:443="">。我们需要在这里进行一些修改，以确保我们的SSL证书正确地应用到我们的站点。</virtualhost></p><p>首先，取消注释DocumentRoot行<br>接下来，取消注释ServerName行，并www.example.com用您的域名或服务器IP地址替换(如下):<br><img src="http://oxwgzg29g.bkt.clouddn.com/2.PNG" alt="5"></p><p>把下图中圈出来的地方注释掉：<br><img src="http://oxwgzg29g.bkt.clouddn.com/3.png" alt="6"></p><p>找到SSLCertificateFile和SSLCertificateKeyFile，并改变他们在我们所做的目录/etc/httpd/ssl：<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">SSLCertificateFile <span class="regexp">/etc/</span>ssl<span class="regexp">/certs/</span>apache-selfsigned.crt</div><div class="line">SSLCertificateKeyFile <span class="regexp">/etc/</span>ssl<span class="regexp">/private/</span>apache-selfsigned.key</div></pre></td></tr></table></figure></p><p>如图：</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/4.png" alt="7"></p><p>在VirtualHost后(配置文件最后)加上如下内容：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Begin copied text</span></div><div class="line"><span class="comment"># from https://cipherli.st/</span></div><div class="line"><span class="comment"># and https://raymii.org/s/tutorials/Strong_SSL_Security_On_Apache2.html</span></div><div class="line"></div><div class="line"><span class="attribute">SSLCipherSuite</span> EECDH+AESGCM:EDH+AESGCM:AES256+EECDH:AES256+EDH</div><div class="line"><span class="attribute">SSLProtocol</span> <span class="literal">All</span> -SSLv2 -SSLv3</div><div class="line"><span class="attribute">SSLHonorCipherOrder</span> <span class="literal">On</span></div><div class="line"><span class="comment"># Disable preloading HSTS for now.  You can use the commented out header line that includes</span></div><div class="line"><span class="comment"># the "preload" directive if you understand the implications.</span></div><div class="line"><span class="comment">#Header always set Strict-Transport-Security "max-age=63072000; includeSubdomains; preload"</span></div><div class="line"><span class="attribute"><span class="nomarkup">Header</span></span> always set Strict-Transport-Security <span class="string">"max-age=63072000; includeSubdomains"</span></div><div class="line"><span class="attribute"><span class="nomarkup">Header</span></span> always set X-Frame-Options DENY</div><div class="line"><span class="attribute"><span class="nomarkup">Header</span></span> always set X-Content-Type-Options nosniff</div><div class="line"><span class="comment"># Requires Apache &gt;= 2.4</span></div><div class="line"><span class="attribute">SSLCompression</span> <span class="literal">off</span> </div><div class="line"><span class="attribute">SSLUseStapling</span> <span class="literal">on</span> </div><div class="line"><span class="attribute">SSLStaplingCache</span> <span class="string">"shmcb:logs/stapling-cache(150000)"</span> </div><div class="line"><span class="comment"># Requires Apache &gt;= 2.4.11</span></div><div class="line"><span class="comment"># SSLSessionTickets Off</span></div></pre></td></tr></table></figure><p>完成这些更改后，可以保存并关闭文件。</p><blockquote><h3 id="修改未加密的虚拟主机文件重定向到HTTPS"><a href="#修改未加密的虚拟主机文件重定向到HTTPS" class="headerlink" title="修改未加密的虚拟主机文件重定向到HTTPS"></a>修改未加密的虚拟主机文件重定向到HTTPS</h3></blockquote><p>命令：<font color="Lime">vi /etc/httpd/conf.d/non-ssl.conf</font></p><p>加入如下内容：其中如果你有域名就把example.com换成你的域名，这里我们讲一下没有域名的操作：<br><figure class="highlight apache"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="section">&lt;VirtualHost *:80&gt;</span></div><div class="line">        <span class="attribute"><span class="nomarkup">ServerName</span></span> www.example.com</div><div class="line">        <span class="attribute">Redirect</span> <span class="string">"/"</span> <span class="string">"https://www.example.com/"</span></div><div class="line"><span class="section">&lt;/VirtualHost&gt;</span></div></pre></td></tr></table></figure></p><p>如下图，改成你自己的IP地址：<br><img src="http://oxwgzg29g.bkt.clouddn.com/6.PNG" alt="8"></p><blockquote><h3 id="最后一步：激活证书"><a href="#最后一步：激活证书" class="headerlink" title="最后一步：激活证书"></a>最后一步：激活证书</h3></blockquote><p>输入：<font color="Lime">apachectl configtest</font></p><p>输出如下则表示正确：</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="bullet">. </span>. .</div><div class="line">Syntax OK</div></pre></td></tr></table></figure><p>重新启动Apache服务器以通过以下命令以应用您的更改：<font color="Lime">systemctl restart httpd.service</font></p><p>安装防火墙(CentOS 7)：<font color="Lime">yum -y install firewalld</font></p><p>启动防火墙：<font color="Lime">systemctl start firewalld</font></p><p>打开如下端口：<br><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">firewall-<span class="keyword">cmd</span><span class="bash"> --add-service=http</span></div><div class="line"><span class="bash">firewall-cmd --add-service=https</span></div><div class="line"><span class="bash">firewall-cmd --runtime-to-permanent</span></div></pre></td></tr></table></figure></p><pre><code>输入https://你的IP地址，然后可以看到如图所示：</code></pre><p><img src="http://oxwgzg29g.bkt.clouddn.com/7.PNG" alt="9"></p><p>可以看到提示我们的https不安全，这是因为我们的自签名证书得不到浏览器的承认，但是不影响我们用。</p><blockquote><h2 id="安装Owncloud"><a href="#安装Owncloud" class="headerlink" title="安装Owncloud"></a>安装Owncloud</h2></blockquote><p>前面准备了这么多，都是准备，现在才开始正式安装。。</p><p>首先，导入rpm包源：<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rpm --import https:<span class="regexp">//</span>download.owncloud.org<span class="regexp">/download/</span>repositories<span class="regexp">/stable/</span>CentOS_7<span class="regexp">/repodata/</span>repomd.xml.key</div></pre></td></tr></table></figure></p><p>使用curl命令下载ownCloud存储库文件：<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">curl -L https:<span class="regexp">//</span>download.owncloud.org<span class="regexp">/download/</span>repositories<span class="regexp">/stable/</span>CentOS_7<span class="regexp">/ce:stable.repo -o /</span>etc<span class="regexp">/yum.repos.d/</span>ownCloud.repo</div></pre></td></tr></table></figure></p><p>清空一下缓存：<font color="Lime">yum clean expire-cache</font></p><p>输出如下：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Loaded plugin<span class="variable">s:</span> fastestmirror</div><div class="line">Cleaning repo<span class="variable">s:</span> base ce_production epel extras updates webtatic</div><div class="line"><span class="number">9</span> metadata <span class="keyword">files</span> removed</div></pre></td></tr></table></figure></p><p>安装OwnCloud：<font color="Lime">yum -y install owncloud</font></p><blockquote><h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3></blockquote><p>登录数据库：<font color="Lime">mysql -u root -p</font><br>输入你之前设置的MySQL的密码<br>创建数据库：<font color="Lime">CREATE DATABASE owncloud;</font><br>为该数据库设置密码：<font color="Lime">GRANT ALL ON owncloud.* to ‘owncloud’@’localhost’ IDENTIFIED BY ‘set_database_password’;</font><br>其中set_datebase_password设置成你自己喜欢的密码</p><p>刷新一下权限：<font color="Lime">FLUSH PRIVILEGES;</font></p><p>退出：<font color="Lime">exit</font></p><blockquote><h3 id="配置Owncloud"><a href="#配置Owncloud" class="headerlink" title="配置Owncloud"></a>配置Owncloud</h3></blockquote><pre><code>在浏览器输入： https://你的服务器ip/owncloud</code></pre><p>如果出现如图所示错误：</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/8.PNG" alt="10"></p><p>使用如下方法进行处理：<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">cd</span> <span class="string">/var/www/html</span></div><div class="line">chown apache owncloud -Rf</div><div class="line">chmod 770 owncloud -Rf</div></pre></td></tr></table></figure></p><p>另外设置：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">vi /etc/selinux/config</div><div class="line">将其中<span class="attribute">SELINUX</span>=enforcing</div><div class="line">改为<span class="attribute">SELINUX</span>=disabled</div></pre></td></tr></table></figure><p>设置完这些之后需要重启一下服务器</p><p>然后刷新刚才的网页就可以看到：</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/10.PNG" alt="11"></p><p>设置管理员账号和密码：</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/11.PNG" alt="12"></p><p>紧接着就配置完成了，用你刚刚设置的账号密码进行登录：</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/9.PNG" alt="13"></p><p>然后你的个人网盘就搭建完成了：</p><p><img src="http://oxwgzg29g.bkt.clouddn.com/12.PNG" alt="14"></p><blockquote><p>如果你感觉文章对你有所帮助，文章下方有打赏按钮，您随意打赏！</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;/blockquote&gt;
&lt;p&gt;现在网盘，云存储等一些虚拟存储技术越来越发达，但是不是空间受限制就是网速受限制，而且随
      
    
    </summary>
    
      <category term="博客" scheme="https://www.bbiao.me/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="OwnCloud" scheme="https://www.bbiao.me/tags/OwnCloud/"/>
    
      <category term="网盘" scheme="https://www.bbiao.me/tags/%E7%BD%91%E7%9B%98/"/>
    
      <category term="LAMP" scheme="https://www.bbiao.me/tags/LAMP/"/>
    
      <category term="SSL" scheme="https://www.bbiao.me/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>博客迁移</title>
    <link href="https://www.bbiao.me/2017/10/15/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB/"/>
    <id>https://www.bbiao.me/2017/10/15/博客迁移/</id>
    <published>2017-10-15T07:12:46.000Z</published>
    <updated>2017-10-15T08:58:25.433Z</updated>
    
    <content type="html"><![CDATA[<blockquote><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3></blockquote><p>网站迁移实属无奈，在新疆上大学，地处新疆限制比较多，首先没法远程连接，服务器登录不上。其次，不能翻墙，在新疆翻墙被定义为暴恐活动。在这里GitHub是被墙了的，CSDN是被墙了的，百度云是被墙了的，网易云课堂是被墙了的！不是说我翻墙是为了访问国外的东西，这些程序员学习几乎算是必备的网站不翻墙都没法进行访问。</p><p>ss搭建的梯子，在这里照样没法访问。前几天博客突然没法访问，服务器也ping不通，远程也连接不上，我还以为是服务出问题了。我远程控制家里的电脑(内地)就可以进行连接。</p><blockquote><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3></blockquote><p>毕竟胳膊拗不过大腿，万般无奈下，博主把博客迁移到了GitHub Page上了。博主通过修改hosts文件，可以在不翻墙的情况下访问GitHub，虽然访问速度很慢，但是勉强能用。</p><p>博客通过hexo进行部署，操作简单，可定制性较高，虽然界面没有WordPress华丽，但是作为一个静态博客还是十分优秀的，hexo支持MarkDown，操作十分简单，通过hexo new “文章名” 进行创建文章；hexo g 生成HTML格式的网页；hexo s 对网站进行预览；hexo d 把文章部署到GitHub上。这几天已经把所有文章迁移过来了，原来的服务器已经不用了。</p><blockquote><h3 id="wordpress-和-hexo-比较"><a href="#wordpress-和-hexo-比较" class="headerlink" title="wordpress 和 hexo 比较"></a>wordpress 和 hexo 比较</h3></blockquote><ul><li><p>hexo 的可定制性真的很高，我在WordPress上布置的页面、内容都可以同样的进行布置，就是WordPress上时通过鼠标点击，hexo上是通过手工配置。</p></li><li><p>hexo 更加简洁，WordPress更加华丽。</p></li><li><p>hexo 搭配GitHub page免费，WordPress搭配服务器收费。</p></li></ul><blockquote><h3 id="博客细节"><a href="#博客细节" class="headerlink" title="博客细节"></a>博客细节</h3></blockquote><p>首先，博客还是按照迁移之前的模式来的，大同小异，页面主要有首页、关于、标签、归档、萌宠专区、给我留言、碎碎念、搜索九个板块。</p><p>hexo创建新界面只需要一条指令: hexo new page “页面名称”</p><ul><li>首页：网站的大纲</li><li>关于：关于我的介绍</li><li>标签：文章的标签，可以根据标签来定位感兴趣的文章</li><li>归档：对文章的一个统计</li><li>萌宠专区：这是我的个人兴趣，对猫咪比较喜欢，创建了这个版块，分享一些可爱的猫咪。</li><li>给我留言：这是一个留言板，使用的是国内的畅言，可以通过这个版块给我留言。</li><li>碎碎念：这是一个博主用来写一些灵感，吐吐槽的地方。</li><li>搜索：这个版块提供了搜索功能，可以对本博客进行快速检索。</li></ul><p>另外定义了RSS订阅功能，同时附上了四个社交链接，分别是<a href="https://github.com/Tactful-biao/" target="_blank" rel="external">我的GitHub</a><br>、<a href="http://bbiao.me/shibiaosun@gmail.com" target="_blank" rel="external">我的Gmail</a>、<a href="https://twitter.com/bb1208339113" target="_blank" rel="external">我的Twitter</a>、<a href="http://weibo.com/u/2714116747" target="_blank" rel="external">我的微博</a></p><p>在头像上面添加了鼠标放上自动旋转的特效，添加了部分友链，开启了背景动画，设置了鼠标点击变成爱心的js。</p><p>文章方面设置了总览的时候默认显示30个字，点击阅读更多可以查看全文，文章最后添加了打赏。开启了网站统计，百度分享。文章评论。</p><p>还有一些细节方面的优化，比如SEO，网站个性化设置等等…</p><blockquote><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3></blockquote><p>折腾了几天了，心中一肚子的气，但更多的是无奈。地区比较特殊，网络管制比较严，在国家面前，个人利益又算的了什么呢？人微言轻，谨言慎行！早日离开这是非之地。</p><blockquote><p>如果你感觉文章对你有所帮助，文章下方有打赏按钮，您随意打赏。支持作者继续创作。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;h3 id=&quot;原因&quot;&gt;&lt;a href=&quot;#原因&quot; class=&quot;headerlink&quot; title=&quot;原因&quot;&gt;&lt;/a&gt;原因&lt;/h3&gt;&lt;/blockquote&gt;
&lt;p&gt;网站迁移实属无奈，在新疆上大学，地处新疆限制比较多，首先没法远程连接，服务器登录不上。
      
    
    </summary>
    
      <category term="博客" scheme="https://www.bbiao.me/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="Hexo" scheme="https://www.bbiao.me/tags/Hexo/"/>
    
      <category term="Next" scheme="https://www.bbiao.me/tags/Next/"/>
    
  </entry>
  
  <entry>
    <title>爬取《电影天堂》所有电影种子</title>
    <link href="https://www.bbiao.me/2017/09/17/%E7%88%AC%E5%8F%96%E3%80%8A%E7%94%B5%E5%BD%B1%E5%A4%A9%E5%A0%82%E3%80%8B%E6%89%80%E6%9C%89%E7%94%B5%E5%BD%B1%E7%A7%8D%E5%AD%90/"/>
    <id>https://www.bbiao.me/2017/09/17/爬取《电影天堂》所有电影种子/</id>
    <published>2017-09-17T06:17:30.000Z</published>
    <updated>2018-03-17T09:43:01.002Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>相信大家都有到处找资源找种子的经历(不管是什么种子)。为何不自己写一个爬虫把一个网站的所有的种子链接都爬取下来呢？<br>所以接下来我们就通过一个简单的爬虫程序把电影天堂的所有的种子链接爬取下来。</p></blockquote><hr><blockquote><h2 id="思路永远是第一步"><a href="#思路永远是第一步" class="headerlink" title="思路永远是第一步"></a>思路永远是第一步</h2></blockquote><ol><li>首先我们需要分析目标站点，然后依次找到我们需要的内容所在的位置。</li><li>然后我们再通过代码对目标内容进行提取</li><li>保存到本地</li></ol><blockquote><h3 id="分析目标网站"><a href="#分析目标网站" class="headerlink" title="分析目标网站"></a>分析目标网站</h3></blockquote><p>注意<font color="red">网址的结构已经发生变化，文章末尾给出了最新的更新过的代码，并且代码已经同步更新到我的github</font></p><p>打开目标网址：<a href="http://www.btbtdy.com/btfl/dy1.html" target="_blank" rel="external">http://www.btbtdy.com/btfl/dy1.html</a>可以看到如下界面：</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/14.png" alt="2"></p><p>我们的目标是所有的电影，所以我们选择电影栏。电视剧什么的我个人不需要，所以我也就不打算爬取它。以下内容根据自己的情况进行选择：</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/15.png" alt="3"></p><p>我们查看网站的源代码可以看到，所有的电影详情页的链接都在div下的class为lsit_su下的ul里面的li标签下，但是链接是隐藏了域名的，所以我们在构造链接的时候需要加上前面的部分：</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/16.png" alt="4"></p><blockquote><p>我们打开一个详情页可以看到下载地址，点开下载地址又是另外一个链接：</p></blockquote><p><img src="http://ovfqn6f2x.bkt.clouddn.com/17.png" alt="5"></p><p>鼠标放在下载地址上面，点击鼠标右键–&gt;检查，直接定位目标所在的源代码，我们可以看到下载页的所在的链接位置。</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/18.png" alt="6"></p><p>我们接着点击下载页的链接：</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/19.png" alt="7"></p><blockquote><p>通过上面的三步操作我们找到了我们想要的东西：</p><ul><li>打开主页</li><li>打开详情页</li><li>打开下载链接</li></ul></blockquote><p>我们先按照常规的方法进行爬取，文章最后我们会将一种简单的方法，一步到位的方法。</p><blockquote><h3 id="构造所有详情页的地址"><a href="#构造所有详情页的地址" class="headerlink" title="构造所有详情页的地址"></a>构造所有详情页的地址</h3></blockquote><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python3</div><div class="line"><span class="meta"># encoding=utf-8</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> requests</div><div class="line">from bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line">url = <span class="string">'http://www.btbtdy.com/btfl/dy1.html'</span></div><div class="line">html = requests.<span class="built_in">get</span>(url).content</div><div class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">content = soup.<span class="built_in">find</span>(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'list_su'</span>&#125;)</div><div class="line">pic_link = content.<span class="built_in">find</span>(<span class="string">'ul'</span>).find_all(<span class="string">'li'</span>)</div><div class="line"><span class="built_in">for</span> link in pic_link:</div><div class="line">    a = link.<span class="built_in">find</span>(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'liimg'</span>&#125;).<span class="built_in">find</span>(<span class="string">'a'</span>)[<span class="string">'href'</span>]</div><div class="line">    <span class="built_in">print</span>(<span class="string">'http://www.btbtdy.com'</span> + a)</div></pre></td></tr></table></figure><p>运行结果如下：</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/20.png" alt="8"></p><p>上面的结果是爬取了当前页面的所有的电影的详情页的链接，我们需要爬取的是整个网站所有页的链接，所有我们需要分析它的翻页规则：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">http:<span class="regexp">//</span>www.btbtdy.com<span class="regexp">/screen/</span><span class="number">1</span>—–time-<span class="number">1</span>.html</div><div class="line">http:<span class="regexp">//</span>www.btbtdy.com<span class="regexp">/screen/</span><span class="number">1</span>—–time-<span class="number">2</span>.html</div><div class="line">…….</div><div class="line">http:<span class="regexp">//</span>www.btbtdy.com<span class="regexp">/screen/</span><span class="number">1</span>—–time-<span class="number">265</span>.html</div></pre></td></tr></table></figure><p>总共有265页，每一页唯一的区别就是一个数字的不同，所有我们可以使用一种偷懒的方式，既然我们已经知道它总共有265页，那么我们就可以通过range来手动指定循环的范围。</p><p>代码更改如下：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python3</div><div class="line"><span class="meta"># encoding=utf-8</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> requests</div><div class="line">from bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line"><span class="built_in">for</span> i in range(<span class="number">1</span>, <span class="number">266</span>):</div><div class="line">    url = <span class="string">'http://www.btbtdy.com/screen/1-----time-'</span> + str(i) + <span class="string">'.html'</span></div><div class="line">    html = requests.<span class="built_in">get</span>(url).content</div><div class="line">    soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">    content = soup.<span class="built_in">find</span>(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'list_su'</span>&#125;)</div><div class="line">    pic_link = content.<span class="built_in">find</span>(<span class="string">'ul'</span>).find_all(<span class="string">'li'</span>)</div><div class="line">    <span class="built_in">for</span> link in pic_link:</div><div class="line">        a = link.<span class="built_in">find</span>(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'liimg'</span>&#125;).<span class="built_in">find</span>(<span class="string">'a'</span>)[<span class="string">'href'</span>]</div><div class="line">        <span class="built_in">print</span>(<span class="string">'http://www.btbtdy.com'</span> + a)</div></pre></td></tr></table></figure><p>运行之后就可以看到所有的详情页都已经被我们构造出来了(由于内容太多，我就截取了一部分)：</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/21.png" alt="9"></p><blockquote><h3 id="解析详情页"><a href="#解析详情页" class="headerlink" title="解析详情页"></a>解析详情页</h3></blockquote><p>下面是对详情页进行解析，然后从详情页中提取下载页的链接，可以看到下载链接大致有三种情况(720P，1080P，网盘下载)，我们要做相应的处理，我们分析一下下面这两种情况的源代码：</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/22.png" alt="10"></p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/23.png" alt="11"></p><p>先来看这个下载地址只有1080P的源代码：<br><img src="http://ovfqn6f2x.bkt.clouddn.com/24.png" alt="12"></p><p>再看看下载地址既有720P又有1080P的：<br><img src="http://ovfqn6f2x.bkt.clouddn.com/25.png" alt="13"></p><blockquote><p><strong>可以看到下载链接所在的位置是相同的，都是class为p_list_02下的ul下的li标签下的a标签内。在最前面的下载地址的链接格式是‘/down/11471-0-0.html’，第二种下载格式为‘/down/11471-1-0.html’，仔细观察这两中下载格式，在/down/后面使用-把数字分成了三部分，每一部分都是有意义的，第一部分是电影的编号，第二部分是用来区分下载方式/down/11471-(0,1,2)-0.html，0表示第一种下载方式，1表示第二种下载方式，2表示第三种下载方式(其中720P，1080P，4K各是一种下载方式)，另外网盘下载是另外的一种方式，网盘下载给出的是网盘的地址，我们不考虑这种情况。第三部分是区分同一种下载方式有几个，比如720P的下载链接给了3个，那么就分别对应/down/11471-0-(0,1,2).html。以上就是下载链接的规则，我们只爬取第一种下载方式，因为每一部电影都会至少给出一种下载的链接，如果是720P的我们就保存720P，是1080P就保存1080P的种子下载页面。</strong></p></blockquote><hr><blockquote><h3 id="获取下载链接"><a href="#获取下载链接" class="headerlink" title="获取下载链接"></a>获取下载链接</h3></blockquote><figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> <span class="keyword">j</span> in link<span class="variable">s:</span></div><div class="line">    htmls = requests.<span class="built_in">get</span>(<span class="keyword">j</span>).content</div><div class="line">    soups = BeautifulSoup(htmls, <span class="string">'lxml'</span>)</div><div class="line">    <span class="keyword">con</span> = soups.<span class="keyword">find</span>(<span class="string">'div'</span>, attrs=&#123;<span class="string">'id'</span>: <span class="string">'nucms_downlist'</span>&#125;).<span class="keyword">find</span>(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'p_list'</span>&#125;).<span class="keyword">find</span>(<span class="string">'ul'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'p_list_02'</span>&#125;).<span class="keyword">find</span>(<span class="string">'li'</span>).<span class="keyword">find</span>(<span class="string">'a'</span>)[<span class="string">'href'</span>]</div><div class="line">    <span class="keyword">print</span>(<span class="keyword">con</span>)</div></pre></td></tr></table></figure><p>我们在对页面进行爬取的时候，爬取的内容被重定向了，显示的内容不是我们想要的：</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/26.png" alt="14"></p><p>那么我们该怎样解决这种情况呢？出现这种情况是因为网站加入了反爬虫策略。首先，它的下载链接是异步请求的，主要通过Ajax实现异步请求，大家可以自行去了解ajax是什么，异步请求又是什么。<br>我们可以使用chrome浏览器查看网页源代码，查看Network下的XHR，ajax请求都在这里：</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/29.png" alt="15"></p><p>点击这个11504.html?timestamp=150606149934的请求可以看到我们需要的链接就在这个请求的Preview里面：</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/30.png" alt="16"></p><p>再看一下Response的内容，响应的内容就是我们想要的东西，是以HTML格式返回的：</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/31.png" alt="17"></p><p>我们只需要对这个链接进行请求，然后直接就可以提取我们想要的种子链接。<br>我们可以看到链接是由域名加上电影编号以及一个timestamp参数构成，电影编号是有规律的，电影编号是从1开始的，依次加一。而这个timestamp是一个时刻在变化的参数，它是一个时间戳。关于时间戳的概念大家可以自己去了解。它的这个链接是电影编号加上请求时刻的时间戳。知道规则之后，我们就可以通过相同的方法去构造这种链接。</p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">import <span class="built_in">time</span></div><div class="line"><span class="keyword">from</span> urllib.parse import urlencode</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="built_in">in</span> range(<span class="number">1</span>, <span class="number">100</span>):</div><div class="line">    ticks = <span class="built_in">time</span>.<span class="built_in">time</span>()</div><div class="line">    timestamp = int(ticks * <span class="number">1000</span>)</div><div class="line">    <span class="built_in">params</span> = &#123;</div><div class="line">        <span class="string">'timestamp'</span>: timestamp</div><div class="line">    &#125;</div><div class="line">    url = <span class="string">'http://www.btbtdy.com/vidlist/'</span> + <span class="built_in">str</span>(i) + <span class="string">'.html?'</span> + urlencode(<span class="built_in">params</span>)</div><div class="line">    print(url)</div></pre></td></tr></table></figure><p>这里我们导入了time模块，用来构造时间戳，另外从urllib.parse中导入urlencode对链接进行编码。ticks 会得到时间戳，但是返回的是浮点数（1506064260.6314487），根据链接的格式我们需要构造十三位的时间戳，我们通过对我们构造的时间戳乘以1000再取整得到十三位的时间戳，最后把我们这些东西构造成一个完整的url，打印出来可以看到：<br><img src="http://ovfqn6f2x.bkt.clouddn.com/32.png" alt="18"></p><p>从上面运行的结果可以看到，时间戳没有改变，但是我们构造的链接还是可以打开的，通过实际操作我发现它这个timestamp参数并不是严格按照时间戳来的，你只要随便写一个十三位的数字就可以请求成功。<br>这里我们随便点开一个链接可以看到：<br><img src="https://i.loli.net/2018/03/17/5aace23e1274f.png" alt="19"></p><p>这里就得到我们想要的东西，我们再从这里提取我们需要的东西就可以了。</p><blockquote><h3 id="完整代码如下："><a href="#完整代码如下：" class="headerlink" title="完整代码如下："></a>完整代码如下：</h3></blockquote><figure class="highlight xl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python3</div><div class="line"># encoding=utf-<span class="number">8</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> <span class="built_in">time</span></div><div class="line"><span class="keyword">import</span> requests</div><div class="line">from bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line">from urllib.parse <span class="keyword">import</span> urlencode</div><div class="line"></div><div class="line"><span class="keyword">with</span> open(<span class="string">'种子.txt'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fp:</div><div class="line">    <span class="keyword">for</span> i <span class="built_in">in</span> range(<span class="number">1</span>, <span class="number">11505</span>):</div><div class="line">        headers = &#123;</div><div class="line">            <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'</span></div><div class="line">        &#125;</div><div class="line">        ticks = <span class="built_in">time</span>.<span class="built_in">time</span>()</div><div class="line">        timestamp = int(ticks * <span class="number">1000</span>)</div><div class="line">        params = &#123;</div><div class="line">            <span class="string">'timestamp'</span>: timestamp</div><div class="line">        &#125;</div><div class="line">        url = <span class="string">'http://www.btbtdy.com/vidlist/'</span> + str(i) + <span class="string">'.html?'</span> + urlencode(params)</div><div class="line">        html = requests.get(url, headers=headers).content</div><div class="line">        soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">        try:</div><div class="line">            <span class="built_in">title</span> = soup.find(<span class="string">'ul'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'p_list_02'</span>&#125;).find(<span class="string">'li'</span>).find(<span class="string">'a'</span>)[<span class="string">'title'</span>]</div><div class="line">            seed = soup.find(<span class="string">'ul'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'p_list_02'</span>&#125;).find(<span class="string">'a'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'d1'</span>&#125;)[<span class="string">'href'</span>]</div><div class="line">            <span class="keyword">data</span> = &#123;</div><div class="line">                <span class="string">'电影名称:'</span>: <span class="built_in">title</span>,</div><div class="line">                <span class="string">'种子链接:'</span>: seed</div><div class="line">            &#125;</div><div class="line">            fp.write(str(<span class="keyword">data</span>)+<span class="string">'\n'</span>)</div><div class="line">            print(<span class="string">'正在下载第%s个种子！'</span> % i)</div><div class="line">        except:</div><div class="line">            pass</div></pre></td></tr></table></figure><p>代码运行效果如下：</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/34.png" alt="20"></p><blockquote><h2 id="另辟蹊径"><a href="#另辟蹊径" class="headerlink" title="另辟蹊径"></a>另辟蹊径</h2></blockquote><h3 id="继续分析"><a href="#继续分析" class="headerlink" title="继续分析"></a>继续分析</h3><blockquote><p>这是详情页：<a href="http://www.btbtdy.com/btdy/dy11460.html" target="_blank" rel="external">http://www.btbtdy.com/btdy/dy11460.html</a><br>这是下载页：<a href="http://www.btbtdy.com/down/11460-0-0.html" target="_blank" rel="external">http://www.btbtdy.com/down/11460-0-0.html</a></p></blockquote><p>可以看出来，下载页的链接后面的电影编号跟详情页里面的数字编号是一样的，这样我们就可以通过从详情页中提取电影编号，然后再自己构造下载页，在构造的过程中我发现这个编号是按顺序来的，从1开始，到最新的电影的编号为止，这样我们连主页都不用解析，直接构造下载页面：<br>通过分析下载页面的源代码可以看到，下载页面包括电影名称已经种子链接，我们可以通过下载页面可以直接爬取电影名称以及种子链接：</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/27.png" alt="21"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python3</span></div><div class="line"><span class="comment"># encoding=utf-8</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line">links = []</div><div class="line"><span class="keyword">for</span> link <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">10000</span>):</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        html = requests.get(url, headers=headers).text</div><div class="line">        soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">        title = soup.find_all(<span class="string">'p'</span>)[<span class="number">0</span>].getText()</div><div class="line">        link = soup.find_all(<span class="string">'p'</span>)[<span class="number">1</span>].getText()</div><div class="line">        data = &#123;</div><div class="line">            <span class="string">'电影名称:'</span>: title,</div><div class="line">            <span class="string">'种子链接:'</span>: link</div><div class="line">          &#125;</div><div class="line">        print(data)</div><div class="line">    <span class="keyword">except</span>:</div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure><p>上面的代码进行了异常处理，处理的方式就是对异常不处理。运行结果如下：</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/28.png" alt="22"></p><p>可以看到我们使用上面的21行代码就可以把所有的电影及其种子链接爬取下来。</p><blockquote><h2 id="保存到本地"><a href="#保存到本地" class="headerlink" title="保存到本地"></a>保存到本地</h2></blockquote><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><p><strong>下面的完整代码是目前(2018-3-17)刚改版过的，加入了多线程，爬取速度更快。</strong><br><figure class="highlight xl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python3</div><div class="line"># encoding=utf-<span class="number">8</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> requests</div><div class="line">from bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line">from multiprocessing <span class="keyword">import</span> Pool</div><div class="line"></div><div class="line">def get_seed(i):</div><div class="line">    url = <span class="string">'http://www.btbtdy.com/downlist/'</span> + str(i) + <span class="string">'-0-0.html'</span></div><div class="line">    headers = &#123;</div><div class="line">        <span class="string">'Host'</span>: <span class="string">'www.btbtdy.com'</span>,<span class="keyword">import</span> requests</div><div class="line">from bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"></div><div class="line">links = []</div><div class="line"><span class="keyword">with</span> open(<span class="string">'电影种子.txt'</span>, <span class="string">'w+'</span>) <span class="keyword">as</span> fp:</div><div class="line">    <span class="keyword">for</span> link <span class="built_in">in</span> range(<span class="number">1</span>, <span class="number">11471</span>):</div><div class="line">        try:</div><div class="line">            rel_url = <span class="string">'http://www.btbtdy.com/down/'</span> + str(link) + <span class="string">'-0-0.html'</span></div><div class="line">            html = requests.get(rel_url).content</div><div class="line">            soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">            <span class="built_in">title</span> = soup.find(<span class="string">'h1'</span>).getText()</div><div class="line">            downlist = soup.find(<span class="string">'form'</span>, attrs=&#123;<span class="string">'name'</span>: <span class="string">'form2'</span>, <span class="string">'id'</span>: <span class="string">'form2'</span>&#125;).find(<span class="string">'input'</span>)[<span class="string">'value'</span>]</div><div class="line">            <span class="keyword">data</span> = &#123;</div><div class="line">                <span class="string">'电影名称:'</span>: <span class="built_in">title</span>,</div><div class="line">                <span class="string">'种子链接:'</span>: downlist</div><div class="line">            &#125;</div><div class="line">            fp.write(str(<span class="keyword">data</span>)+<span class="string">'\n'</span>)</div><div class="line">            print(<span class="keyword">data</span>)</div><div class="line">        except:</div><div class="line">            pass</div><div class="line">        <span class="string">'Upgrade-Insecure-Requests'</span>: <span class="string">'1'</span>,</div><div class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'</span></div><div class="line">    &#125;</div><div class="line">    try:</div><div class="line">        html = requests.get(url, headers=headers).<span class="keyword">text</span></div><div class="line">        soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">        <span class="built_in">title</span> = soup.find_all(<span class="string">'p'</span>)[<span class="number">0</span>].getText()</div><div class="line">        link = soup.find_all(<span class="string">'p'</span>)[<span class="number">1</span>].getText()</div><div class="line">        <span class="keyword">data</span> = &#123;</div><div class="line">            <span class="string">'电影名称:'</span>: <span class="built_in">title</span>,</div><div class="line">            <span class="string">'种子链接:'</span>: link</div><div class="line">        &#125;</div><div class="line">        print(<span class="keyword">data</span>)</div><div class="line">        <span class="keyword">with</span> open(<span class="string">'seed.txt'</span>, <span class="string">'a+'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> sd:</div><div class="line">            sd.write(str(<span class="keyword">data</span>) + <span class="string">'\n'</span>)</div><div class="line">    except:</div><div class="line">        <span class="keyword">with</span> open(<span class="string">'error.txt'</span>, <span class="string">'a+'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> e:</div><div class="line">            e.write(<span class="string">'遇到异常，链接的id是: '</span> + str(i) + <span class="string">'\n'</span>)</div><div class="line">        pass</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    pool = Pool(<span class="number">10</span>)</div><div class="line">    <span class="built_in">page</span> = [x <span class="keyword">for</span> x <span class="built_in">in</span> range(<span class="number">1</span>, <span class="number">12411</span>)]</div><div class="line">    pool.map(get_seed, <span class="built_in">page</span>)</div><div class="line">    pool.close()</div><div class="line">    pool.join()</div></pre></td></tr></table></figure></p><blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><pre><code>这次的爬虫很简单，代码没怎么进行解释，相信大家如果学习过基础知识还是能够看懂的，实在看不懂的可以给我留言，我会解释的。这次的爬虫思路很新奇，思路是十分重要的，特别是在编程上，不同的思路可能给你带来的工作量的差别很大，本次爬虫我采用了两种思路，第一种是普通的思路：从原网页进行解析，一步一步的对目标(下载页面)进行靠拢。这种思路是最简单的思路，当然随着网页的复杂，可能工作量会特别大。第二种思路就是上面用到的思路，因为网页规律性很强，很少会有人给每一个页面进行命名，特别是页面非常多的网站，仔细观察，他们可能区别很小，我们可以直接跳过前面的解析，直接观察目标的特点。当然这个思路肯定不适合所有的站点，所以大家的思路还是要活一点，同一个问题多思考一下，在思路上多花点时间完全是值得的，不要看到问题就想都不想的去解决，不要把思维固定了。在思路上花时间完全是值得的，另外无论是怎样的思路，一定要清晰，不然只会把自己搞迷。另外，这里涉及到了一些异步请求，ajax，时间戳的知识，我没有进行介绍，大家可以自己去查询一下。这些东西随着你写的爬虫一步步的强大起来，肯定都会遇到的。</code></pre></blockquote><hr><h3 id="项目在我的GitHub上，欢迎查看，记得点star哦！"><a href="#项目在我的GitHub上，欢迎查看，记得点star哦！" class="headerlink" title="项目在我的GitHub上，欢迎查看，记得点star哦！"></a><strong>项目在<a href="https://github.com/Tactful-biao/scrapy" target="_blank" rel="external">我的GitHub</a>上，欢迎查看，记得点star哦！</strong></h3><hr><blockquote><p>文章如有出错的地方，请多多指正！如果你感觉文章对你有所帮助，文章下方有打赏按钮，您随意打赏。支持作者继续创作。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;相信大家都有到处找资源找种子的经历(不管是什么种子)。为何不自己写一个爬虫把一个网站的所有的种子链接都爬取下来呢？&lt;br&gt;所以接下来我们就通过一个简单的爬虫程序把电影天堂的所有的种子链接爬取下来。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;b
      
    
    </summary>
    
      <category term="网络爬虫" scheme="https://www.bbiao.me/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python" scheme="https://www.bbiao.me/tags/Python/"/>
    
      <category term="爬虫" scheme="https://www.bbiao.me/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="种子" scheme="https://www.bbiao.me/tags/%E7%A7%8D%E5%AD%90/"/>
    
  </entry>
  
  <entry>
    <title>爬取糗事百科</title>
    <link href="https://www.bbiao.me/2017/09/01/%E7%88%AC%E5%8F%96%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91/"/>
    <id>https://www.bbiao.me/2017/09/01/爬取糗事百科/</id>
    <published>2017-09-01T05:48:53.000Z</published>
    <updated>2017-10-14T06:14:38.678Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>今天是伟大的九月一号，万千学生的噩梦。吓得我赶紧打打代码压压惊。程序员打代码才是王道。</p></blockquote><p>废话不多说，开始我们今天的正题：爬取糗事百科，前面两篇文章我们分别学习了Python的requests库和正则表达式，今天我们就用它们来爬去糗事百科，怎样才能更快的学习并掌握编程，当然是做项目啦。很多人都会有这种感觉，自己已经跟着基础教程学完了，为什么做一个项目的时候发现自己还是无从下手。就跟盖房子的一样，刚学完基础教程就相当于以已经有了材料了，但是如果盖房子只是有材料是不够了，你还要知道怎么设计房子，怎样建牢固。这些不可能一下就能够掌握，怎么才能快速掌握呢？多练。 熟能生巧，只有练得多了，自然而然的就会了。</p><p>今天，我们来爬去糗事百科<a href="https://www.qiushibaike.com/hot/" target="_blank" rel="external">https://www.qiushibaike.com/hot/</a>，这上面可谓段子手云集。那么我们就来会会这些段子手们。</p><blockquote><h2 id="先理下思路"><a href="#先理下思路" class="headerlink" title="先理下思路"></a>先理下思路</h2></blockquote><p>第一步：分析网页结构(找到我们要爬取的内容所在位置)<br>第二步：解析网页<br>第三步：保存目标内容</p><blockquote><h3 id="打开目标网址"><a href="#打开目标网址" class="headerlink" title="打开目标网址"></a>打开目标网址</h3></blockquote><p><img src="http://ovfqn6f2x.bkt.clouddn.com/6.png" alt="1"></p><p>从图片可以看到，糗事百科的内容有文本也有图片，还有混杂的，这些我们在实际爬取中都要考虑到。首先我们要明确我们要爬取的内容，在这里我们爬去糗事百科的文本内容以及一些图片。</p><p>在chrome下按F12可以查看源代码(在文字内容上点击鼠标右键–&gt;检查 可以快速定位HTML位置)：</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/7.png" alt="2"></p><p><strong>从上面的源代码中可以看到每条内容都包含在一个单独的class为article block untagged mb15 typs….的div里面，而这所以的内容又包含在id为content-left class为col1的div里，仔细观察可以看到内容所在的标签中class的内容并不是完全一样的，总共有三种，分别是typs_long、type_hot、typs_old。我们再仔细观察一下具体的内容所在的标签以及图片所在的标签位置：</strong></p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/8.png" alt="3"></p><p>从上图可以看到内容所在的标签是在class为content的div下面的span中，图片在class为thumb的div下面的a标签下面的img标签。图同圈出来的分别是内容和图片所在标签位置。</p><hr><blockquote><h3 id="过程分析"><a href="#过程分析" class="headerlink" title="过程分析"></a>过程分析</h3></blockquote><p>我们面临这一个问题，因为我们的图片是没法和文字保存在一起的，我们只能把文字放在文本中，图片另外保存，所以我们打算这样做，如果内容中有图片，那么我们就把图片保存下来但是不保存其文字内容，如果没有图片则把文字内容保存下来。下面我们开始实战代码：</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"></div><div class="line">url = <span class="string">'http://www.qiushibaike.com/hot/'</span></div><div class="line">html = requests.<span class="keyword">get</span>(url).content</div><div class="line"><span class="keyword">return</span> html</div></pre></td></tr></table></figure><p>上面的代码我们通过requests库想指定网址发送get请求，获取到指定页面的HTML源代码.</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">from bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"> </div><div class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">content = soup.<span class="built_in">find</span>(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>:<span class="string">'article block untagged mb15 typs_long'</span>&#125;)</div><div class="line"><span class="built_in">text</span> = content.<span class="built_in">find</span>(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'content'</span>&#125;)</div><div class="line"><span class="built_in">print</span>(<span class="built_in">text</span>.getText())</div></pre></td></tr></table></figure><p>运行上面的代码就可以打印出来第一条内容，如果你打印的内容和你网页上当前打印的不一样，不用担心这不是出错，因为糗事百科的内容是一直在更新的，你爬取的是当前最新的内容。<br>上面只实现了爬取一条内容，我们想要的是所有的内容，首先先把当前页的内容爬取下来：<br>通过修改上面的代码我们就可以爬取当前页面所有的class为article block untagged mb15 typs_hot下的所有的文字内容：</p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">content = soup.<span class="built_in">find</span>(<span class="string">'div'</span>, attrs=&#123;<span class="string">'id'</span>:<span class="string">'content-left'</span>, <span class="string">'class'</span>: <span class="string">'col1'</span>&#125;)</div><div class="line">link_lists = content.find_all(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'article block untagged mb15 typs_hot'</span>&#125;)</div><div class="line"><span class="keyword">for</span> link <span class="built_in">in</span> link_lists:</div><div class="line">    <span class="built_in">text</span> = link.<span class="built_in">find</span>(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>, <span class="string">'content'</span>&#125;)</div><div class="line">    print(<span class="built_in">text</span>.<span class="built_in">getText</span>())</div></pre></td></tr></table></figure><blockquote><h3 id="处理翻页"><a href="#处理翻页" class="headerlink" title="处理翻页"></a>处理翻页</h3></blockquote><p><img src="http://ovfqn6f2x.bkt.clouddn.com/9.png" alt="4"><br><img src="http://ovfqn6f2x.bkt.clouddn.com/10.png" alt="5"></p><p>可以看到页面翻转在class为pagination的ur下的li里面，可以看到最后一个li的内容是下一页，倒数第二个li标签里面才是真正包含总页数的数字，并且不同页面的差别只是最后的数字不同。我们只需要把它提取处理，然后再根据总页数通过循环构造不同页面的链接。</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">html = requests.get(url).content</div><div class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">content = soup.<span class="keyword">find</span>(<span class="string">'ul'</span>, attrs=&#123;<span class="string">'class'</span>:<span class="string">'pagination'</span>&#125;)</div><div class="line">links = []</div><div class="line"><span class="keyword">for</span> link in content.find_all(<span class="string">'li'</span>):</div><div class="line">    <span class="keyword">if</span> link.<span class="keyword">find</span>(<span class="string">'span'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'page-numbers'</span>&#125;):</div><div class="line">        nums = link.<span class="keyword">find</span>(<span class="string">'span'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'page-numbers'</span>&#125;)</div><div class="line">        num = nums.<span class="keyword">getText</span>()</div><div class="line">        links.<span class="keyword">append</span>(<span class="keyword">int</span>(num))</div><div class="line"><span class="keyword">print</span>(links[-<span class="number">1</span>])</div></pre></td></tr></table></figure><p>通过上面的代码就可以返回总页数，解释一下上面的代码，首先我们通过find查找到所有页数所在的标签，然后通过循环查找该标签下的所有的li子标签，然后我们通过if判断li标签下面是否有class为page-numbers的span，如果存在，则获取该标签下的文本信息，即页码数，然后再把找到的所有的页数转化成int类型之后保存到links列表里面。最后我们就可以通过列表的切片方法把总页数找出来，这种方法可能比较麻烦，这是我目前能够想到的方法，随着以后学习的加深，再慢慢优化吧。当然如果你有什么好的方法，欢迎与博主进行交流。</p><blockquote><h3 id="构造链接"><a href="#构造链接" class="headerlink" title="构造链接"></a>构造链接</h3></blockquote><p>我们得到总页数之后下一步就是通过循环构造每一页的链接，只有最终构造成链接。我们才能进行访问翻页。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">each_pages = []</div><div class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> range(<span class="number">1</span>, link[-<span class="number">1</span>]+<span class="number">1</span>)):</div><div class="line">    each_page = url + <span class="string">'page/'</span> + str(i)</div><div class="line">    each_pages.append(each_page)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(each_pages)</span></span></div></pre></td></tr></table></figure><p>上面的代码通过url + ‘page/’ + str(i) 构造出来完成的不同页的链接。通过循环把所有的链接都放在each_pages这个列表里面。</p><p>当然在上面我们分析网址源代码的时候发现内容所在的标签有三种，article block untagged mb15 typs_hot、article block untagged mb15 typs_old、article block untagged mb15 typs_long.而我们演示的时间只是抓取了数量最多的hot类型，我们当然不想舍弃数据，虽然old和long的内容不多，但是蚊子腿再细，它也是肉啊，下面我们就来简单介绍一下怎么处理这一块。思路是使用我们上一章刚讲过的正则表达式，通过正则表达式我们就可以把这三个内容筛选出来，然后再使用循环依次去抓取不同标签的内容</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"> </div><div class="line">url = <span class="string">'http://www.qiushibaike.com/hot/'</span></div><div class="line">html = requests.get(url).content</div><div class="line">soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">pattern = re.<span class="keyword">compile</span>(<span class="string">'article block untagged mb15 typs_\w+'</span>)</div><div class="line">content = soup.<span class="keyword">find</span>(<span class="string">'div'</span>, attrs=&#123;<span class="string">'id'</span>: <span class="string">'content-left'</span>, <span class="string">'class'</span>: <span class="string">'col1'</span>&#125;)</div><div class="line">attrs  = re.<span class="keyword">findall</span>(pattern, str(soup))</div><div class="line"><span class="keyword">print</span>(set(attrs))</div></pre></td></tr></table></figure><p>通过上面的代码就可以把三个不同的标签内容筛选出来。首先我们通过re.compile()写出我们需要找的标签长什么样子，观察可以发现，我们要找的标签前面article block untagged mb15 typs_都是一样的，唯一的区别就在后的一点点，而后面又都是字母，我们可以使用\w+进行统配，具体使用可以参考我的这篇文章《正则表达式》。然后通过re.findall(pattern, string[, flags])进行全文匹配，而匹配的全文就是我们通过BeautifulSoup获得的HTML源代码，这里我们需要使用str()函数，把源代码转化成字符串的格式，这样才能进行查找。但是由于全文中可能有很多同样的标签，这样就会抓到很多重复的，我们可以通过set()进行去重，set()的用法这里不再说了，不了解的可以给博主留言，最终就可以筛选出来我们想要的内容。</p><blockquote><h2 id="最后一步-图文分离"><a href="#最后一步-图文分离" class="headerlink" title="最后一步 图文分离"></a>最后一步 图文分离</h2></blockquote><p><img src="http://ovfqn6f2x.bkt.clouddn.com/11.png" alt="6"></p><p>可以看到，每一个文章链接下都有一个图片的模块，但是真正有图片的内容多了个模块class为thumb的div，这里面放着图片。我们就可以通过它来判断是否有图片，如果有图片，我们就保存图片，如果没有图片，我们就保存内容。</p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">import codecs</div><div class="line"> </div><div class="line"><span class="keyword">with</span> codecs.open(<span class="string">'duanzi'</span>, <span class="string">'wb'</span>, <span class="string">'utf-8'</span>) as fp:</div><div class="line">    n = <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> i <span class="built_in">in</span> url:</div><div class="line">        html = get_html(i)</div><div class="line">        soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">        content = soup.<span class="built_in">find</span>(<span class="string">'div'</span>, attrs=&#123;<span class="string">'id'</span>: <span class="string">'content-left'</span>, <span class="string">'class'</span>: <span class="string">'col1'</span>&#125;)</div><div class="line">        text_lists = []</div><div class="line">        <span class="keyword">for</span> attr <span class="built_in">in</span> <span class="built_in">set</span>(re.findall(pattern, <span class="built_in">str</span>(soup))):</div><div class="line">            <span class="keyword">for</span> text_list <span class="built_in">in</span> content.find_all(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: attr&#125;):</div><div class="line">                <span class="keyword">if</span> text_list.<span class="built_in">find</span>(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>: <span class="string">'thumb'</span>&#125;):</div><div class="line">                    img_link = text_list.<span class="built_in">find</span>(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>: <span class="string">'thumb'</span>&#125;)</div><div class="line">                    img = img_link.<span class="built_in">find</span>(<span class="string">'img'</span>)[<span class="string">'src'</span>]</div><div class="line">                    rel_img = <span class="string">'https:'</span> + img</div><div class="line">                    print(<span class="string">'正在下载第%s张图片'</span> % n)</div><div class="line">                    filename = (<span class="string">'第%s张.jpg'</span> % n)</div><div class="line">                    <span class="keyword">with</span> open(filename, <span class="string">'wb+'</span>) as jpg:</div><div class="line">                        jpg.write(requests.get(rel_img).content)</div><div class="line">                    n += <span class="number">1</span></div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    <span class="built_in">text</span> = text_list.<span class="built_in">find</span>(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'content'</span>&#125;).<span class="built_in">find</span>(<span class="string">'span'</span>).<span class="built_in">getText</span>()</div><div class="line">                    text_lists.<span class="built_in">append</span>(<span class="built_in">text</span>)</div><div class="line">        fp.write(<span class="string">'&#123;duanzi&#125;'</span>.<span class="built_in">format</span>(duanzi=<span class="string">'\n'</span>.<span class="built_in">join</span>(text_lists)))</div></pre></td></tr></table></figure><p>以上就是所有的内容，运行后的结果如下：<br>都是些什么鬼图 </p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/12.png" alt="7"></p><p>打开我们的文本内容，可以看到所有的段子也都已经抓取下来了。</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/13.png" alt="8"></p><blockquote><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python3</span></div><div class="line"><span class="comment"># encoding=utf-8</span></div><div class="line"><span class="comment">### 时间 2017-9-1 ###</span></div><div class="line"><span class="comment">### 项目地址：https://github.com/Tactful-biao/scrapy</span></div><div class="line"> </div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">import</span> codecs</div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"> </div><div class="line">__author__ = <span class="string">'sunshibiao'</span></div><div class="line"> </div><div class="line">pattern = re.compile(<span class="string">'article block untagged mb15 typs_\w+'</span>)</div><div class="line">url = <span class="string">'http://www.qiushibaike.com/hot/'</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_html</span><span class="params">(url)</span>:</span></div><div class="line">    <span class="string">''' 获取html '''</span></div><div class="line">    html = requests.get(url).content</div><div class="line">    <span class="keyword">return</span> html</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_page_list</span><span class="params">(html)</span>:</span></div><div class="line">    <span class="string">''' 构造所有的页面 '''</span></div><div class="line"> </div><div class="line">    next_page_url = url</div><div class="line">    html = get_html(next_page_url)</div><div class="line">    soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">    content = soup.find(<span class="string">'ul'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'pagination'</span>&#125;)</div><div class="line">    links = []</div><div class="line">    each_pages = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> content.find_all(<span class="string">'li'</span>):</div><div class="line">        <span class="keyword">if</span> i.find(<span class="string">'span'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'page-numbers'</span>&#125;):</div><div class="line">            nums = i.find(<span class="string">'span'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'page-numbers'</span>&#125;)</div><div class="line">            num = nums.getText()</div><div class="line">            links.append(int(num))</div><div class="line"> </div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> range(<span class="number">1</span>, links[<span class="number">-1</span>]+<span class="number">1</span>):</div><div class="line">        each_page = next_page_url + <span class="string">'page/'</span> + str(link)</div><div class="line">        each_pages.append(each_page)</div><div class="line">    <span class="keyword">return</span> each_pages</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_text_or_pic</span><span class="params">(url)</span>:</span></div><div class="line">    <span class="string">''' 获取文本内容并写入文件, 如果是图片就保存图片 '''</span></div><div class="line">    <span class="keyword">with</span> codecs.open(<span class="string">'duanzi'</span>, <span class="string">'wb'</span>, <span class="string">'utf-8'</span>) <span class="keyword">as</span> fp:</div><div class="line">        n = <span class="number">1</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> url:</div><div class="line">            html = get_html(i)</div><div class="line">            soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">            content = soup.find(<span class="string">'div'</span>, attrs=&#123;<span class="string">'id'</span>: <span class="string">'content-left'</span>, <span class="string">'class'</span>: <span class="string">'col1'</span>&#125;)</div><div class="line">            text_lists = []</div><div class="line">            <span class="keyword">for</span> attr <span class="keyword">in</span> set(re.findall(pattern, str(soup))):</div><div class="line">                <span class="keyword">for</span> text_list <span class="keyword">in</span> content.find_all(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: attr&#125;):</div><div class="line">                    <span class="keyword">if</span> text_list.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>: <span class="string">'thumb'</span>&#125;):</div><div class="line">                        img_link = text_list.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>: <span class="string">'thumb'</span>&#125;)</div><div class="line">                        img = img_link.find(<span class="string">'img'</span>)[<span class="string">'src'</span>]</div><div class="line">                        rel_img = <span class="string">'https:'</span> + img</div><div class="line">                        print(<span class="string">'正在下载第%s张图片'</span> % n)</div><div class="line">                        filename = (<span class="string">'第%s张.jpg'</span> % n)</div><div class="line">                        <span class="keyword">with</span> open(filename, <span class="string">'wb+'</span>) <span class="keyword">as</span> jpg:</div><div class="line">                            jpg.write(requests.get(rel_img).content)</div><div class="line">                        n += <span class="number">1</span></div><div class="line">                    <span class="keyword">else</span>:</div><div class="line">                        text = text_list.find(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'content'</span>&#125;).find(<span class="string">'span'</span>).getText()</div><div class="line">                        text_lists.append(text)</div><div class="line">            fp.write(<span class="string">'&#123;duanzi&#125;'</span>.format(duanzi=<span class="string">'\n'</span>.join(text_lists)))</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">  get_text_or_pic(get_page_list(get_html(url)))</div><div class="line"> </div><div class="line"> </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure><blockquote><h2 id="项目地址：https-github-com-Tactful-biao-scrapy-如果对你有帮助，记得点个Star哦。"><a href="#项目地址：https-github-com-Tactful-biao-scrapy-如果对你有帮助，记得点个Star哦。" class="headerlink" title="项目地址：https://github.com/Tactful-biao/scrapy 如果对你有帮助，记得点个Star哦。"></a>项目地址：<a href="https://github.com/Tactful-biao/scrapy" target="_blank" rel="external">https://github.com/Tactful-biao/scrapy</a> 如果对你有帮助，记得点个Star哦。</h2></blockquote><hr><blockquote><h3 id="文章如有出错的地方，请多多指正！如果你感觉文章对你有所帮助，文章下方有打赏按钮，您随意打赏。支持作者继续创作。"><a href="#文章如有出错的地方，请多多指正！如果你感觉文章对你有所帮助，文章下方有打赏按钮，您随意打赏。支持作者继续创作。" class="headerlink" title="文章如有出错的地方，请多多指正！如果你感觉文章对你有所帮助，文章下方有打赏按钮，您随意打赏。支持作者继续创作。"></a>文章如有出错的地方，请多多指正！如果你感觉文章对你有所帮助，文章下方有打赏按钮，您随意打赏。支持作者继续创作。</h3></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;今天是伟大的九月一号，万千学生的噩梦。吓得我赶紧打打代码压压惊。程序员打代码才是王道。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;废话不多说，开始我们今天的正题：爬取糗事百科，前面两篇文章我们分别学习了Python的requests库和正则表达式，
      
    
    </summary>
    
      <category term="网络爬虫" scheme="https://www.bbiao.me/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="requests" scheme="https://www.bbiao.me/tags/requests/"/>
    
      <category term="爬虫" scheme="https://www.bbiao.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>正则表达式</title>
    <link href="https://www.bbiao.me/2017/08/31/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <id>https://www.bbiao.me/2017/08/31/正则表达式/</id>
    <published>2017-08-31T15:10:57.000Z</published>
    <updated>2017-10-15T09:30:10.516Z</updated>
    
    <content type="html"><![CDATA[<p>很多语言都支持正则表达式，而正则表达式是爬虫中经常需要使用的，所以我们在这里介绍一下在Python中怎样使用正则表达式。</p><blockquote><h2 id="什么是正则表达式？"><a href="#什么是正则表达式？" class="headerlink" title="什么是正则表达式？"></a>什么是正则表达式？</h2></blockquote><p>正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组合一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。它能帮助你方便的检查一个字符串是否与某种模式匹配<br>正则表达式的大致匹配过程：</p><ol><li>依次拿出表达式和文本的字符比较</li><li>如果每一个字符都能够匹配，则匹配成功；一旦有匹配不成功的字符则匹配失败。</li><li>如果表达式中有量词或边界，这个过程会稍微有一些不同</li></ol><blockquote><h2 id="正则表达式的语法规则"><a href="#正则表达式的语法规则" class="headerlink" title="正则表达式的语法规则"></a>正则表达式的语法规则</h2></blockquote><p><img src="http://ovfqn6f2x.bkt.clouddn.com/1.jpg" alt="1"></p><blockquote><h2 id="re-模块"><a href="#re-模块" class="headerlink" title="re 模块"></a>re 模块</h2></blockquote><p>Python的re模块让Python拥有了全部的正则表达式的功能，re常见的方法列举如下：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># 返回<span class="selector-tag">pattern</span>对象</div><div class="line"><span class="selector-tag">re</span><span class="selector-class">.compile</span>(<span class="selector-tag">string</span><span class="selector-attr">[,flag]</span>)</div><div class="line"># 下面的是用来匹配的函数</div><div class="line"><span class="selector-tag">re</span><span class="selector-class">.match</span>(<span class="selector-tag">pattern</span>, <span class="selector-tag">string</span><span class="selector-attr">[, flags]</span>)</div><div class="line"><span class="selector-tag">re</span><span class="selector-class">.search</span>(<span class="selector-tag">pattern</span>, <span class="selector-tag">string</span><span class="selector-attr">[, flags]</span>)</div><div class="line"><span class="selector-tag">re</span><span class="selector-class">.split</span>(<span class="selector-tag">pattern</span>, <span class="selector-tag">string</span><span class="selector-attr">[, maxsplit]</span>)</div><div class="line"><span class="selector-tag">re</span><span class="selector-class">.findall</span>(<span class="selector-tag">pattern</span>, <span class="selector-tag">string</span><span class="selector-attr">[, flags]</span>)</div><div class="line"><span class="selector-tag">re</span><span class="selector-class">.finditer</span>(<span class="selector-tag">pattern</span>, <span class="selector-tag">string</span><span class="selector-attr">[, flags]</span>)</div><div class="line"><span class="selector-tag">re</span><span class="selector-class">.sub</span>(<span class="selector-tag">pattern</span>, <span class="selector-tag">repl</span>, <span class="selector-tag">string</span><span class="selector-attr">[, count]</span>)</div><div class="line"><span class="selector-tag">re</span><span class="selector-class">.subn</span>(<span class="selector-tag">pattern</span>, <span class="selector-tag">repl</span>, <span class="selector-tag">string</span><span class="selector-attr">[, count]</span>)</div></pre></td></tr></table></figure></p><p>compile函数根据模式字符串和可选的标志参数生成一个正则表达式对象。该对象拥有一系列方法用于正则表达式匹配和替换，其中compile会返回一个pattern对象。</p><p>compile(pattern[,flags]) 根据包含正则表达式的字符串创建模式对象。<br>可以先使用Python的help函数查看compile的介绍：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import re</div><div class="line"><span class="function"><span class="title">help</span><span class="params">(re.compile)</span></span></div></pre></td></tr></table></figure></p><p>返回如下内容：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="title">compile</span><span class="params">(pattern, flags=<span class="number">0</span>)</span></span></div><div class="line">Compile <span class="selector-tag">a</span> regular expression pattern, returning <span class="selector-tag">a</span> pattern <span class="selector-tag">object</span>.</div></pre></td></tr></table></figure></p><p>可以看到返回了一个pattern对象，但是却没有对第二个对象flags进行介绍。第二个参数flags是匹配模式，可以使用按位或’ |’表示同时生效(例如：re.I | re.M)，也可以在正则表达式字符串中指定。pattern对象是不能直接实例化的，只能通过compile方法得到。匹配模式有：<br><figure class="highlight livescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="number">1</span>）re.I(re.IGNORECASE) :忽略大小写</div><div class="line"><span class="number">2</span>）re.M(re.MULTILINE) :多行模式，改变‘ ^’和’ $’的行为</div><div class="line"><span class="number">3</span>）re.S(DOTALL) : 点任意匹配模式，改变’ .’的行为</div><div class="line"><span class="number">4</span>）re.L(LOCALL) : 使预定字符类 <span class="string">\w</span> <span class="string">\W</span> <span class="string">\b</span> <span class="string">\B</span> <span class="string">\s</span> <span class="string">\S</span> 取决于当前区域设定</div><div class="line"><span class="number">5</span>).re.U(UNICODE): 使预定字符类 <span class="string">\w</span> <span class="string">\W</span> <span class="string">\b</span> <span class="string">\B</span> <span class="string">\s</span> <span class="string">\S</span> <span class="string">\d</span> <span class="string">\D</span> 取决于unicode定义的字符属性</div><div class="line"><span class="number">6</span>).re.X(VERBOSE): 详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释</div></pre></td></tr></table></figure></p><p>通过compile函数返回的pattern之后，我们就可以使用匹配函数进行匹配了:</p><blockquote><h3 id="re-match-模块"><a href="#re-match-模块" class="headerlink" title="re.match 模块"></a>re.match 模块</h3></blockquote><figure class="highlight coq"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">re.<span class="keyword">match</span>(<span class="built_in">pattern</span>, string[, flags])</div></pre></td></tr></table></figure><p>re.match函数会尝试从字符串的其实位置匹配一个模式，如果不是起始位置匹配成功的话，match()就会返回None。</p><p>参数介绍：</p><blockquote><p>参数 描述<br>pattern 要匹配的正则表达式<br>string 要匹配的字符串</p><h2 id="flags-标志位，用于控制正则表达式的匹配方式"><a href="#flags-标志位，用于控制正则表达式的匹配方式" class="headerlink" title="flags 标志位，用于控制正则表达式的匹配方式"></a>flags 标志位，用于控制正则表达式的匹配方式</h2><p>我们可以使用grup(num) 或 groups()匹配对象函数来匹配表达式：<br>匹配对象方法 描述<br>group(num=0) 匹配的整个 表达式的字符串，group()可以一次输入多个组号，<br>在这种情况下它将返回一个包含哪些组所对应值的元组。<br>groups() 返回一个包含所有小组的字符串的元组，从1到所含的小组号。<br>下面我们举个简单的例子：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"> </div><div class="line">pattern = re.compile(<span class="string">r'bbiao'</span>)</div><div class="line">print(re.match(pattern, <span class="string">'bbiao'</span>).group())   <span class="comment"># 在起始位置匹配</span></div><div class="line">print(re.match(pattern, <span class="string">'www.bbiao.me'</span>))   <span class="comment"># 不在起始位置匹配</span></div><div class="line">print(re.match(pattern, <span class="string">'bbiao.me'</span>).group())</div></pre></td></tr></table></figure><p>上面的代码运行结果如下：</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">bbiao</span></div><div class="line"><span class="keyword">None</span></div><div class="line"><span class="keyword">bbiao</span></div></pre></td></tr></table></figure><p>从上面的例子可以看到，第一个print返回了我们要匹配的字符串，这个毫无疑问，因为它和我们的字符串完全一样，所有肯定会匹配。</p><p>第二个print虽然也包含我们的目标字符串，但是它不在首位，re.match函数是从首位开始进行匹配的，如果首位不匹配就不进行匹配了，会返回None，这里不给它加group是因为 ‘NoneType’ object has no attribute ‘group’。</p><p>第三个也匹配成功了，因为它同样包含我们的目标字符串，并且它也在首位，这样当我们匹配到最后一个字母‘o’之后就不在继续进行匹配了，并且返回匹配成功的信息。</p><p>match对象是一次匹配的结果，包含很多关于此次匹配的信息，可以使用如下属性或方法来获取这些信息：</p><blockquote><p>属性<br>1、 string：匹配时使用的文本。<br>2、 re：匹配时使用的pattern对象<br>3、 pos： 文本中正则表达式开始搜索的索引。值与pattern.match()和pattern.search()方法同名参数相同。<br>4、 endpos: 文本中正则表达式结束搜索的索引。 值与pattern.match()和和pattern.search()方法同名参数相同。<br>5、 lastindex：最后一个被捕获的分组在文本中的索引。如果没有被捕获的分组，将为None。<br>6、 lastgroup：最后一个被捕获的分组的别名。如果这个分组没有别名或者没有被捕获的分组，将为None/</p></blockquote><hr><blockquote><p>方法</p><ol><li>group([default]):<br>获得一个或多个分组截获的字符串；指定多个参数时将元组形式返回。group1可以使用编号也可以使用别名；编号0代表整个匹配的子串；不填写参数时，返回group(0)；没有截获字符串的组返回None；截获了多次的组返回最后一次截获的子串。</li><li>groups([default]):<br>以元组形式返回全部分组截获的字符串。相当于调用group(1, 2, ….last)。default表示没有截获字符串的组以这个值替代，默认为None。</li><li>groupdict([default]):<br>返回以有别名的组的别名为键、以该组截获的子串为值的字典，没有别名的组不包含在内。default含义同上。</li><li>start([group]):<br>返回指定的组截获的子串在string中的起始索引（子串第一个字符的索引）。group默认值为0.</li><li>end([group]):<br>返回指定的组截获的子串在string中的结束索引（子串第一个字符的索引+1）。group默认值为0.</li><li>span([group]):<br>返回(start(group), end(group))</li><li>expand(template):<br>将匹配到的分组带入template中然后返回。template可以使用\id或\g、\g引用分组，但不能使用编号0.\id与\g等价；但\10将被认为是第10个分组，如果你想表达\1之后是字符’ 0’，只能使用\g0<br>下面再来看一看的例子</li></ol></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"> </div><div class="line">line = <span class="string">"Cats are smarter than dogs"</span></div><div class="line"> </div><div class="line">matchobj = re.match(<span class="string">r'(.*) are (.*?) .*'</span>, line, re.M | re.I)</div><div class="line"> </div><div class="line"><span class="keyword">if</span> matchobj:</div><div class="line">    print(<span class="string">'matchobj.sting:'</span>, matchobj.string)</div><div class="line">    print(<span class="string">'matchobj.re:'</span>, matchobj.re)</div><div class="line">    print(<span class="string">'matchobj.pos:'</span>, matchobj.pos)</div><div class="line">    print(<span class="string">'matchobj.endpos:'</span>, matchobj.endpos)</div><div class="line">    print(<span class="string">'matchobj.lastindex:'</span>, matchobj.lastindex)</div><div class="line">    print(<span class="string">'matchobj.lastgroup:'</span>, matchobj.lastgroup)</div><div class="line">    print(<span class="string">'matchobj.group() :'</span>, matchobj.group())</div><div class="line">    print(<span class="string">'matchobj.group(1):'</span>, matchobj.group(<span class="number">1</span>))</div><div class="line">    print(<span class="string">'matchobj.group(2)'</span>, matchobj.group(<span class="number">2</span>))</div><div class="line">    print(<span class="string">'matchobj.groups():'</span>, matchobj.groups())</div><div class="line">    print(<span class="string">'matchobj.groupdict():'</span>, matchobj.groupdict())</div><div class="line">    print(<span class="string">'matchobj.start:(2)'</span>, matchobj.start(<span class="number">2</span>))</div><div class="line">    print(<span class="string">'matchobj.end:(2)'</span>, matchobj.end(<span class="number">2</span>))</div><div class="line">    print(<span class="string">'matchobj.span:(2)'</span>, matchobj.span(<span class="number">2</span>))</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    print(<span class="string">'No match!!'</span>)</div></pre></td></tr></table></figure><p>代码运行结果：</p><figure class="highlight less"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">matchobj</span><span class="selector-class">.sting</span>: <span class="selector-tag">Cats</span> <span class="selector-tag">are</span> <span class="selector-tag">smarter</span> <span class="selector-tag">than</span> <span class="selector-tag">dogs</span></div><div class="line"><span class="selector-tag">matchobj</span><span class="selector-class">.re</span>: <span class="selector-tag">re</span><span class="selector-class">.compile</span>(<span class="string">'(.*) are (.*?) .*'</span>, re.IGNORECASE|re.MULTILINE)</div><div class="line"><span class="selector-tag">matchobj</span><span class="selector-class">.pos</span>: <span class="selector-tag">0</span></div><div class="line"><span class="selector-tag">matchobj</span><span class="selector-class">.endpos</span>: <span class="selector-tag">26</span></div><div class="line"><span class="selector-tag">matchobj</span><span class="selector-class">.lastindex</span>: <span class="selector-tag">2</span></div><div class="line"><span class="selector-tag">matchobj</span><span class="selector-class">.lastgroup</span>: <span class="selector-tag">None</span></div><div class="line"><span class="selector-tag">matchobj</span><span class="selector-class">.group</span>() : <span class="selector-tag">Cats</span> <span class="selector-tag">are</span> <span class="selector-tag">smarter</span> <span class="selector-tag">than</span> <span class="selector-tag">dogs</span></div><div class="line"><span class="selector-tag">matchobj</span><span class="selector-class">.group</span>(<span class="number">1</span>): <span class="selector-tag">Cats</span></div><div class="line"><span class="selector-tag">matchobj</span><span class="selector-class">.group</span>(<span class="number">2</span>) <span class="selector-tag">smarter</span></div><div class="line"><span class="selector-tag">matchobj</span><span class="selector-class">.groups</span>(): (<span class="string">'Cats'</span>, <span class="string">'smarter'</span>)</div><div class="line"><span class="selector-tag">matchobj</span><span class="selector-class">.groupdict</span>(): &#123;&#125;</div><div class="line"><span class="selector-tag">matchobj</span><span class="selector-class">.start</span><span class="selector-pseudo">:(2)</span> <span class="selector-tag">9</span></div><div class="line"><span class="selector-tag">matchobj</span><span class="selector-class">.end</span><span class="selector-pseudo">:(2)</span> <span class="selector-tag">16</span></div><div class="line"><span class="selector-tag">matchobj</span><span class="selector-class">.span</span><span class="selector-pseudo">:(2)</span> (<span class="number">9</span>, <span class="number">16</span>)</div></pre></td></tr></table></figure><blockquote><h3 id="re-search-模块"><a href="#re-search-模块" class="headerlink" title="re.search 模块"></a>re.search 模块</h3></blockquote><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">re</span><span class="selector-class">.search</span>(<span class="selector-tag">pattern</span>, <span class="selector-tag">string</span><span class="selector-attr">[, flags]</span>)</div></pre></td></tr></table></figure><p>search方法与match方法十分相似，区别在于match()函数只检测re是不是在string的开始位置匹配，search()会扫描整个string查找匹配，match()只有在0位置匹配成功的话才有返回，如果不是开始位置匹配成功的话，match()就返回None。同样，search方法的返回对象同样match()返回同样的方法和属性。举个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"> </div><div class="line">pattern = re.compile(<span class="string">r'world'</span>)</div><div class="line">match = re.search(pattern, <span class="string">'hello world!'</span>)</div><div class="line"><span class="keyword">if</span> match:</div><div class="line">    print(match.group())</div></pre></td></tr></table></figure><p>上面的代码运行结果是(代码很简单，参考上面的，match可以很容易理解，这里不做解释了)：</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attribute">world</span></div></pre></td></tr></table></figure><blockquote><h3 id="re-splite-模块"><a href="#re-splite-模块" class="headerlink" title="re.splite 模块"></a>re.splite 模块</h3></blockquote><figure class="highlight coq"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">re.<span class="built_in">split</span>(<span class="built_in">pattern</span>, string[, maxsplit])</div></pre></td></tr></table></figure><p>按照能够匹配的子串将string分割后返回列表。maxsplit用于指定最大分割次数，不指定将全部分割，我们同样举个例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"> </div><div class="line">pattern = re.compile(<span class="string">r'\d+'</span>)</div><div class="line">print(re.split(pattern, <span class="string">'one1two2three3four'</span>))</div></pre></td></tr></table></figure></p><p>输出</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="symbol">'one</span>', <span class="symbol">'two</span>', <span class="symbol">'three</span>', <span class="symbol">'four</span>']</div></pre></td></tr></table></figure><blockquote><h3 id="re-findall-模块"><a href="#re-findall-模块" class="headerlink" title="re.findall 模块"></a>re.findall 模块</h3></blockquote><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">re</span><span class="selector-class">.findall</span>(<span class="selector-tag">pattern</span>, <span class="selector-tag">string</span><span class="selector-attr">[, flags]</span>)</div></pre></td></tr></table></figure><p>搜索string， 以列表形式返回全部能匹配的子串。举例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"> </div><div class="line">pattern = re.compile(<span class="string">r'\d+'</span>)</div><div class="line">print(re.findall(pattern, <span class="string">'one1two2three3four4'</span>))</div></pre></td></tr></table></figure><p>输出结果</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="symbol">'1</span>', <span class="symbol">'2</span>', <span class="symbol">'3</span>', <span class="symbol">'4</span>']</div></pre></td></tr></table></figure><blockquote><h3 id="re-finditerl模块"><a href="#re-finditerl模块" class="headerlink" title="re.finditerl模块"></a>re.finditerl模块</h3></blockquote><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">re</span><span class="selector-class">.finditerl</span>(<span class="selector-tag">pattern</span>, <span class="selector-tag">string</span><span class="selector-attr">[, flags]</span>)</div></pre></td></tr></table></figure><p>搜索string，返回一个顺序访问每一个匹配结果(Match对象)的迭代器。例子如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"> </div><div class="line">pattern = re.compile(<span class="string">r'\d+'</span>)</div><div class="line"><span class="keyword">for</span> m <span class="keyword">in</span> re.finditer(pattern, <span class="string">'one1two2three3four4'</span>):</div><div class="line">    print(m.group(),)</div></pre></td></tr></table></figure></p><p>输出结果 </p><figure class="highlight basic"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">1 </span><span class="number">2</span> <span class="number">3</span> <span class="number">4</span></div></pre></td></tr></table></figure><blockquote><h3 id="re-sub模块"><a href="#re-sub模块" class="headerlink" title="re.sub模块"></a>re.sub模块</h3></blockquote><figure class="highlight applescript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">re.sub(pattern, repl, <span class="built_in">string</span>[, <span class="built_in">count</span>])</div></pre></td></tr></table></figure><p>使用repl替换string中每一个匹配的子串后返回替换后的字符串。当repl是一个字符串时，可以使用\id或\g、\g引用分组，但不能使用编号0.<br>当repl是一个方法时，这个方法应当只接受一个参数(Match对象)，并返回一个字符串用于替换(返回的字符串中不能再引用分组)。<br>count用于指定最多替换次数不指定时全部替换。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"> </div><div class="line">pattern = re.compile(<span class="string">r'(\w+) (\w+)'</span>)</div><div class="line">s = <span class="string">'i say, hello word!'</span></div><div class="line"> </div><div class="line">print(re.sub(pattern, <span class="string">r'\2 \1'</span>, s))</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(m)</span>:</span></div><div class="line">    <span class="keyword">return</span> m.group(<span class="number">1</span>).title() + <span class="string">' '</span> + m.group(<span class="number">2</span>).title()</div><div class="line"> </div><div class="line">print(re.sub(pattern, func, s))</div></pre></td></tr></table></figure></p><p>输出结果</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">say</span> i, <span class="built_in">word</span> hello!</div><div class="line">I Say, Hello Word!</div></pre></td></tr></table></figure><blockquote><h3 id="re-subn"><a href="#re-subn" class="headerlink" title="re.subn"></a>re.subn</h3></blockquote><figure class="highlight applescript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">re.subn(pattern, repl, <span class="built_in">string</span>[, <span class="built_in">count</span>])</div></pre></td></tr></table></figure><p>返回(sub(repl, string[, count]), 替换次数)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line"> </div><div class="line">pattern = re.compile(<span class="string">r'(\w+) (\w+)'</span>)</div><div class="line">s = <span class="string">'i say, hello world!'</span></div><div class="line"> </div><div class="line">print(re.subn(pattern, <span class="string">r'\2 \1'</span>, s))</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(m)</span>:</span></div><div class="line">    <span class="keyword">return</span> m.gtoup(<span class="number">1</span>).title() + <span class="string">' '</span> + m.group(<span class="number">2</span>).title()</div><div class="line"> </div><div class="line">print(re.subn(pattrtn, func, s))</div></pre></td></tr></table></figure><p>输出结果<br><figure class="highlight scheme"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">(<span class="symbol">'say</span> i, world hello!', <span class="number">2</span>)</div><div class="line">(<span class="symbol">'I</span> Say, Hello World!', <span class="number">2</span>)</div></pre></td></tr></table></figure></p><blockquote><h3 id="Python-Re模块的另一种使用方式"><a href="#Python-Re模块的另一种使用方式" class="headerlink" title="Python Re模块的另一种使用方式"></a>Python Re模块的另一种使用方式</h3></blockquote><p>在上面我们介绍了7个工具方法，例如match，search等等，不过调用方式都是 re.match，re.search的方式，其实还有另外一种调用方式，可以通过pattern.match，pattern.search调用，这样调用便不用将pattern作为第一个参数传入了，大家想怎样调用皆可。<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">search</span>(<span class="built_in">string</span>[, pos[, endpos]]) | re.<span class="built_in">search</span>(pattern, <span class="built_in">string</span>[, flags])</div><div class="line"><span class="keyword">split</span>(<span class="built_in">string</span>[, maxsplit]) | re.<span class="keyword">split</span>(pattern, <span class="built_in">string</span>[, maxsplit])</div><div class="line">findall(<span class="built_in">string</span>[, pos[, endpos]]) | re.findall(pattern, <span class="built_in">string</span>[, flags])</div><div class="line">finditer(<span class="built_in">string</span>[, pos[, endpos]]) | re.finditer(pattern, <span class="built_in">string</span>[, flags])</div><div class="line">sub(repl, <span class="built_in">string</span>[, <span class="built_in">count</span>]) | re.sub(pattern, repl, <span class="built_in">string</span>[, <span class="built_in">count</span>])</div><div class="line">subn(repl, <span class="built_in">string</span>[, <span class="built_in">count</span>]) |re.sub(pattern, repl, <span class="built_in">string</span>[, <span class="built_in">count</span>])</div></pre></td></tr></table></figure></p><p>选择哪一种都可以。正则表达式的内容大致就这么多，想要掌握唯一的方法就是多加练习。</p><p>参考：<a href="http://cuiqingcai.com/977.html" target="_blank" rel="external">http://cuiqingcai.com/977.html</a></p><blockquote><p>文章如有出错的地方，请多多指正！如果你感觉文章对你有所帮助，文章下方有打赏按钮，您随意打赏。支持作者继续创作。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;很多语言都支持正则表达式，而正则表达式是爬虫中经常需要使用的，所以我们在这里介绍一下在Python中怎样使用正则表达式。&lt;/p&gt;
&lt;blockquote&gt;
&lt;h2 id=&quot;什么是正则表达式？&quot;&gt;&lt;a href=&quot;#什么是正则表达式？&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="Python" scheme="https://www.bbiao.me/categories/Python/"/>
    
    
      <category term="Python" scheme="https://www.bbiao.me/tags/Python/"/>
    
      <category term="正则表达式" scheme="https://www.bbiao.me/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>爬虫神器 ——requests</title>
    <link href="https://www.bbiao.me/2017/08/30/%E7%88%AC%E8%99%AB%E7%A5%9E%E5%99%A8-%E2%80%94%E2%80%94requests/"/>
    <id>https://www.bbiao.me/2017/08/30/爬虫神器-——requests/</id>
    <published>2017-08-30T11:45:48.000Z</published>
    <updated>2017-10-14T12:16:11.661Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>今天我们来介绍一下requests库，这可是python在爬虫方面的神器，有了它在使用python进行爬虫方面的操作的时候简直不能太爽。requests号称“HTTP for Human”，它的功能十分强大，而它的操作却十分简单。</p></blockquote><p>python提供的http库有很多，包括自带的urllib和urllib2都提供了强大的HTTP支持，但是API借口太难用了，requests就简单很多。</p><blockquote><h2 id="requests库安装"><a href="#requests库安装" class="headerlink" title="requests库安装"></a>requests库安装</h2></blockquote><p>可以使用下面的方法进行安装：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip <span class="keyword">install</span> requests</div></pre></td></tr></table></figure><blockquote><h2 id="requests的使用方法："><a href="#requests的使用方法：" class="headerlink" title="requests的使用方法："></a>requests的使用方法：</h2></blockquote><h3 id="发送请求"><a href="#发送请求" class="headerlink" title="发送请求"></a>发送请求</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line"> </div><div class="line">r = request.get(<span class="string">'http://bbiao.me'</span>)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(type(r)</span></span>)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(r.status_code)</span></span></div><div class="line"><span class="function"><span class="title">print</span><span class="params">(r.encoding)</span></span></div><div class="line"><span class="function"><span class="title">print</span><span class="params">(r.coolies)</span></span></div></pre></td></tr></table></figure><p>上面的代码运行后得到如下结果：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="section">&lt;class 'requests.models.Response'&gt;</span></div><div class="line"><span class="attribute">200</span></div><div class="line"><span class="attribute">UTF</span>-8</div><div class="line"><span class="section">&lt;RequestsCookieJar[]&gt;</span></div></pre></td></tr></table></figure><p>只需要使用上面的代码就可以获得一个网址的类型、状态码、编码、cookies等内容，是不是很方便呢？后面还有更方便的呢。</p><p>除了get请求，基本请求有以下几种：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="attr">r</span> = requests.post(<span class="string">'http://httpbin.org/post'</span>)</div><div class="line"><span class="attr">r</span> = requests.put(<span class="string">'http://httpbin.org/put'</span>)</div><div class="line"><span class="attr">r</span> = requests.delete(<span class="string">'http://httpbin.org/delete'</span>)</div><div class="line"><span class="attr">r</span> = requests.hrad(<span class="string">'http://httpbin.orh/head'</span>)</div><div class="line"><span class="attr">r</span> = request.options(<span class="string">"http://httpbin.org/get"</span>)</div></pre></td></tr></table></figure></p><p>针对不同的请求同样只需要一句话就可以。</p><p>在爬虫的使用过程中最常用的就是get请求方法，当然GET请求方法就使用get方法。</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">r</span> = requests.get(<span class="string">'http://bbiao.me'</span>)</div></pre></td></tr></table></figure><p>如果请求的时候想加上参数，可以使用params参数：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"> </div><div class="line">payload = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>, <span class="string">'value2'</span>&#125;</div><div class="line">r = requests.<span class="built_in">get</span>(<span class="string">'http://bbiao.me/get'</span>, params=payload)</div><div class="line"><span class="built_in">print</span>(r.url)</div></pre></td></tr></table></figure><p>运行的话请求的url就是：</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">http:</span><span class="comment">//bbiao.me/get?key2=value2&amp;key1=value1</span></div></pre></td></tr></table></figure><p>当我们写爬虫的时候有时候需要我们构造请求，有很多的参数需要我们构造，这时候我们就可以把这些参数写入一个列表或者json文件里面：<br>例如写一个json文件命名为a.json，内容如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[<span class="string">"foo"</span>, <span class="string">"bar"</span>, &#123;</div><div class="line">    <span class="attr">"foo"</span>: <span class="string">"bar"</span></div><div class="line">&#125;]</div></pre></td></tr></table></figure><p>使用requests进行解析：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line"> </div><div class="line">r = requests.<span class="builtin-name">get</span>(<span class="string">"a.json"</span>)</div><div class="line"><span class="builtin-name">print</span>(r.text)</div><div class="line"><span class="builtin-name">print</span>(r.json())</div></pre></td></tr></table></figure><p>运行之后会看到如下结果(其中一个是直接输出内容，另外一个方法是利用json()方法解析)：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[<span class="string">"foo"</span>, <span class="string">"bar"</span>, &#123;</div><div class="line"> <span class="string">"foo"</span>: <span class="string">"bar"</span></div><div class="line"> &#125;]</div><div class="line"> [<span class="string">u'foo'</span>, <span class="string">u'bar'</span>, &#123;<span class="string">u'foo'</span>: <span class="string">u'bar'</span>&#125;]</div></pre></td></tr></table></figure><p>在面对反爬虫机制的时候，我们可能需要构造请求头部，如果想要想要传入headers，可以传入headers参数：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"> </div><div class="line">payload = &#123;<span class="symbol">'key</span>1': <span class="symbol">'value</span>1', <span class="symbol">'key</span>2':<span class="symbol">'value</span>2'&#125;</div><div class="line">headers = &#123;<span class="symbol">'content</span>-<span class="class"><span class="keyword">type</span>'</span>: <span class="symbol">'application</span>/json'&#125;</div><div class="line">r = requests.get(<span class="symbol">'http</span>:<span class="comment">//httpbin.org/get', params=payload, headers=headers)</span></div><div class="line">print(r.url)</div></pre></td></tr></table></figure><blockquote><h3 id="基本的post请求"><a href="#基本的post请求" class="headerlink" title="基本的post请求"></a>基本的post请求</h3></blockquote><p>对于post请求来说，我们一般需要为它增加一些参数，最基本的传参方法可以利用data这个参数：</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"> </div><div class="line">payload = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>:<span class="string">'value2'</span>&#125;</div><div class="line">r = requests.post(<span class="string">'http://httpbin.org/post'</span>,<span class="keyword">data</span>=payload)</div><div class="line">print(r.<span class="keyword">text</span>)</div></pre></td></tr></table></figure><p>运行之后可以得到如下内容：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"args"</span>: &#123;&#125;, </div><div class="line">  <span class="attr">"data"</span>: <span class="string">""</span>, </div><div class="line">  <span class="attr">"files"</span>: &#123;&#125;, </div><div class="line">  <span class="attr">"form"</span>: &#123;</div><div class="line">    <span class="attr">"key1"</span>: <span class="string">"value1"</span>, </div><div class="line">    <span class="attr">"key2"</span>: <span class="string">"value2"</span></div><div class="line">  &#125;, </div><div class="line">  <span class="attr">"headers"</span>: &#123;</div><div class="line">    <span class="attr">"Accept"</span>: <span class="string">"*/*"</span>, </div><div class="line">    <span class="attr">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>, </div><div class="line">    <span class="attr">"Connection"</span>: <span class="string">"close"</span>, </div><div class="line">    <span class="attr">"Content-Length"</span>: <span class="string">"23"</span>, </div><div class="line">    <span class="attr">"Content-Type"</span>: <span class="string">"application/x-www-form-urlencoded"</span>, </div><div class="line">    <span class="attr">"Host"</span>: <span class="string">"httpbin.org"</span>, </div><div class="line">    <span class="attr">"User-Agent"</span>: <span class="string">"python-requests/2.18.4"</span></div><div class="line">  &#125;, </div><div class="line">  <span class="attr">"json"</span>: <span class="literal">null</span>,  </div><div class="line">  <span class="attr">"url"</span>: <span class="string">"http://httpbin.org/post"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>可以看到参数已经上传成功了，然后服务器返回了我们传的数据</p><p>如果我们想要传入json格式的数据，可以使用json.dumps()方法把表单数据序列化：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">import</span> json</div><div class="line"> </div><div class="line"><span class="title">url</span> = <span class="string">"http://httpbin.org/post"</span></div><div class="line"><span class="title">payload</span> = &#123;'some': '<span class="class"><span class="keyword">data</span>'&#125;</span></div><div class="line"><span class="title">r</span> = requests.post(url, <span class="class"><span class="keyword">data</span>=json.dumps(<span class="title">payload</span>))</span></div><div class="line"><span class="title">print</span>(r.text)</div></pre></td></tr></table></figure><p>运行结果:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"args"</span>: &#123;&#125;, </div><div class="line">  <span class="attr">"data"</span>: <span class="string">"&#123;\"some\": \"data\"&#125;"</span>, </div><div class="line">  <span class="attr">"files"</span>: &#123;&#125;, </div><div class="line">  <span class="attr">"form"</span>: &#123;&#125;, </div><div class="line">  <span class="attr">"headers"</span>: &#123;</div><div class="line">    <span class="attr">"Accept"</span>: <span class="string">"*/*"</span>, </div><div class="line">    <span class="attr">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>, </div><div class="line">    <span class="attr">"Connection"</span>: <span class="string">"close"</span>, </div><div class="line">    <span class="attr">"Content-Length"</span>: <span class="string">"16"</span>, </div><div class="line">    <span class="attr">"Host"</span>: <span class="string">"httpbin.org"</span>, </div><div class="line">    <span class="attr">"User-Agent"</span>: <span class="string">"python-requests/2.18.4"</span></div><div class="line">  &#125;, </div><div class="line">  <span class="attr">"json"</span>: &#123;</div><div class="line">    <span class="attr">"some"</span>: <span class="string">"data"</span></div><div class="line">  &#125;, </div><div class="line">  <span class="attr">"url"</span>: <span class="string">"http://httpbin.org/post"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>通过上面的方法，我们可以传入json格式的数据，如果想要上传文件可以使用files参数，新建一个a.txt的文件，内容写上hello word！</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line"> </div><div class="line">url = <span class="string">"http://httpbin.org/post"</span></div><div class="line"><span class="keyword">files</span> = &#123;<span class="string">'file'</span> :<span class="keyword">open</span>(<span class="string">'a.txt'</span>, <span class="string">'rb'</span>)&#125;</div><div class="line">r = requests.post(url, <span class="keyword">files</span>=<span class="keyword">files</span>)</div><div class="line"><span class="keyword">print</span>(r.text)</div></pre></td></tr></table></figure><p>运行之后可以看到如下结果：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"args"</span>: &#123;&#125;, </div><div class="line">  <span class="attr">"data"</span>: <span class="string">""</span>, </div><div class="line">  <span class="attr">"files"</span>: &#123;</div><div class="line">    <span class="attr">"file"</span>: <span class="string">"hello word!\n"</span></div><div class="line">  &#125;, </div><div class="line">  <span class="attr">"form"</span>: &#123;&#125;, </div><div class="line">  <span class="attr">"headers"</span>: &#123;</div><div class="line">    <span class="attr">"Accept"</span>: <span class="string">"*/*"</span>, </div><div class="line">    <span class="attr">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>, </div><div class="line">    <span class="attr">"Connection"</span>: <span class="string">"close"</span>, </div><div class="line">    <span class="attr">"Content-Length"</span>: <span class="string">"153"</span>, </div><div class="line">    <span class="attr">"Content-Type"</span>: <span class="string">"multipart/form-data; boundary=91d3dfe3cd4e4f8094644cee3445b4eb"</span>, </div><div class="line">    <span class="attr">"Host"</span>: <span class="string">"httpbin.org"</span>, </div><div class="line">    <span class="attr">"User-Agent"</span>: <span class="string">"python-requests/2.18.4"</span></div><div class="line">  &#125;, </div><div class="line">  <span class="attr">"json"</span>: <span class="literal">null</span>, </div><div class="line">  <span class="attr">"url"</span>: <span class="string">"http://httpbin.org/post"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>通过结果可以看到我们上传的file内容等等，说明我们的文件已经上传成功了。<br>另外requests是支持流式上传的，这允许你发送大的数据流或文件而无需先把它们读入内存。要使用流式上传，仅需要你的请求提供一个类文件对象即可：</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'massive-body'</span>) <span class="keyword">as</span> f:</div><div class="line">    requests.<span class="built_in">post</span>(<span class="string">'http://some.url/streamed'</span>, data=f)</div></pre></td></tr></table></figure><p>这是一个非常方便的功能。</p><blockquote><h3 id="Cookies"><a href="#Cookies" class="headerlink" title="Cookies"></a>Cookies</h3></blockquote><p>什么是cookies这里就不介绍了，不懂的自己去查一下。<br>如果一个响应中包含了cookies，那么我们可以利用cookies变量拿到</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line"> </div><div class="line">url = <span class="string">"http:example.com"</span></div><div class="line">r = requests.get(url)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(r.cookies)</span></span></div><div class="line"><span class="function"><span class="title">print</span><span class="params">(r.cookies[<span class="string">'example_cookie_name'</span>])</span></span></div></pre></td></tr></table></figure><p>上面的代码只是个样例，可以用cookies变量来得到站点的cookies，另外可以利用cookies变量向服务器发送cookies信息</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line"> </div><div class="line">url = <span class="string">"http://httpbin.org/cookies"</span></div><div class="line">cookies = dict(<span class="attribute">cookies_are</span>=<span class="string">'working'</span>)</div><div class="line">r = requests.<span class="builtin-name">get</span>(url,  <span class="attribute">cookies</span>=cookies)</div><div class="line"><span class="builtin-name">print</span>(r.text)</div></pre></td></tr></table></figure><p>运行之后可以得到如下结果：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"cookies"</span>: &#123;</div><div class="line">    <span class="attr">"cookies_are"</span>: <span class="string">"working"</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>可以看到已经向服务器发送了cookies。</p><blockquote><h3 id="超时配置"><a href="#超时配置" class="headerlink" title="超时配置"></a>超时配置</h3></blockquote><p>可以通过timeout变量来配置最大请求时间<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">requests.<span class="built_in">get</span>(<span class="string">'http://githum.com'</span>, timeout=<span class="number">0.001</span>)</div></pre></td></tr></table></figure></p><p>需要额外注意的是这个timeout不是说你设置了你的访问就会变快，这个没有一毛钱的关系，timeout仅对链接过程有效，与响应体的下载无关，也就是说，这个时间只是限制请求时间，返回的内容有多大该花多少时间还是多少时间。</p><blockquote><h3 id="会话对象"><a href="#会话对象" class="headerlink" title="会话对象"></a>会话对象</h3></blockquote><p>在上面的请求中，每次请求都相当于发起了一个新的请求。也就是相当于我们每个请求都用了不同的浏览器单独打开的效果。也就是它并不是指的一个会话，即使请求相同的网址，例如：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line"> </div><div class="line">requests.<span class="builtin-name">get</span>(<span class="string">"http://httpbin.org/cookies/set/sessioncookie/123456789"</span>)</div><div class="line">r = requests.<span class="builtin-name">get</span>(<span class="string">"http://httpbin.org/cookies"</span>)</div><div class="line"><span class="builtin-name">print</span>(r.text)</div></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"cookies"</span>: &#123;&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>从返回的结果可以看到，这不在一个会话中，这种情况就好比，我们登录知乎，来到知乎主页，然后我们点击一篇文章又让我们登录一次，如果不能保持会话就会导致我们没进行一步操作都会让我们重新输入密码进行身份验证，这显然不符合逻辑和用户体验，所以建立持久会话是必要的，那么该怎样建立持久会话呢？</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line"> </div><div class="line">s = requests.Session()</div><div class="line">s .<span class="builtin-name">get</span>(<span class="string">"http://httpbin.org/cookies/set/sessioncookie/123456789"</span>)</div><div class="line">r = s.<span class="builtin-name">get</span>(<span class="string">"http://httpbin.org/cookies"</span>)</div><div class="line"><span class="builtin-name">print</span>(r.text)</div></pre></td></tr></table></figure><p>在这里我们同样请求了两次，一次是设置cookies，一次是获得cookies</p><p>运行结果：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"cookies"</span>: &#123;</div><div class="line">    <span class="attr">"sessioncookie"</span>: <span class="string">"123456789"</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>可以看到，我们成功获取到cookies了，这就是建立一个会话的作用和方法，体会一下。</p><p>既然会话是全局变量，那么我们肯定可以用来全局的配置。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line"> </div><div class="line">s = requests.Session()</div><div class="line">s<span class="selector-class">.headers</span><span class="selector-class">.update</span>(&#123;<span class="string">'x-text'</span>: <span class="string">'true'</span>&#125;)</div><div class="line">r = s.get(<span class="string">'http://httpbin.org/headers'</span>, headers=&#123;<span class="string">'x-text2'</span> :<span class="string">'true'</span>&#125;)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(r.text)</span></span></div></pre></td></tr></table></figure><p>通过s.headers.update方法设置了headers的变量。然后我们又在请求中设置一个headers，那么会出现什么结果呢？</p><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"headers"</span>: &#123;</div><div class="line">    <span class="attr">"Accept"</span>: <span class="string">"*/*"</span>, </div><div class="line">    <span class="attr">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>, </div><div class="line">    <span class="attr">"Connection"</span>: <span class="string">"close"</span>, </div><div class="line">    <span class="attr">"Cookie"</span>: <span class="string">"sessioncookie=123456789"</span>, </div><div class="line">    <span class="attr">"Host"</span>: <span class="string">"httpbin.org"</span>, </div><div class="line">    <span class="attr">"User-Agent"</span>: <span class="string">"python-requests/2.18.4"</span>, </div><div class="line">    <span class="attr">"X-Text"</span>: <span class="string">"true"</span>, </div><div class="line">    <span class="attr">"X-Text2"</span>: <span class="string">"true"</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>可以看到两个变量都传送过去了。</p><p>如果使用GET方法传的headers ，结果会一样吗？</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">r</span> = s.get(<span class="string">'http://httpbin.org/headers'</span>, headers=&#123;<span class="string">'x-text'</span>: <span class="string">'true'</span>&#125;)</div></pre></td></tr></table></figure><p>运行结果可以看到，它会覆盖掉全局的配置：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"headers"</span>: &#123;</div><div class="line">    <span class="attr">"Accept"</span>: <span class="string">"*/*"</span>, </div><div class="line">    <span class="attr">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>, </div><div class="line">    <span class="attr">"Connection"</span>: <span class="string">"close"</span>, </div><div class="line">    <span class="attr">"Cookie"</span>: <span class="string">"sessioncookie=123456789"</span>, </div><div class="line">    <span class="attr">"Host"</span>: <span class="string">"httpbin.org"</span>, </div><div class="line">    <span class="attr">"User-Agent"</span>: <span class="string">"python-requests/2.18.4"</span>, </div><div class="line">    <span class="attr">"X-Text"</span>: <span class="string">"true"</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>如果你不想要全局配置中的变量了，可以把他对应的值设置为None，就可以了：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attr">r</span> = s.get(<span class="string">'http://httpbin.org/headers'</span>, headers=&#123;<span class="string">'x-test'</span>: None&#125;)</div></pre></td></tr></table></figure></p><p>运行结果：</p><figure class="highlight xquery"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">  <span class="string">"headers"</span>: &#123;</div><div class="line">    <span class="string">"Accept"</span>: <span class="string">"*/*"</span>, </div><div class="line">    <span class="string">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>, </div><div class="line">    <span class="string">"Host"</span>: <span class="string">"httpbin.org"</span>, </div><div class="line">    <span class="string">"User-Agent"</span>: <span class="string">"python-requests/2.9.1"</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>以上就是session会话的基本用法。</p><blockquote><h3 id="SSL-证书验证"><a href="#SSL-证书验证" class="headerlink" title="SSL 证书验证"></a>SSL 证书验证</h3></blockquote><p>现在很多网址都是以https开头的，requests可以为https请求验证SSL证书，就像web浏览器一样。想要检查某个主机的SSL证书，可以使用verify参数</p><p>我们知道12306的证书无效了，我们测试一下：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"> </div><div class="line">r = requests.<span class="built_in">get</span>(<span class="string">'https://kyfw.12306.cn/otn/'</span>, verify=True)</div><div class="line"><span class="built_in">print</span>(r.<span class="built_in">text</span>)</div></pre></td></tr></table></figure><p>结果可以看出来它果然出问题了：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">requests.exceptions.SSLError: HTTPSConnectionPool(<span class="attribute">host</span>=<span class="string">'kyfw.12306.cn'</span>, <span class="attribute">port</span>=443)</div></pre></td></tr></table></figure></p><p>我们再来测试一下github的<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"> </div><div class="line">r = requests.<span class="built_in">get</span>(<span class="string">'https://github.com'</span>, verify=True)</div><div class="line"><span class="built_in">print</span>(r.<span class="built_in">text</span>)</div></pre></td></tr></table></figure></p><p>可以正常访问，返回的都是HTML代码，太多了，我就不贴出来了。</p><p>12306的网址证书无效，我们只需要把verify设置为False就可以跳过验证了：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"> </div><div class="line">r = requests.<span class="built_in">get</span>(<span class="string">'https://kyfw.12306.cn/otn/'</span>, verify=False)</div><div class="line"><span class="built_in">print</span>(r.<span class="built_in">text</span>)</div></pre></td></tr></table></figure><p>就可以正常请求返回的也是HTML代码，在默认情况下verify是True，但是就跟上面的12306的网站，就可以手动更改一下这个变量。</p><blockquote><h3 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h3></blockquote><p>有些时候，我们在写爬虫程序的时候，有些网址对请求有限制，如果一个IP频繁的访问或者发送请求，这个网站就可能把你的ip给封掉，这个时候为了防止这种事情的发生，我们就可以使用代理：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line"> </div><div class="line">proxies = &#123;</div><div class="line">   <span class="string">"https"</span>: <span class="string">"http://41.118.132.69:4433"</span></div><div class="line">&#125;</div><div class="line">r = requests.post(<span class="string">"http://httpbin.org/post"</span>, proxies=proxies)</div><div class="line">print(r.text)</div></pre></td></tr></table></figure><p>也可以通过环境变量HTTP_PROXY和HTTPS_PROXY来配置代理：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="builtin-name">export</span> <span class="attribute">HTTP_PROXY</span>=<span class="string">"http://10.10.1.10:3128"</span></div><div class="line"><span class="builtin-name">export</span> <span class="attribute">HTTPS_PROXY</span>=<span class="string">"http://10.10.1.10:1080"</span></div></pre></td></tr></table></figure></p><p>通过以上方式，可以方便地设置代理。</p><blockquote><h3 id="官方文档API"><a href="#官方文档API" class="headerlink" title="官方文档API"></a>官方文档API</h3></blockquote><p>上面简单介绍了requests中最常用的参数，如果还需要更多的了解，可以参考<a href="http://docs.python-requests.org/en/master/api/" target="_blank" rel="external">官方文档API</a></p><blockquote><p>文章如有出错的地方，请多多指正！如果你感觉文章对你有所帮助，文章下方有打赏按钮，您随意打赏。支持作者继续创作。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;今天我们来介绍一下requests库，这可是python在爬虫方面的神器，有了它在使用python进行爬虫方面的操作的时候简直不能太爽。requests号称“HTTP for Human”，它的功能十分强大，而它的操作却十分简单。&lt;/p&gt;
&lt;/bl
      
    
    </summary>
    
      <category term="Python" scheme="https://www.bbiao.me/categories/Python/"/>
    
    
      <category term="requests" scheme="https://www.bbiao.me/tags/requests/"/>
    
      <category term="爬虫" scheme="https://www.bbiao.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬取豆瓣电影TOP 250</title>
    <link href="https://www.bbiao.me/2017/08/29/%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1TOP-250/"/>
    <id>https://www.bbiao.me/2017/08/29/爬取豆瓣电影TOP-250/</id>
    <published>2017-08-29T04:10:22.000Z</published>
    <updated>2017-10-14T04:38:07.808Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><h3 id="随着大数据的爆发，数据分析越来越火。数据收集就变得十分重要，爬虫就是为数据收集而生的，Python刚好又有很强大的爬虫相关的库。Python在爬虫这方面是十分强大的。市面上一般的Python爬虫工程师工资每月都在10K-15K之间，而懂得一些框架的资深工程师工资都在30K以上，所以做爬虫这方面还是十分吃香的。万事开头难，只有迈开第一步，后面才能够大步往前走，博主也是初入爬虫这一行，并且打算深入学习它，一步一步往下走吧。说再多都没有用，想学好计算机相关的东西，光靠说是没有用到，手上功夫要跟上，不打代码是学不会计算机的。只有不段的打代码，代码量积累到一定程度之后，由一个量到质的变化。今天，就开始一个简单爬虫项目来对爬虫有一个初步的认识。"><a href="#随着大数据的爆发，数据分析越来越火。数据收集就变得十分重要，爬虫就是为数据收集而生的，Python刚好又有很强大的爬虫相关的库。Python在爬虫这方面是十分强大的。市面上一般的Python爬虫工程师工资每月都在10K-15K之间，而懂得一些框架的资深工程师工资都在30K以上，所以做爬虫这方面还是十分吃香的。万事开头难，只有迈开第一步，后面才能够大步往前走，博主也是初入爬虫这一行，并且打算深入学习它，一步一步往下走吧。说再多都没有用，想学好计算机相关的东西，光靠说是没有用到，手上功夫要跟上，不打代码是学不会计算机的。只有不段的打代码，代码量积累到一定程度之后，由一个量到质的变化。今天，就开始一个简单爬虫项目来对爬虫有一个初步的认识。" class="headerlink" title="随着大数据的爆发，数据分析越来越火。数据收集就变得十分重要，爬虫就是为数据收集而生的，Python刚好又有很强大的爬虫相关的库。Python在爬虫这方面是十分强大的。市面上一般的Python爬虫工程师工资每月都在10K~15K之间，而懂得一些框架的资深工程师工资都在30K以上，所以做爬虫这方面还是十分吃香的。万事开头难，只有迈开第一步，后面才能够大步往前走，博主也是初入爬虫这一行，并且打算深入学习它，一步一步往下走吧。说再多都没有用，想学好计算机相关的东西，光靠说是没有用到，手上功夫要跟上，不打代码是学不会计算机的。只有不段的打代码，代码量积累到一定程度之后，由一个量到质的变化。今天，就开始一个简单爬虫项目来对爬虫有一个初步的认识。"></a>随着大数据的爆发，数据分析越来越火。数据收集就变得十分重要，爬虫就是为数据收集而生的，Python刚好又有很强大的爬虫相关的库。Python在爬虫这方面是十分强大的。市面上一般的Python爬虫工程师工资每月都在10K~15K之间，而懂得一些框架的资深工程师工资都在30K以上，所以做爬虫这方面还是十分吃香的。万事开头难，只有迈开第一步，后面才能够大步往前走，博主也是初入爬虫这一行，并且打算深入学习它，一步一步往下走吧。说再多都没有用，想学好计算机相关的东西，光靠说是没有用到，手上功夫要跟上，不打代码是学不会计算机的。只有不段的打代码，代码量积累到一定程度之后，由一个量到质的变化。今天，就开始一个简单爬虫项目来对爬虫有一个初步的认识。</h3><h2 id="环境以及相关的库"><a href="#环境以及相关的库" class="headerlink" title="环境以及相关的库"></a>环境以及相关的库</h2><blockquote><h3 id="系统：-Ubuntu-16-04"><a href="#系统：-Ubuntu-16-04" class="headerlink" title="系统： Ubuntu 16.04"></a>系统： Ubuntu 16.04</h3><h3 id="Python环境：python-3-5"><a href="#Python环境：python-3-5" class="headerlink" title="Python环境：python 3.5"></a>Python环境：python 3.5</h3><h3 id="库：requests，BeautifulSoup，codecs"><a href="#库：requests，BeautifulSoup，codecs" class="headerlink" title="库：requests，BeautifulSoup，codecs"></a>库：requests，BeautifulSoup，codecs</h3></blockquote><p>以上只是我个人的环境，系统的话可以根据自己的习惯进行选择。</p><h2 id="正题部分"><a href="#正题部分" class="headerlink" title="正题部分"></a>正题部分</h2><blockquote><h3 id="这里我们爬取豆瓣电影Top-250的电影名称，网址是：https-movie-douban-com-top250"><a href="#这里我们爬取豆瓣电影Top-250的电影名称，网址是：https-movie-douban-com-top250" class="headerlink" title="这里我们爬取豆瓣电影Top 250的电影名称，网址是：https://movie.douban.com/top250"></a>这里我们爬取豆瓣电影Top 250的电影名称，网址是：<a href="https://movie.douban.com/top250" target="_blank" rel="external">https://movie.douban.com/top250</a></h3></blockquote><p><img src="http://ovfqn6f2x.bkt.clouddn.com/1.png" alt="1"></p><h3 id="首先我们要对网页的结构进行分析，爬虫很笨的，你必须清清楚楚的告诉它每一步该干什么，它才能知道做什么，所以我们先来分析网页结构，找到我们想要的内容，然后告诉爬虫让它给我们提取出来，这就是我们写爬虫的目的。"><a href="#首先我们要对网页的结构进行分析，爬虫很笨的，你必须清清楚楚的告诉它每一步该干什么，它才能知道做什么，所以我们先来分析网页结构，找到我们想要的内容，然后告诉爬虫让它给我们提取出来，这就是我们写爬虫的目的。" class="headerlink" title="首先我们要对网页的结构进行分析，爬虫很笨的，你必须清清楚楚的告诉它每一步该干什么，它才能知道做什么，所以我们先来分析网页结构，找到我们想要的内容，然后告诉爬虫让它给我们提取出来，这就是我们写爬虫的目的。"></a>首先我们要对网页的结构进行分析，爬虫很笨的，你必须清清楚楚的告诉它每一步该干什么，它才能知道做什么，所以我们先来分析网页结构，找到我们想要的内容，然后告诉爬虫让它给我们提取出来，这就是我们写爬虫的目的。</h3><p>在chrome浏览器下按F12打开开发者工具，就可以看到网页的源代码：<br><img src="http://ovfqn6f2x.bkt.clouddn.com/2.png" alt="2"><br><img src="http://ovfqn6f2x.bkt.clouddn.com/2.png" alt="3"></p><p>可以看到这个页面有25条电影信息，可以通过在任意一个电影上面点击鼠标右键–&gt;检查看到其HTML的结构。可以看到这一页的所有的电影都在class为”grid_view“的ol标签里面，每个电影对应一个li，在li下面的a标签下class为“title”的span对应的就是电影名称，也就是我们需要的内容，我们下面要做的就是告诉我们的爬虫，把这里的内容提取并且保存到文件就可以了。<br><img src="http://ovfqn6f2x.bkt.clouddn.com/3.png" alt="4"></p><h3 id="到目前为止，我们得到的内容有："><a href="#到目前为止，我们得到的内容有：" class="headerlink" title="到目前为止，我们得到的内容有："></a>到目前为止，我们得到的内容有：</h3><blockquote><ol><li>每页有25条电影，共有10页。</li><li>电影列表在页面的位置为一个class属性为grid_view的ol标签里。</li><li>每条电影都放在这个ol标签的li标签里</li></ol></blockquote><p>上面简单分析了一下网页结构，网页结构分析哪些东西呢？<br>需要手动的找到我们需要的东西，然后找到其对应的标签，这些标签的作用就跟路标一样，爬虫不想人类这样有智慧，你需要把这些路标告诉它，然后它就可以通过这些路标找到我们想要的东西。(当然这是简单爬虫的思路，如果是复杂的爬虫可能还需要构造请求，cookie，首部，IP等等一些东西，后面我们遇到了再一起学，现在还用不到)</p><p>找到标签之后该怎么办，第一步是通过python的requests库构造请求，就跟我们浏览网页一样，我们也是想服务器发送到请求，服务器收到我们的请求之后进行处理，如果没问题就好把我们所请求的内容发给我们，请求的方式有很多，最常见的两种是GET、POST，这两者具体有什么区别，请参考这篇文章：<a href="http://www.cnblogs.com/hyddd/archive/2009/03/31/1426026.html" target="_blank" rel="external">浅谈HTTP中Get与Post的区别 – hyddd – 博客园</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python3</div><div class="line"># encoding=utf-8</div><div class="line"></div><div class="line">import requests</div><div class="line"> </div><div class="line">url = &apos;https://movie.douban.com/top250&apos;</div><div class="line"> </div><div class="line">def get_html(url):</div><div class="line">    date = requests.get(url).content</div><div class="line">    return date</div><div class="line"> </div><div class="line">def main():</div><div class="line">    print(get_html(url))</div><div class="line"> </div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    main()</div></pre></td></tr></table></figure><p>上面的代码首先导入了requests模块，然后定义了一个url的变量用了放链接地址，然后定义了一个get_html的函数这个函数的作用就是返回网页所对应的HTML代码。main函数用来打印输出请求的结果，上面的代码运行后得到如下内容：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">b'<span class="meta">&lt;!DOCTYPE html&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"zh-cmn-Hans"</span> <span class="attr">class</span>=<span class="string">""</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">"Content-Type"</span> <span class="attr">content</span>=<span class="string">"text/html; charset=utf-8"</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">"renderer"</span> <span class="attr">content</span>=<span class="string">"webkit"</span>&gt;</span>\n    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">"referrer"</span> <span class="attr">content</span>=<span class="string">"always"</span>&gt;</span></div><div class="line">......</div><div class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></div><div class="line">....</div><div class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></div><div class="line">.....</div></pre></td></tr></table></figure></p><p>返回了一大堆的东西，这些都是HTML源代码。<br>下一步就是对这么多HTML的内容进行筛选和清洗，因为里面的绝大多数的内容都是我们不需要的。查找我们想要的内容有三种方法.</p><blockquote><h3 id="第一种是使用正则表达式"><a href="#第一种是使用正则表达式" class="headerlink" title="第一种是使用正则表达式"></a>第一种是使用正则表达式</h3></blockquote><p>这是一把万能钥匙，再大再乱的内容，哪怕是大海捞针，只要告诉我这个针长什么样子，就可以使用正则表达式提取你想要的数据。</p><blockquote><h3 id="第二种使用BeautifulSoup"><a href="#第二种使用BeautifulSoup" class="headerlink" title="第二种使用BeautifulSoup"></a>第二种使用BeautifulSoup</h3></blockquote><p>由于第一种方法相对要复杂一些，我们还有第二种方法，就是使用BeautifulSoup库，我们只需要把原始数据和我们想要的数据都扔个这个BeautifulSoup，然后让它给我们去寻找，这也是一个很不错的方案，但是论灵活性，第二种方法还是略微逊色于第一种方法</p><blockquote><p>第三种方法 双剑合璧</p></blockquote><p>最厉害的一种方法就是把这两种方法结合起来使用，具体怎么结合，后面我们用到了再说。</p><p><strong> 这里我们只需要使用BeautifulSoup就可以很轻松的完成：</strong><br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"> </div><div class="line"><span class="keyword">def</span> parse_html(html):</div><div class="line">    </div><div class="line">    soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">    movie_list_soup = soup.<span class="keyword">find</span>(<span class="string">'ol'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'grid_view'</span>&#125;)</div><div class="line">    <span class="keyword">for</span> movie_li in movie_list_soup.find_all(<span class="string">'li'</span>):</div><div class="line">        detail = movie_li.<span class="keyword">find</span>(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'hd'</span>&#125;)</div><div class="line">        movie_name = detail.<span class="keyword">find</span>(<span class="string">'span'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'title'</span>&#125;).<span class="keyword">getText</span>()</div><div class="line">        <span class="keyword">print</span>(movie_name)</div></pre></td></tr></table></figure></p><p>上面的代码就是我们爬虫的核心代码，首先我们导入了BeautifulSoup，然后我们定义了一个parse_html函数，它接受一个HTML源码作为输出，然后我们把我们的获取到的HTML代码丢给通过BeautifulSoup操作丢给soup变量，然后通过BeautifulSoup的find属性，在这一堆HTML源代码中找到我们让它找的标签(class为grid_view的ol标签)，这一步我们的爬虫就能够找到文章列表。下一步是通过循环让它依次遍历每一个li标签内的内容(因为每一个电影都是单独放在一个li标签里面的)。然后在for循环内部进行同样的查找操作，最后再把得到的文本内容(电影名称)打印出来.</p><blockquote><p>到目前为止，我们已经能够获得当前页面的所有电影的名称了，但是我们要的是提取所有页的电影名称，我们人知道看完这一页翻到下一页，但是爬虫很笨，你如果不告诉它怎么翻页，它是不会处理翻页的。<br>一般来说的话因为HTML代码具有很强的规律性，当我们知道网址的格式之后可以把页面跳转写死，因为页面的变化仅仅只是数字的变化，其他的内容几乎不会有改变，这样就可以把链接固定，只改变数字就可以进行翻页处理。这里为了让我们的爬虫更像爬虫，我们让它找到代码导航的下一页的链接，然后进行跳转，也就是说把链接的构造方式告诉我们的爬虫，然后让构造出不同页。以此实现页面跳转。</p></blockquote><p><img src="http://ovfqn6f2x.bkt.clouddn.com/4.png" alt="6"></p><p>通过把页面拉到最下面，鼠标放在后页的位置点击右键–&gt;检查可以看到上面的图片中的不同的页面的url的区别，每一页都有在一个a标签下，而class为next的span标签里面的link就是当前的url，我们通过从soup中find该属性下的a标签，就可以定位所有的页。最后一页之后就没有a标签了，只需要把next_page作为if的判断条件，就可以遍历所有的页，根据这样的思路我们修改一下上面的代码：</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">def</span> parse_html(html):</div><div class="line">    soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">    movie_list = soup.<span class="keyword">find</span>(<span class="string">'ol'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'grid_view'</span>&#125;)</div><div class="line"> </div><div class="line">    movie_name_list = []</div><div class="line">    <span class="keyword">for</span> movie_li in movie_list.find_all(<span class="string">'li'</span>):</div><div class="line">        detail = movie_li.<span class="keyword">find</span>(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'hd'</span>&#125;)</div><div class="line">        movie_name = detail.<span class="keyword">find</span>(<span class="string">'span'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'title'</span>&#125;).<span class="keyword">getText</span>()</div><div class="line">        movie_name_list.<span class="keyword">append</span>(movie_name)</div><div class="line">        </div><div class="line">    next_page = soup.<span class="keyword">find</span>(<span class="string">'span'</span>, attrs=&#123;<span class="string">'class'</span>, <span class="string">'next'</span>&#125;).<span class="keyword">find</span>(<span class="string">'a'</span>)</div><div class="line">    <span class="keyword">if</span> next_page:</div><div class="line">        <span class="keyword">return</span> movie_name_list, url + next_page[<span class="string">'href'</span>]</div><div class="line">    <span class="keyword">return</span> movie_name_list, None</div></pre></td></tr></table></figure><p>上面的代码我们构造了完整的页面链接，我们通过标签获取的内容只是完整链接的一部分，这时候我们就需要加上我们的url构造完整的链接，上面的代码就可以返回一个包含电影名的list，以及下一页的链接，如果到了最后一页就返回None。</p><p>到这里，我们的爬虫以及完成了绝大部分，接下来就只需要把这个list写入文件，把上面的函数组装起来就是一个完整的程序了：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> codecs</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    </div><div class="line">    relurl = url</div><div class="line">    <span class="keyword">with</span> codecs.open(<span class="string">'movies'</span>, <span class="string">'wb'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fp:</div><div class="line">        <span class="keyword">while</span> relurl:</div><div class="line">            html = get_html(relurl)</div><div class="line">            movies, relurl = parse_html(html)</div><div class="line">            fp.write(<span class="string">u'&#123;movies&#125;\n'</span>.format(movies=<span class="string">'\n'</span>.join(movies)))</div></pre></td></tr></table></figure></p><p>上面的代码中我们引入了codecs模块，这个模块是为了更方便的处理中文，文件写入的时候通过codecs.open写入并且指定编码格式，这样所有的数据都会进行统一编码，这里把reurl(页面)作为循环条件，当所有的页面遍历完，reurl就会变成None，这时候就可以跳出循环。最后把名称写入名为movies的文件中，整个程序就完成了。</p><blockquote><h2 id="小结部分："><a href="#小结部分：" class="headerlink" title="小结部分："></a>小结部分：</h2></blockquote><h3 id="这是我们写的一个简单的爬虫，麻雀虽小，五脏俱全。这里我们简单的使用了BeautifulSoup的使用，其中关于find的使用我在这里介绍对你用处也不大，真的想要掌握还需要自己去练习，不断地敲代码巩固才是王道。"><a href="#这是我们写的一个简单的爬虫，麻雀虽小，五脏俱全。这里我们简单的使用了BeautifulSoup的使用，其中关于find的使用我在这里介绍对你用处也不大，真的想要掌握还需要自己去练习，不断地敲代码巩固才是王道。" class="headerlink" title="这是我们写的一个简单的爬虫，麻雀虽小，五脏俱全。这里我们简单的使用了BeautifulSoup的使用，其中关于find的使用我在这里介绍对你用处也不大，真的想要掌握还需要自己去练习，不断地敲代码巩固才是王道。"></a>这是我们写的一个简单的爬虫，麻雀虽小，五脏俱全。这里我们简单的使用了BeautifulSoup的使用，其中关于find的使用我在这里介绍对你用处也不大，真的想要掌握还需要自己去练习，不断地敲代码巩固才是王道。</h3><h3 id="这里我们没有遇到阻碍，如果目标站点加入了反爬虫机制，我们的爬虫就需要构造UA，以及面对更加复杂的网页结构，使用数据库来存储我们爬取的内容，等等"><a href="#这里我们没有遇到阻碍，如果目标站点加入了反爬虫机制，我们的爬虫就需要构造UA，以及面对更加复杂的网页结构，使用数据库来存储我们爬取的内容，等等" class="headerlink" title="这里我们没有遇到阻碍，如果目标站点加入了反爬虫机制，我们的爬虫就需要构造UA，以及面对更加复杂的网页结构，使用数据库来存储我们爬取的内容，等等.."></a>这里我们没有遇到阻碍，如果目标站点加入了反爬虫机制，我们的爬虫就需要构造UA，以及面对更加复杂的网页结构，使用数据库来存储我们爬取的内容，等等..</h3><h3 id="爬虫的路还很长，这才刚刚开始"><a href="#爬虫的路还很长，这才刚刚开始" class="headerlink" title="爬虫的路还很长，这才刚刚开始"></a>爬虫的路还很长，这才刚刚开始</h3><blockquote><h2 id="完整代码如下："><a href="#完整代码如下：" class="headerlink" title="完整代码如下："></a>完整代码如下：</h2></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python3</span></div><div class="line"><span class="comment"># encoding=utf-8</span></div><div class="line"> </div><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">import</span> codecs</div><div class="line"> </div><div class="line">url = <span class="string">'https://movie.douban.com/top250'</span></div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_html</span><span class="params">(url)</span>:</span></div><div class="line">    date = requests.get(url).content</div><div class="line">    <span class="keyword">return</span> date</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_html</span><span class="params">(html)</span>:</span></div><div class="line">    soup = BeautifulSoup(html, <span class="string">'lxml'</span>)</div><div class="line">    movie_list = soup.find(<span class="string">'ol'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'grid_view'</span>&#125;)</div><div class="line"> </div><div class="line">    movie_name_list = []</div><div class="line">    <span class="keyword">for</span> movie_li <span class="keyword">in</span> movie_list.find_all(<span class="string">'li'</span>):</div><div class="line">        detail = movie_li.find(<span class="string">'div'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'hd'</span>&#125;)</div><div class="line">        movie_name = detail.find(<span class="string">'span'</span>, attrs=&#123;<span class="string">'class'</span>: <span class="string">'title'</span>&#125;).getText()</div><div class="line">        movie_name_list.append(movie_name)</div><div class="line"> </div><div class="line">    next_page = soup.find(<span class="string">'span'</span>, attrs=&#123;<span class="string">'class'</span>, <span class="string">'next'</span>&#125;).find(<span class="string">'a'</span>)</div><div class="line">    <span class="keyword">if</span> next_page:</div><div class="line">        <span class="keyword">return</span> movie_name_list, url + next_page[<span class="string">'href'</span>]</div><div class="line">    <span class="keyword">return</span> movie_name_list, <span class="keyword">None</span></div><div class="line"> </div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line"> </div><div class="line">    relurl = url</div><div class="line">    <span class="keyword">with</span> codecs.open(<span class="string">'movies'</span>, <span class="string">'wb'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fp:</div><div class="line">        <span class="keyword">while</span> relurl:</div><div class="line">            html = get_html(relurl)</div><div class="line">            movies, relurl = parse_html(html)</div><div class="line">            fp.write(<span class="string">u'&#123;movies&#125;\n'</span>.format(movies=<span class="string">'\n'</span>.join(movies)))</div><div class="line"> </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure><p>所有的电影名称就安静的躺在了我们的电脑里面了。</p><p><img src="http://ovfqn6f2x.bkt.clouddn.com/5.png" alt="7"></p><blockquote><p>文章如有出错的地方，请多多指正！如果你感觉文章对你有所帮助，文章下方有打赏按钮，您随意打赏。支持作者继续创作。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前提&quot;&gt;&lt;a href=&quot;#前提&quot; class=&quot;headerlink&quot; title=&quot;前提&quot;&gt;&lt;/a&gt;前提&lt;/h2&gt;&lt;h3 id=&quot;随着大数据的爆发，数据分析越来越火。数据收集就变得十分重要，爬虫就是为数据收集而生的，Python刚好又有很强大的爬虫相关的库。P
      
    
    </summary>
    
      <category term="网络爬虫" scheme="https://www.bbiao.me/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="https://www.bbiao.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>使用python制作词云</title>
    <link href="https://www.bbiao.me/2017/08/08/%E4%BD%BF%E7%94%A8python%E5%88%B6%E4%BD%9C%E8%AF%8D%E4%BA%91/"/>
    <id>https://www.bbiao.me/2017/08/08/使用python制作词云/</id>
    <published>2017-08-08T12:18:14.000Z</published>
    <updated>2017-12-03T10:04:27.519Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>python是一门灵活而又强大的语言，使用的人越来越多，大有大爆发的形式，越早掌握，越能把握优势！</p></blockquote><p>网上经常看到这样的图片：</p><p><img src="https://moetu.fastmirror.org/images/2017/12/03/1a79496dd80aaf14c.jpg" alt="1"> </p><p><img src="https://moetu.fastmirror.org/images/2017/12/03/37b03ddbcf1806336.jpg" alt="2"></p><p><img src="https://moetu.fastmirror.org/images/2017/12/03/2fa4442a0efa7e272.jpg" alt="3"></p><p>以上图片简称词云，网上有词云在线生成工具，但是样式都是固定的，不一定能够满足我们的需求，今天我们就来自己动手来制作词云，使用Python可以很轻松的制作词云。</p><blockquote><h2 id="词云制作"><a href="#词云制作" class="headerlink" title="词云制作"></a>词云制作</h2></blockquote><h3 id="依赖的库文件："><a href="#依赖的库文件：" class="headerlink" title="依赖的库文件："></a>依赖的库文件：</h3><p>wordcloud python下制作词云最强大的库<br>可以使用<font color="Lime">pip install wordcloud</font> 进行安装<br>matplotlib python下画图的利器，几句话也介绍不完，详细信息请参考这里<br>可以到这个网址进行手动安装：<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#matplotlib" target="_blank" rel="external">http://www.lfd.uci.edu/~gohlke/pythonlibs/#matplotlib</a></p><p>有了上面上个工具，我们就可以开始进行简单的词云制作了：<br>首先，我们需要先收集一些词语，我随便从网络上收集了一些英文，先介绍英文，因为汉语的有所不同，下面再单独介绍！</p><p><img src="https://moetu.fastmirror.org/images/2017/12/03/48a7391432a277b14.png" alt="5"></p><p>上面就是我们收集的英文，保存到txt的文本里就可以了。</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">from wordcloud <span class="built_in">import</span> WordCloud</div><div class="line"><span class="built_in">import</span> matplotlib.pyplot as plt</div><div class="line"></div><div class="line"><span class="attr">filename</span> = 'ciyun.txt'</div><div class="line"><span class="keyword">with</span> open(filename) as f:</div><div class="line">    <span class="attr">mytext</span> = f.read()</div><div class="line"><span class="attr">wordcloud</span> = WordCloud(<span class="attr">collocations=False,</span> <span class="attr">width=1700,</span> <span class="attr">height=1400,</span> <span class="attr">margin=2).generate(mytext)</span></div><div class="line">plt.imshow(wordcloud, <span class="attr">interpolation='bilinear')</span></div><div class="line">plt.axis('off')</div><div class="line">plt.show()</div></pre></td></tr></table></figure><blockquote><p>简单解读一下上面的代码的含义：</p></blockquote><ul><li>先导入我们需要用到的模块</li><li>然后找到txt的路径，通过with open打开文件，把文件读给mytext</li><li>WordCloud(collocations=False, width=1700, #设置宽度<br>height=1400, #设置高度<br>margin=2,<br>background_color=”white”, #背景颜色<br>max_words=2000,# 词云显示的最大词数,<br>mask=alice_coloring, #设置背景图片<br>stopwords=STOPWORDS.add(“said”),<br>max_font_size=40, #字体最大值<br>random_state=42<br>)</li><li>generate 可以对全部文本进行自动分词,但是他对中文支持不好<br>imshow(wordcloud, interpolation=’bilinear’) # 通过这种方式词云将会按照给定的图片颜色布局生成字体颜色策略</li><li>plt.axis(‘off’) #不显示坐标尺寸</li><li>plt.show() # 把图片显示出来<br>效果如下：<br><img src="https://moetu.fastmirror.org/images/2017/12/03/5b25bcb9a462a2c7b.png" alt="6"></li></ul><p>上面的词云是针对英文进行制作的，下面介绍一下怎么对中文进行词云的制作（仅仅有一点点的区别）<br>汉语内容只需要在代码中加入，如果不加的话，中文会显示方框乱码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">font = <span class="string">r'C://Windows//Fonts/simfang.ttf'</span>    <span class="comment">#从本地读取字体</span></div><div class="line">wordcloud = WordCloud(collocations=<span class="keyword">False</span>, font_path=font, width=<span class="number">1700</span>, height=<span class="number">1400</span>, margin=<span class="number">2</span>).generate(mytext)</div></pre></td></tr></table></figure></p><p><img src="https://moetu.fastmirror.org/images/2017/12/03/6f759bff3738b3e0b.png" alt="7"></p><p>当然，这种词云是最基础的，有时候这些不能够满足我们的需求，我们想要制作一些个性化的词云，下面就开始进行词云的进阶。<br>很多时候我们想要这样的词云：</p><p><img src="https://moetu.fastmirror.org/images/2017/12/03/70ecbfe5fd7fd813e.png" alt="8"></p><p><img src="https://moetu.fastmirror.org/images/2017/12/03/8f5553bb73883b45f.png" alt="9"></p><p><img src="https://moetu.fastmirror.org/images/2017/12/03/90ba9092561ad0541.png" alt="10"></p><blockquote><h3 id="个性化词云"><a href="#个性化词云" class="headerlink" title="个性化词云"></a>个性化词云</h3></blockquote><p>这种自定义形状的词云该如何制作呢？<br>需要用到的库有PIL、numpy、matplotlib、wordcloud<br>如果有汉语的话还需要使用jieba库进行中文分词。<br>先亮代码吧：<br><figure class="highlight xl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">from os <span class="keyword">import</span> <span class="built_in">path</span></div><div class="line">from PIL <span class="keyword">import</span> Image</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"> </div><div class="line">from wordcloud <span class="keyword">import</span> WordCloud, STOPWORDS, ImageColorGenerator</div><div class="line"> </div><div class="line">d = <span class="built_in">path</span>.dirname(__file__)</div><div class="line"> </div><div class="line"><span class="keyword">text</span> = open(<span class="built_in">path</span>.join(d, <span class="string">'./ciyun.txt'</span>)).read()</div><div class="line"> </div><div class="line">coloring = np.array(Image.open(<span class="built_in">path</span>.join(d,<span class="string">".\bg7.jpg"</span>)))</div><div class="line">font=<span class="string">'C:/Windows/Fonts/simfang.ttf'</span></div><div class="line">wc = WordCloud(background_color=<span class="string">"white"</span>, max_words=<span class="number">200</span>, mask=alice_coloring,font_path=font,</div><div class="line">               stopwords=STOPWORDS.add(<span class="string">"said"</span>),</div><div class="line">               max_font_size=<span class="number">80</span>, random_state=<span class="number">42</span>)</div><div class="line"> </div><div class="line">wc.generate(<span class="keyword">text</span>)</div><div class="line"> </div><div class="line">image_colors = ImageColorGenerator(coloring)</div><div class="line"> </div><div class="line">plt.imshow(wc)</div><div class="line">plt.axis(<span class="string">"off"</span>)</div><div class="line">plt.figure()</div><div class="line">plt.imshow(wc.recolor(color_func=image_colors))</div><div class="line">plt.axis(<span class="string">"off"</span>)</div><div class="line">plt.figure()</div><div class="line">plt.imshow(coloring, cmap=plt.cm.gray)</div><div class="line">plt.axis(<span class="string">"off"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></p><p>对比图如下：</p><p><img src="https://moetu.fastmirror.org/images/2017/12/03/10f87e39b623f68e25.png" alt="11"></p><p><img src="https://moetu.fastmirror.org/images/2017/12/03/117c1a9b412b0387a1.png" alt="12"></p><p>具体内容以及更多例子可以参考wordcloud作者的GitHub：<a href="https://github.com/amueller/word_cloud/tree/master/examples" target="_blank" rel="external">https://github.com/amueller/word_cloud/tree/master/examples</a></p><p>建议图片最好选择纯色的，清晰的，白色背景并且形状鲜明的。否则图片内容不好提取。</p><blockquote><p>如果你感觉文章对你有所帮助，文章下方有打赏按钮，请您随意打赏，以支持作者继续写作！</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;python是一门灵活而又强大的语言，使用的人越来越多，大有大爆发的形式，越早掌握，越能把握优势！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;网上经常看到这样的图片：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://moetu.fastmirro
      
    
    </summary>
    
      <category term="Python" scheme="https://www.bbiao.me/categories/Python/"/>
    
    
      <category term="wordcloud" scheme="https://www.bbiao.me/tags/wordcloud/"/>
    
      <category term="词云" scheme="https://www.bbiao.me/tags/%E8%AF%8D%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>kali-linux破解wifi密码</title>
    <link href="https://www.bbiao.me/2017/08/05/kali-linux%E7%A0%B4%E8%A7%A3wifi%E5%AF%86%E7%A0%81/"/>
    <id>https://www.bbiao.me/2017/08/05/kali-linux破解wifi密码/</id>
    <published>2017-08-05T13:03:39.000Z</published>
    <updated>2017-12-03T09:51:23.402Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>现在的人都离不开网络，感觉离开网络整个人就被这个世界隔离了。而流量又那么的昂贵，所有WI-FI的作用变的举足轻重，几乎绝大多数的WI-FI都是要密码的，这就让我们很头疼，市面上也有很多破解WI-FI的软件，最知名的就数某万能钥匙，手机电脑版的都有，操作方便，但是电脑版的在使用的过程中需要先连接上网络，然后从云端获取指定WI-FI的密码，如果数据库中没有该密码，就不能进行破解。<br>今天，我们来简单介绍一下在kali linux下怎么在无网络环境的情况下进行WI-FI密码破解：</p></blockquote><font color="rad">破解别人WI-FI是违法行为，仅限于学习！大家知道怎么破解的，然后就能知道怎么防范，知己知彼，百战不殆！</font><blockquote><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2></blockquote><ul><li>装有kali linux的计算机(物理机上，虚拟机的话需要有额外的网卡)</li><li>使用Aircrack-ng进行破解</li><li>支持监控模式的网卡，现在的笔记本一般都支持</li><li>字典文件（没有网络的情况下，就是靠从字典里暴力破解）</li><li>耐心（如果密码很复杂，可能破解的时间要长）</li></ul><blockquote><h2 id="Aircrack-ng"><a href="#Aircrack-ng" class="headerlink" title="Aircrack-ng"></a>Aircrack-ng</h2></blockquote><p>Aircrack-ng是一个与802.11标准的无线网络分析有关的安全软件，主要功能有：网络侦测，数据包嗅探，WEP和WPA/WPA2-PSK破解。Aircrack-ng可以工作在任何支持监听模式的无线网卡上并嗅探802.11a，802.11b，802.11g的数据。该程序可运行在Linux和Windows上。Linux版本已经被移植到了Zaurus和Maemo系统平台, 并概念验证可移植到iPhone。<br>总之Aircrack-ng是一个用来测试WIFI安全性的强大工具。<br>Aircrack-ng包包含有如下模块：</p><p><img src="https://i.loli.net/2017/12/03/5a23c7c9d074b.png" alt="1"></p><p><a href="https://www.aircrack-ng.org/" target="_blank" rel="external">Aircrack-NG官方网站</a><br>在kali linux里给我们默认了一份字典，在/usr/share/wordlists/rockyou.txt.gz<br>下面的操作都是在没有网络环境的情况下进行的，如果你连着WI-FI就先把ＷI-FI断掉。</p><blockquote><h3 id="实战演练"><a href="#实战演练" class="headerlink" title="实战演练"></a>实战演练</h3></blockquote><p>我使用自家的WIFI进行演示。<br>首先需要把破解密码的字典给解压出来，使用如下命令：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gzip -d /usr/share/wordlists/rockyou<span class="selector-class">.txt</span><span class="selector-class">.gz</span></div></pre></td></tr></table></figure></p><p>紧接着在终端中输入：<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="attribute">airmon-ng</span></div></pre></td></tr></table></figure></p><p>如果出现如下画面，则表示你的网卡是支持监控模式的，如果输入命令之后，没有反应，则表示你的网卡是不支持监控模式的：</p><p><img src="https://i.loli.net/2017/12/03/5a23c886e8220.png" alt="3"></p><p>下一步当然是开启无线网卡的监控模式啦(这里根据你自己的情况来，我的网卡显示的是wlan0，你的显示的是什么就写什么)：<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">airmon-ng <span class="literal">start</span> wlan0</div></pre></td></tr></table></figure></p><p><img src="https://i.loli.net/2017/12/03/5a23c888182a4.png" alt="4"></p><p>可以看到（mac80211 monitor mode vif enabled …）就表示开启成功，并且网卡信息也变成可wlan0mon，可以使用ifconfig进行查看。<br>下面查看wifi网络，就是扫描你周边的wifi：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">airodump-ng wlan0mon</div></pre></td></tr></table></figure></p><p>可以看到你周边的wifi的详细信息，包括信号强度、加密类型、频道等，，你需要记住要破解的wifi的频道号和BSSID，然后就可以按Ctrl-C结束了，第一个FAST_E984就是我家的WIFI：</p><p><img src="https://i.loli.net/2017/12/03/5a23c8885d7de.png" alt="5"></p><p>下一步是抓取握手包，使用网卡的监听模式抓取周围的无线网络数据包。其中，对我们最重要的数据包是：包含密码的包（也叫握手包），当有新用户断开或者自动连接wifi时，会发生握手包。<br>使用如下命令进行抓包：<br><figure class="highlight less"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">airodump-ng</span> <span class="selector-tag">-c</span> <span class="selector-tag">1</span> <span class="selector-tag">--bssid</span> <span class="selector-tag">D4</span><span class="selector-pseudo">:83</span><span class="selector-pseudo">:04</span><span class="selector-pseudo">:5E</span><span class="selector-pseudo">:E9</span><span class="selector-pseudo">:84</span> <span class="selector-tag">-w</span> ~/ <span class="selector-tag">wlan0mon</span></div></pre></td></tr></table></figure></p><blockquote><p>-c 是指频道号，在上面的图中的CH的值<br>-bssid 指定路由器的bssid<br>-w 把数据包写入到指定位置</p></blockquote><p><img src="https://i.loli.net/2017/12/03/5a23c888c435b.png" alt="6"></p><p>紧接着，我们就等着用户连接/重新连接wifi了，运气好的话可能很快就可以。运气不好的话可能要等很长时间。这不是我们想要的结果，当然还有其他的方法可以做，aircrack-ng下有一个aireplay-ng模块，它可以强制用户断开wifi连接，原理是:给连接到的wifi的一个设备发送一个deauth（反认证）包，让那个设备断开wifi，随后它自己就会再次连接wifi。<br>但是这种方法生效的前提是：wifi网络中至少要有一个连接的设备。从上图中可以看到有哪些网络设备连接到该路由上了，SIATTON就是连接设备的MAC地址，记住一个。<br>另外再打开一个新的终端，并执行如下命令：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="selector-tag">aireplay-ng</span> <span class="selector-tag">-0</span> 2 <span class="selector-tag">-a</span> <span class="selector-tag">D4</span><span class="selector-pseudo">:83</span><span class="selector-pseudo">:04</span><span class="selector-pseudo">:5E</span><span class="selector-pseudo">:E9</span><span class="selector-pseudo">:84</span> <span class="selector-tag">-c</span> <span class="selector-tag">A0</span><span class="selector-pseudo">:18</span><span class="selector-pseudo">:28</span><span class="selector-pseudo">:A2</span><span class="selector-pseudo">:0E</span><span class="selector-pseudo">:E9</span> <span class="selector-tag">wlan0mon</span></div></pre></td></tr></table></figure></p><blockquote><p>-0 表示发起deauthentication攻击<br>-a 表示指定无线路由器bssid<br>-c 指定强制断开的设备<br>如何成功的话可以看到在第一行后面会多出来 [WPA handshake:D4:83:04:5E:E9:84]</p></blockquote><p><img src="https://i.loli.net/2017/12/03/5a23c8895a2db.png" alt="7"></p><p>我们前面的所做都是为了得到这个握手包，得到之后就可以把无线网卡的监控模式关闭了，否则没法正常连接网络：<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">airmon-ng <span class="built_in">stop</span> wlan0mon</div></pre></td></tr></table></figure></p><p>可以看到（mac80211 station mode vif enabled on [py0]wlan0）,这样则表示切换成wlan0模式了。</p><p><img src="https://i.loli.net/2017/12/03/5a23c88992f82.png" alt="8"></p><p>下一步就是对密码进行破解了：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">aircrack-ng -a2 -b <span class="string">D4:</span><span class="number">83</span>:<span class="number">04</span>:<span class="number">5</span><span class="string">E:</span><span class="string">E9:</span><span class="number">84</span> -w <span class="regexp">/usr/</span>share<span class="regexp">/wordlists/</span>rockyou.txt ~<span class="comment">/*.cap</span></div></pre></td></tr></table></figure></p><blockquote><p>-a2 代表WPA握手包<br>-b 表示要破解的wifi 的BSSID<br>-w 指定字典文件<br>最后的是你抓取到的握手包的所在位置，在上面我们抓包的时候把它放在了根目录下.<br>可以看到我们已经破解得到了密码，KEY FOUND中显示的内容就是我们破解得到的密码(如果密码很简单，可以在很短的时间内破解掉)：<br><img src="https://i.loli.net/2017/12/03/5a23c889a000e.png" alt="9"></p></blockquote><p>测试成功的关键是有一个靠谱的字典文件，字典越全面，破解成功的可能性越大，电脑性能越好，破解速度越快。<br>关于如何防范最简单的方法就是把密码设置的足够复杂，足够长。<br>还有就是不要使用WEP加密方式，这种方式最容易被破解。</p><p>参考：<a href="http://topspeedsnail.com/kali-linux-crack-wifi-wpa/" target="_blank" rel="external">http://topspeedsnail.com/kali-linux-crack-wifi-wpa/</a></p><p>这里给出一个相对全面一点的字典下载地址：</p><p><a href="https://pan.baidu.com/s/1jIdmIHK?errno=0&amp;errmsg=Auth%20Login%20Sucess&amp;&amp;bduss=&amp;ssnerror=0#list/path=/" target="_blank" rel="external">百度网盘下载地址</a></p><blockquote><p>如果你感觉文章对你有所帮助，文章下方有打赏按钮，请您随意打赏，以支持作者继续写作！</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;现在的人都离不开网络，感觉离开网络整个人就被这个世界隔离了。而流量又那么的昂贵，所有WI-FI的作用变的举足轻重，几乎绝大多数的WI-FI都是要密码的，这就让我们很头疼，市面上也有很多破解WI-FI的软件，最知名的就数某万能钥匙，手机电脑版的都有，
      
    
    </summary>
    
      <category term="Kali Linux" scheme="https://www.bbiao.me/categories/Kali-Linux/"/>
    
    
      <category term="kali linux" scheme="https://www.bbiao.me/tags/kali-linux/"/>
    
  </entry>
  
</feed>
